"""
LLM Providers - å„LLMæœåŠ¡æä¾›å•†çš„å®ç°

æ”¯æŒçš„Provider:
- OpenAI (GPT-4, GPT-3.5)
- Anthropic (Claude-3)
- vLLM (è‡ªæ‰˜ç®¡æ¨¡å‹)
"""
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import asyncio
import time
import json
import openai
import anthropic
import aiohttp
from loguru import logger


@dataclass
class ProviderConfig:
    """Provideré…ç½®"""
    api_key: Optional[str] = None
    api_url: Optional[str] = None
    model: str = "gpt-4"
    max_tokens: int = 1000
    temperature: float = 0.8
    timeout: int = 60


class LLMProvider(ABC):
    """LLM ProvideræŠ½è±¡åŸºç±»"""

    def __init__(self, config: ProviderConfig):
        self.config = config
        self._name = "base"

    @property
    def name(self) -> str:
        """Provideråç§°"""
        return self._name

    def _log_request(
        self,
        messages: List[Dict[str, str]],
        model: str,
        max_tokens: int,
        temperature: float,
        extra_params: Optional[Dict] = None
    ) -> str:
        """è®°å½•è¯·æ±‚æ—¥å¿—ï¼Œè¿”å›è¯·æ±‚IDç”¨äºå…³è”å“åº”"""
        import uuid
        request_id = str(uuid.uuid4())[:8]

        # è®¡ç®—æ¶ˆæ¯ç»Ÿè®¡
        message_count = len(messages)
        total_chars = sum(len(msg.get("content", "")) for msg in messages)

        # æå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼ˆæˆªå–å‰100å­—ç¬¦ï¼‰
        last_user_msg = ""
        for msg in reversed(messages):
            if msg.get("role") == "user":
                last_user_msg = msg.get("content", "")[:100]
                if len(msg.get("content", "")) > 100:
                    last_user_msg += "..."
                break

        logger.info(
            f"ğŸš€ [LLM-REQ][{request_id}] provider={self._name} | "
            f"model={model} | messages={message_count} | chars={total_chars} | "
            f"max_tokens={max_tokens} | temperature={temperature}"
        )
        logger.debug(
            f"ğŸ“ [LLM-REQ][{request_id}] last_user_message: {last_user_msg}"
        )

        if extra_params:
            logger.debug(f"ğŸ“ [LLM-REQ][{request_id}] extra_params: {extra_params}")

        return request_id

    def _log_response(
        self,
        request_id: str,
        content: str,
        usage: Dict[str, int],
        model: str,
        finish_reason: str,
        latency_ms: float
    ) -> None:
        """è®°å½•å“åº”æ—¥å¿—"""
        response_preview = content[:150] + "..." if len(content) > 150 else content
        # ç§»é™¤æ¢è¡Œç¬¦ä»¥ä¾¿æ—¥å¿—æ›´æ˜“è¯»
        response_preview = response_preview.replace("\n", " ")

        logger.info(
            f"âœ… [LLM-RES][{request_id}] provider={self._name} | "
            f"model={model} | latency={latency_ms:.0f}ms | "
            f"tokens(prompt={usage.get('prompt_tokens', 0)}, "
            f"completion={usage.get('completion_tokens', 0)}, "
            f"total={usage.get('total_tokens', 0)}) | "
            f"finish_reason={finish_reason}"
        )
        logger.debug(f"ğŸ“¤ [LLM-RES][{request_id}] response_preview: {response_preview}")

    def _log_error(
        self,
        request_id: str,
        error: Exception,
        latency_ms: float
    ) -> None:
        """è®°å½•é”™è¯¯æ—¥å¿—"""
        logger.error(
            f"âŒ [LLM-ERR][{request_id}] provider={self._name} | "
            f"latency={latency_ms:.0f}ms | error_type={type(error).__name__} | "
            f"error={str(error)}"
        )

    @abstractmethod
    async def generate(
        self,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> Dict[str, Any]:
        """ç”Ÿæˆå“åº”"""
        pass

    @abstractmethod
    def count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
        pass


class OpenAIProvider(LLMProvider):
    """OpenAI GPT Provider"""

    def __init__(self, config: ProviderConfig):
        super().__init__(config)
        self._name = "openai"
        if not config.api_key:
            raise ValueError("OpenAI API key is required")
        self.client = openai.AsyncOpenAI(api_key=config.api_key)

    async def generate(
        self,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> Dict[str, Any]:
        """ä½¿ç”¨OpenAIç”Ÿæˆå“åº”"""
        model = kwargs.get("model", self.config.model)
        max_tokens = kwargs.get("max_tokens", self.config.max_tokens)
        temperature = kwargs.get("temperature", self.config.temperature)

        # è®°å½•è¯·æ±‚æ—¥å¿—
        request_id = self._log_request(messages, model, max_tokens, temperature)
        start_time = time.perf_counter()

        try:
            response = await self.client.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature
            )

            latency_ms = (time.perf_counter() - start_time) * 1000

            result = {
                "content": response.choices[0].message.content,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                },
                "model": response.model,
                "finish_reason": response.choices[0].finish_reason
            }

            # è®°å½•å“åº”æ—¥å¿—
            self._log_response(
                request_id=request_id,
                content=result["content"],
                usage=result["usage"],
                model=result["model"],
                finish_reason=result["finish_reason"],
                latency_ms=latency_ms
            )

            return result

        except Exception as e:
            latency_ms = (time.perf_counter() - start_time) * 1000
            self._log_error(request_id, e, latency_ms)
            raise

    def count_tokens(self, text: str) -> int:
        """ä¼°ç®—tokenæ•°ï¼ˆç®€å•å®ç°ï¼‰"""
        chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        other_chars = len(text) - chinese_chars
        return int(chinese_chars / 1.5 + other_chars / 4)


class AnthropicProvider(LLMProvider):
    """Anthropic Claude Provider"""

    def __init__(self, config: ProviderConfig):
        super().__init__(config)
        self._name = "anthropic"
        if not config.api_key:
            raise ValueError("Anthropic API key is required")
        self.client = anthropic.AsyncAnthropic(api_key=config.api_key)

    async def generate(
        self,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> Dict[str, Any]:
        """ä½¿ç”¨Anthropic Claudeç”Ÿæˆå“åº”"""
        model = kwargs.get("model", self.config.model)
        max_tokens = kwargs.get("max_tokens", self.config.max_tokens)

        # æå–system message
        system_prompt = None
        claude_messages = []
        for msg in messages:
            if msg["role"] == "system":
                system_prompt = msg["content"]
            else:
                role = "user" if msg["role"] == "user" else "assistant"
                claude_messages.append({
                    "role": role,
                    "content": msg["content"]
                })

        # è®°å½•è¯·æ±‚æ—¥å¿—
        request_id = self._log_request(
            messages, model, max_tokens,
            self.config.temperature,
            {"has_system_prompt": system_prompt is not None}
        )
        start_time = time.perf_counter()

        try:
            response = await self.client.messages.create(
                model=model,
                max_tokens=max_tokens,
                system=system_prompt or "",
                messages=claude_messages
            )

            latency_ms = (time.perf_counter() - start_time) * 1000

            result = {
                "content": response.content[0].text,
                "usage": {
                    "prompt_tokens": response.usage.input_tokens,
                    "completion_tokens": response.usage.output_tokens,
                    "total_tokens": response.usage.input_tokens + response.usage.output_tokens
                },
                "model": response.model,
                "finish_reason": response.stop_reason
            }

            # è®°å½•å“åº”æ—¥å¿—
            self._log_response(
                request_id=request_id,
                content=result["content"],
                usage=result["usage"],
                model=result["model"],
                finish_reason=result["finish_reason"],
                latency_ms=latency_ms
            )

            return result

        except Exception as e:
            latency_ms = (time.perf_counter() - start_time) * 1000
            self._log_error(request_id, e, latency_ms)
            raise

    def count_tokens(self, text: str) -> int:
        """ä¼°ç®—tokenæ•°"""
        chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        other_chars = len(text) - chinese_chars
        return int(chinese_chars / 1.5 + other_chars / 4)


class VLLMProvider(LLMProvider):
    """vLLMè‡ªæ‰˜ç®¡æ¨¡å‹Provider"""

    def __init__(self, config: ProviderConfig):
        super().__init__(config)
        self._name = "vllm"
        if not config.api_url:
            raise ValueError("vLLM API URL is required")
        self.api_url = config.api_url.rstrip('/')
        self.api_token = config.api_key

    async def generate(
        self,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> Dict[str, Any]:
        """ä½¿ç”¨vLLMç”Ÿæˆå“åº”"""
        model = kwargs.get("model", self.config.model)
        max_tokens = kwargs.get("max_tokens", self.config.max_tokens)
        temperature = kwargs.get("temperature", self.config.temperature)

        # è®°å½•è¯·æ±‚æ—¥å¿—
        request_id = self._log_request(
            messages, model, max_tokens, temperature,
            {"api_url": self.api_url}
        )
        start_time = time.perf_counter()

        headers = {"Content-Type": "application/json"}
        if self.api_token:
            headers["Authorization"] = f"Bearer {self.api_token}"

        payload = {
            "model": model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature
        }

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.api_url}/v1/chat/completions",
                    json=payload,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=self.config.timeout)
                ) as response:
                    latency_ms = (time.perf_counter() - start_time) * 1000

                    if response.status != 200:
                        error_text = await response.text()
                        error = Exception(f"vLLM API error: {response.status} - {error_text}")
                        self._log_error(request_id, error, latency_ms)
                        raise error

                    result_json = await response.json()

                    usage = result_json.get("usage", {})
                    result = {
                        "content": result_json["choices"][0]["message"]["content"],
                        "usage": {
                            "prompt_tokens": usage.get("prompt_tokens", 0),
                            "completion_tokens": usage.get("completion_tokens", 0),
                            "total_tokens": usage.get("total_tokens", 0)
                        },
                        "model": result_json.get("model", model),
                        "finish_reason": result_json["choices"][0].get("finish_reason", "stop")
                    }

                    # è®°å½•å“åº”æ—¥å¿—
                    self._log_response(
                        request_id=request_id,
                        content=result["content"],
                        usage=result["usage"],
                        model=result["model"],
                        finish_reason=result["finish_reason"],
                        latency_ms=latency_ms
                    )

                    return result

        except aiohttp.ClientError as e:
            latency_ms = (time.perf_counter() - start_time) * 1000
            self._log_error(request_id, e, latency_ms)
            raise
        except Exception as e:
            latency_ms = (time.perf_counter() - start_time) * 1000
            self._log_error(request_id, e, latency_ms)
            raise

    def count_tokens(self, text: str) -> int:
        """ä¼°ç®—tokenæ•°"""
        chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        other_chars = len(text) - chinese_chars
        return int(chinese_chars / 1.5 + other_chars / 4)
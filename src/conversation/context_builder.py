"""
Unified Context Builder - ç»Ÿä¸€ä¸Šä¸‹æ–‡æ„å»ºå™¨

è´Ÿè´£æ„å»ºå‘é€ç»™ LLM çš„å®Œæ•´æ¶ˆæ¯ç»“æ„ï¼Œé‡‡ç”¨åˆ†å±‚æ–¹å¼ï¼š

æ¶ˆæ¯ç»“æ„ï¼š
1. System Promptï¼ˆåŒ…å«äººè®¾ + é•¿æœŸè®°å¿† + å¯¹è¯ç­–ç•¥ï¼‰
2. çŸ­æœŸå¯¹è¯å†å²ï¼ˆæœ€è¿‘ 3-5 è½®å®Œæ•´å†…å®¹ï¼‰
3. å½“å‰ç”¨æˆ·æ¶ˆæ¯

åŠŸèƒ½ï¼š
- åˆ†å‰²å†å²ï¼ˆçŸ­æœŸ vs ä¸­æœŸï¼‰
- ç”Ÿæˆä¸­æœŸæ‘˜è¦ï¼ˆæ”¯æŒç¼“å­˜ï¼‰
- æ•´åˆæ‰€æœ‰ä¸Šä¸‹æ–‡
- æ„å»ºæœ€ç»ˆæ¶ˆæ¯åˆ—è¡¨
- Token é¢„ç®—ç®¡ç†
- å†å²å¯¹è¯è¿‡æ»¤ï¼ˆURLã€ç®€å•å¯’æš„ç­‰ï¼‰
"""
from typing import List, Dict, Optional, Any, Tuple
from dataclasses import dataclass, field
from loguru import logger

from .summary_service import ConversationSummaryService, ConversationSummary
from .proactive_strategy import (
    ProactiveDialogueStrategyAnalyzer,
    ProactiveAction,
    UserProfile,
    TopicAnalysis
)
from src.utils.history_filter import HistoryFilter, get_history_filter


@dataclass
class ContextConfig:
    """
    ä¸Šä¸‹æ–‡é…ç½®
    
    ç”¨äºæ§åˆ¶ä¸Šä¸‹æ–‡æ„å»ºçš„å„é¡¹å‚æ•°
    """
    # å¯¹è¯å†å²åˆ†å±‚
    short_term_rounds: int = 5  # çŸ­æœŸå†å²è½®æ•°ï¼ˆæœ€è¿‘ N è½®ï¼‰
    mid_term_start: int = 3  # ä¸­æœŸå†å²å¼€å§‹è½®æ¬¡
    mid_term_end: int = 20  # ä¸­æœŸå†å²ç»“æŸè½®æ¬¡
    
    # é•¿æœŸè®°å¿†
    max_memories: int = 8  # æœ€å¤šåŒ…å«çš„é•¿æœŸè®°å¿†æ•°é‡
    
    # Token é¢„ç®—
    max_total_tokens: int = 8000  # æ€» token é¢„ç®—
    reserved_output_tokens: int = 1000  # ä¸ºè¾“å‡ºä¿ç•™çš„ token
    
    # æ‘˜è¦é€‰é¡¹
    use_llm_summary: bool = False  # æ˜¯å¦ä½¿ç”¨ LLM æ‘˜è¦ï¼ˆæ¶ˆè€— tokenï¼‰
    max_summary_length: int = 200  # æ‘˜è¦æœ€å¤§é•¿åº¦
    
    # ä¸»åŠ¨ç­–ç•¥
    enable_proactive_strategy: bool = True  # æ˜¯å¦å¯ç”¨ä¸»åŠ¨ç­–ç•¥
    
    # å†å²è¿‡æ»¤é€‰é¡¹
    enable_history_filter: bool = True  # æ˜¯å¦å¯ç”¨å†å²è¿‡æ»¤ï¼ˆè¿‡æ»¤URLã€ç®€å•å¯’æš„ç­‰ï¼‰
    filter_urls: bool = True  # æ˜¯å¦è¿‡æ»¤URLä¸»å¯¼çš„å†…å®¹
    filter_trivial: bool = True  # æ˜¯å¦è¿‡æ»¤ç®€å•å¯’æš„


@dataclass
class BuilderResult:
    """
    æ„å»ºå™¨ç»“æœ
    
    åŒ…å«æ„å»ºå¥½çš„æ¶ˆæ¯åˆ—è¡¨å’Œå…ƒæ•°æ®
    """
    messages: List[Dict[str, str]]  # å®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨
    token_estimate: int  # ä¼°ç®—çš„ token æ•°
    metadata: Dict[str, Any] = field(default_factory=dict)  # å…ƒæ•°æ®


class UnifiedContextBuilder:
    """
    ç»Ÿä¸€ä¸Šä¸‹æ–‡æ„å»ºå™¨
    
    æ ¸å¿ƒèŒè´£ï¼š
    1. å°†å¯¹è¯å†å²åˆ†å±‚ï¼ˆçŸ­æœŸã€ä¸­æœŸã€é•¿æœŸï¼‰
    2. ç”Ÿæˆä¸­æœŸå¯¹è¯æ‘˜è¦
    3. æ„å»ºå¢å¼ºçš„ System Prompt
    4. æ•´åˆæ‰€æœ‰ç»„ä»¶åˆ°æœ€ç»ˆæ¶ˆæ¯åˆ—è¡¨
    5. ç®¡ç† token é¢„ç®—
    6. è¿‡æ»¤ä¸é‡è¦çš„å†å²å†…å®¹ï¼ˆURLã€ç®€å•å¯’æš„ç­‰ï¼‰
    """
    
    def __init__(
        self,
        summary_service: Optional[ConversationSummaryService] = None,
        proactive_analyzer: Optional[ProactiveDialogueStrategyAnalyzer] = None,
        history_filter: Optional[HistoryFilter] = None,
        config: Optional[ContextConfig] = None
    ):
        """
        åˆå§‹åŒ–æ„å»ºå™¨
        
        Args:
            summary_service: æ‘˜è¦æœåŠ¡ï¼ˆå¯é€‰ï¼Œé»˜è®¤åˆ›å»ºï¼‰
            proactive_analyzer: ä¸»åŠ¨ç­–ç•¥åˆ†æå™¨ï¼ˆå¯é€‰ï¼Œé»˜è®¤åˆ›å»ºï¼‰
            history_filter: å†å²è¿‡æ»¤å™¨ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€å®ä¾‹ï¼‰
            config: é…ç½®ï¼ˆå¯é€‰ï¼Œä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
        """
        self.summary_service = summary_service or ConversationSummaryService()
        self.proactive_analyzer = proactive_analyzer or ProactiveDialogueStrategyAnalyzer()
        self.config = config or ContextConfig()
        
        # åˆå§‹åŒ–å†å²è¿‡æ»¤å™¨
        if history_filter:
            self.history_filter = history_filter
        elif self.config.enable_history_filter:
            self.history_filter = get_history_filter()
        else:
            self.history_filter = None


    async def build_context(
        self,
        bot_system_prompt: str,
        conversation_history: List[Dict[str, str]],
        current_message: str,
        user_memories: Optional[List[Dict[str, Any]]] = None,
        dialogue_strategy: Optional[str] = None,
        llm_generated_summary: Optional[Dict] = None,  # æ–°å¢å‚æ•°
        chat_id: Optional[str] = None,  # ç”¨äºå†å²è¿‡æ»¤å­˜å‚¨
        user_id: Optional[str] = None  # ç”¨äºå†å²è¿‡æ»¤å­˜å‚¨
    ) -> BuilderResult:
        """
        æ„å»ºå®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡
        
        Args:
            bot_system_prompt: Bot çš„åŸå§‹äººè®¾
            conversation_history: å®Œæ•´å¯¹è¯å†å²ï¼ˆä¸åŒ…å« system promptï¼‰
            current_message: å½“å‰ç”¨æˆ·æ¶ˆæ¯
            user_memories: ç”¨æˆ·é•¿æœŸè®°å¿†åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            dialogue_strategy: å·²ç”Ÿæˆçš„å¯¹è¯ç­–ç•¥ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™ä¸é‡æ–°ç”Ÿæˆï¼‰
            llm_generated_summary: LLM ç”Ÿæˆçš„å¯¹è¯æ‘˜è¦ï¼ˆå¯é€‰ï¼‰
            chat_id: å¯¹è¯IDï¼ˆå¯é€‰ï¼Œç”¨äºå†å²è¿‡æ»¤å­˜å‚¨ï¼‰
            user_id: ç”¨æˆ·IDï¼ˆå¯é€‰ï¼Œç”¨äºå†å²è¿‡æ»¤å­˜å‚¨ï¼‰
            
        Returns:
            BuilderResult: åŒ…å«æ¶ˆæ¯åˆ—è¡¨å’Œå…ƒæ•°æ®
        """
        logger.debug(f"ğŸ” å¼€å§‹æ„å»ºä¸Šä¸‹æ–‡ï¼Œå†å²æ¶ˆæ¯æ•°: {len(conversation_history)}")
        
        # 0. åº”ç”¨å†å²è¿‡æ»¤ï¼ˆè¿‡æ»¤URLã€ç®€å•å¯’æš„ç­‰ï¼‰
        filtered_count = 0
        if self.history_filter and self.config.enable_history_filter:
            filter_result = self.history_filter.filter_history(
                conversation_history,
                chat_id=chat_id,
                user_id=user_id
            )
            conversation_history = filter_result.filtered_history
            filtered_count = len(filter_result.filtered_out)
            if filtered_count > 0:
                logger.debug(f"ğŸ” è¿‡æ»¤äº† {filtered_count} æ¡ä¸é‡è¦çš„å†å²æ¶ˆæ¯")
        
        # 1. åˆ†å‰²å¯¹è¯å†å²
        short_term, mid_term = self._split_history(conversation_history)
        logger.debug(f"åˆ†å‰²å¯¹è¯å†å²: çŸ­æœŸ={len(short_term)}æ¡, ä¸­æœŸ={len(mid_term)}æ¡")
        
        # 2. ç”Ÿæˆä¸­æœŸæ‘˜è¦ï¼ˆå¦‚æœæœ‰ä¸­æœŸå¯¹è¯ï¼‰
        mid_term_summary = None
        if mid_term:
            mid_term_summary = await self.summary_service.summarize_conversations(
                mid_term,
                use_llm=self.config.use_llm_summary,
                max_summary_length=self.config.max_summary_length
            )
            logger.debug(f"ç”Ÿæˆä¸­æœŸæ‘˜è¦: {mid_term_summary.summary_text[:50]}...")
        
        # 3. æ ¼å¼åŒ–é•¿æœŸè®°å¿†
        memory_context = self._format_memories(user_memories)
        
        # 4. ç”Ÿæˆä¸»åŠ¨ç­–ç•¥ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        proactive_guidance = ""
        if self.config.enable_proactive_strategy:
            proactive_guidance = await self._generate_proactive_guidance(
                conversation_history, user_memories
            )
        
        # 5. æ„å»ºå¢å¼ºçš„ System Prompt
        enhanced_system_prompt = self._build_enhanced_system_prompt(
            bot_system_prompt=bot_system_prompt,
            memory_context=memory_context,
            mid_term_summary=mid_term_summary,
            llm_generated_summary=llm_generated_summary,  # ä¼ é€’ LLM æ‘˜è¦
            dialogue_strategy=dialogue_strategy,
            proactive_guidance=proactive_guidance
        )
        
        # 6. æ„å»ºæœ€ç»ˆæ¶ˆæ¯åˆ—è¡¨
        messages = self._build_messages(
            enhanced_system_prompt,
            short_term,
            current_message
        )
        
        # 7. ä¼°ç®— token ä½¿ç”¨
        token_estimate = self._estimate_tokens(messages)
        
        # 8. æ£€æŸ¥ token é¢„ç®—
        if token_estimate > (self.config.max_total_tokens - self.config.reserved_output_tokens):
            logger.warning(f"Token ä½¿ç”¨ ({token_estimate}) è¶…è¿‡é¢„ç®—ï¼Œè¿›è¡Œæˆªæ–­")
            messages = self._truncate_messages(messages)
            token_estimate = self._estimate_tokens(messages)
        
        logger.info(f"ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆ: {len(messages)}æ¡æ¶ˆæ¯, ä¼°ç®—token={token_estimate}, è¿‡æ»¤äº†{filtered_count}æ¡")
        
        return BuilderResult(
            messages=messages,
            token_estimate=token_estimate,
            metadata={
                "short_term_count": len(short_term),
                "mid_term_count": len(mid_term),
                "has_mid_term_summary": mid_term_summary is not None,
                "memory_count": len(user_memories) if user_memories else 0,
                "has_proactive_guidance": bool(proactive_guidance),
                "filtered_history_count": filtered_count,
                "history_filter_enabled": self.config.enable_history_filter
            }
        )
    
    def _split_history(
        self,
        conversation_history: List[Dict[str, str]]
    ) -> Tuple[List[Dict[str, str]], List[Dict[str, str]]]:
        """
        åˆ†å‰²å¯¹è¯å†å²ä¸ºçŸ­æœŸå’Œä¸­æœŸ
        
        çŸ­æœŸï¼šæœ€è¿‘ N è½®ï¼ˆconfig.short_term_roundsï¼‰
        ä¸­æœŸï¼šç¬¬ M åˆ° N è½®ï¼ˆconfig.mid_term_start åˆ° config.mid_term_endï¼‰
        
        Returns:
            (short_term, mid_term): çŸ­æœŸå†å²å’Œä¸­æœŸå†å²
        """
        if not conversation_history:
            return [], []
        
        # è®¡ç®—çŸ­æœŸå†å²çš„æ¶ˆæ¯æ•°é‡
        # æ³¨æ„ï¼šä¸€è½®å¯¹è¯é€šå¸¸åŒ…å«ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯å’Œä¸€æ¡åŠ©æ‰‹æ¶ˆæ¯
        # ä½†æˆ‘ä»¬æŒ‰å®é™…æ¶ˆæ¯æ•°è®¡ç®—ï¼Œä¸å‡è®¾æ¯è½®æ°å¥½ä¸¤æ¡
        user_messages = [msg for msg in conversation_history if msg.get("role") == "user"]
        num_user_messages = len(user_messages)
        
        # çŸ­æœŸï¼šå–æœ€è¿‘ N è½®å¯¹è¯ï¼ˆåŸºäºç”¨æˆ·æ¶ˆæ¯æ•°ï¼‰
        if num_user_messages <= self.config.short_term_rounds:
            # æ‰€æœ‰å†å²éƒ½æ˜¯çŸ­æœŸ
            return conversation_history, []
        
        # æ‰¾åˆ°å€’æ•°ç¬¬ N æ¡ç”¨æˆ·æ¶ˆæ¯çš„ä½ç½®
        user_msg_indices = [i for i, msg in enumerate(conversation_history) if msg.get("role") == "user"]
        short_term_start_idx = user_msg_indices[-self.config.short_term_rounds]
        
        # çŸ­æœŸå†å²ä»è¯¥ä½ç½®åˆ°ç»“å°¾
        short_term = conversation_history[short_term_start_idx:]
        
        # å‰©ä½™çš„å†å²ï¼ˆä¸åŒ…æ‹¬çŸ­æœŸéƒ¨åˆ†ï¼‰
        remaining = conversation_history[:short_term_start_idx]
        
        if not remaining:
            return short_term, []
        
        # è®¡ç®—ä¸­æœŸèŒƒå›´ï¼ˆåŸºäºç”¨æˆ·æ¶ˆæ¯è½®æ•°ï¼‰
        # æ‰¾åˆ°ç¬¬ mid_term_start è½®åˆ° mid_term_end è½®çš„æ¶ˆæ¯
        remaining_user_indices = [i for i, msg in enumerate(remaining) if msg.get("role") == "user"]
        
        # å¦‚æœæœ‰è¶³å¤Ÿçš„å†å²ï¼Œæå–ä¸­æœŸ
        if len(remaining_user_indices) >= self.config.mid_term_start and self.config.mid_term_start > 0:
            start_idx = remaining_user_indices[self.config.mid_term_start - 1]
            end_user_idx = min(self.config.mid_term_end - 1, len(remaining_user_indices) - 1)
            if end_user_idx >= 0 and end_user_idx < len(remaining_user_indices):
                end_idx = remaining_user_indices[end_user_idx]
                mid_term = remaining[start_idx:end_idx + 1]
            else:
                mid_term = []
        else:
            mid_term = []
        
        return short_term, mid_term
    
    def _format_memories(self, user_memories: Optional[List[Dict[str, Any]]]) -> str:
        """
        æ ¼å¼åŒ–é•¿æœŸè®°å¿†ä¸ºæ–‡æœ¬
        
        Args:
            user_memories: ç”¨æˆ·è®°å¿†åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–çš„è®°å¿†æ–‡æœ¬
        """
        if not user_memories:
            return ""
        
        # æœ€å¤šå– max_memories æ¡
        memories_to_use = user_memories[:self.config.max_memories]
        
        memory_lines = ["ã€å…³äºè¿™ä½ç”¨æˆ·çš„è®°å¿†ã€‘"]
        for memory in memories_to_use:
            summary = memory.get("event_summary", "")
            event_date = memory.get("event_date")
            if event_date:
                event_summary = f"- ç”¨æˆ·åœ¨{event_date}è¡¨ç¤º{summary}"
            else:
                event_summary = f"- {summary}"
            if event_summary not in memory_lines:
                memory_lines.append(event_summary)
        return "\n".join(memory_lines)
    
    async def _generate_proactive_guidance(
        self,
        conversation_history: List[Dict[str, str]],
        user_memories: Optional[List[Dict[str, Any]]]
    ) -> str:
        """
        ç”Ÿæˆä¸»åŠ¨å¯¹è¯ç­–ç•¥æŒ‡å¯¼
        
        Args:
            conversation_history: å¯¹è¯å†å²
            user_memories: ç”¨æˆ·è®°å¿†
            
        Returns:
            ä¸»åŠ¨ç­–ç•¥æ–‡æœ¬
        """
        try:
            # æ„å»ºç”¨æˆ·ç”»åƒ
            user_profile = self.proactive_analyzer.analyze_user_profile(
                conversation_history, user_memories
            )
            
            # åˆ†æè¯é¢˜
            topic_analysis = self.proactive_analyzer.analyze_topic(
                conversation_history, user_profile
            )
            
            # ç”Ÿæˆä¸»åŠ¨ç­–ç•¥
            proactive_action = self.proactive_analyzer.generate_proactive_strategy(
                user_profile, topic_analysis, conversation_history, user_memories
            )
            
            # æ ¼å¼åŒ–ä¸ºæ–‡æœ¬
            guidance = self.proactive_analyzer.format_proactive_guidance(proactive_action)
            
            # æ·»åŠ ç”¨æˆ·ç”»åƒä¿¡æ¯
            profile_info = f"""
ã€å½“å‰å¯¹è¯æƒ…å¢ƒã€‘
- ç”¨æˆ·å‚ä¸åº¦ï¼š{user_profile.engagement_level.value}
- ç”¨æˆ·æƒ…ç»ªï¼š{user_profile.emotional_state}
- å…³ç³»æ·±åº¦ï¼š{user_profile.relationship_depth}/5
- ç”¨æˆ·å…´è¶£ï¼š{', '.join(user_profile.interests[:3]) if user_profile.interests else 'å¾…æ¢ç´¢'}
- å¯æ¢ç´¢è¯é¢˜ï¼š{', '.join(topic_analysis.topics_to_explore[:3]) if topic_analysis.topics_to_explore else 'æ— '}
"""
            
            return profile_info + "\n" + guidance
            
        except Exception as e:
            logger.warning(f"ç”Ÿæˆä¸»åŠ¨ç­–ç•¥å¤±è´¥: {e}")
            return ""
    
    def _build_enhanced_system_prompt(
        self,
        bot_system_prompt: str,
        memory_context: str,
        mid_term_summary: Optional[ConversationSummary],
        llm_generated_summary: Optional[Dict] = None,  # æ–°å¢ï¼šLLM ç”Ÿæˆçš„æ‘˜è¦
        dialogue_strategy: Optional[str] = None,
        proactive_guidance: str = ""
    ) -> str:
        """
        æ„å»ºå¢å¼ºçš„ System Prompt
        
        ç»“æ„ï¼š
        1. åŸå§‹äººè®¾
        2. é•¿æœŸè®°å¿†
        3. ä¸­æœŸå¯¹è¯æ‘˜è¦ï¼ˆä¼˜å…ˆä½¿ç”¨LLMç”Ÿæˆçš„æ‘˜è¦ï¼‰
        4. å¯¹è¯ç­–ç•¥
        5. ä¸»åŠ¨ç­–ç•¥
        
        Args:
            bot_system_prompt: åŸå§‹äººè®¾
            memory_context: é•¿æœŸè®°å¿†æ–‡æœ¬
            mid_term_summary: ä¸­æœŸæ‘˜è¦
            llm_generated_summary: LLM ç”Ÿæˆçš„å¯¹è¯æ‘˜è¦ï¼ˆå¯é€‰ï¼‰
            dialogue_strategy: å¯¹è¯ç­–ç•¥
            proactive_guidance: ä¸»åŠ¨ç­–ç•¥
            
        Returns:
            å¢å¼ºåçš„ system prompt
        """
        components = [bot_system_prompt]
        
        # æ·»åŠ é•¿æœŸè®°å¿†
        if memory_context:
            components.append(memory_context)
        
        # æ·»åŠ å¯¹è¯æ‘˜è¦ï¼ˆä¼˜å…ˆä½¿ç”¨ LLM ç”Ÿæˆçš„ï¼‰
        if llm_generated_summary:
            # éªŒè¯æ‘˜è¦ç»“æ„
            if not isinstance(llm_generated_summary, dict):
                logger.warning("llm_generated_summary should be a dict, skipping")
            else:
                key_elements = llm_generated_summary.get('key_elements', {})
                if not isinstance(key_elements, dict):
                    key_elements = {}
                
                # è¾…åŠ©å‡½æ•°ï¼šå¤„ç†ç©ºåˆ—è¡¨æ˜¾ç¤º
                def format_list(items):
                    return ', '.join(items) if items else 'æ— '
                
                summary_text = f"""
ã€æœ¬æ¬¡å¯¹è¯å›é¡¾ã€‘
{llm_generated_summary.get('summary_text', '')}

å…³é”®è¦ç´ ï¼š
- æ—¶é—´ï¼š{format_list(key_elements.get('time', []))}
- åœ°ç‚¹ï¼š{format_list(key_elements.get('place', []))}
- äººç‰©ï¼š{format_list(key_elements.get('people', []))}
- äº‹ä»¶ï¼š{format_list(key_elements.get('events', []))}
- æƒ…ç»ªï¼š{format_list(key_elements.get('emotions', []))}

è¯é¢˜ï¼š{format_list(llm_generated_summary.get('topics', []))}
ç”¨æˆ·çŠ¶æ€ï¼š{llm_generated_summary.get('user_state', '')}
"""
                components.append(summary_text.strip())
            
        elif mid_term_summary:
            # å›é€€åˆ°è§„åˆ™æ‘˜è¦
            summary_text = f"""
ã€æœ¬æ¬¡å¯¹è¯å›é¡¾ã€‘
{mid_term_summary.summary_text}
è®¨è®ºè¯é¢˜ï¼š{', '.join(mid_term_summary.key_topics[:3])}
"""
            if mid_term_summary.emotion_trajectory:
                summary_text += f"æƒ…ç»ªå˜åŒ–ï¼š{mid_term_summary.emotion_trajectory}\n"
            
            components.append(summary_text.strip())
        
        # æ·»åŠ ä¸»åŠ¨ç­–ç•¥ï¼ˆåœ¨å¯¹è¯ç­–ç•¥ä¹‹å‰ï¼‰
        if proactive_guidance:
            components.append(proactive_guidance)
        
        # æ·»åŠ å¯¹è¯ç­–ç•¥ï¼ˆå¦‚æœæä¾›ï¼‰
        if dialogue_strategy:
            components.append(dialogue_strategy)

        components.append("""ã€å¯¹è¯å†å²ã€‘\nä»¥ä¸‹æ˜¯æœ€è¿‘çš„å¯¹è¯è®°å½•ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡ç†è§£ç”¨æˆ·å½“å‰çš„çŠ¶æ€å’Œéœ€æ±‚ï¼š""")

        # ç”¨åŒæ¢è¡Œç¬¦è¿æ¥æ‰€æœ‰ç»„ä»¶
        enhanced_prompt = "\n\n".join(components)
        
        return enhanced_prompt
    
    def _build_messages(
        self,
        system_prompt: str,
        short_term_history: List[Dict[str, str]],
        current_message: str
    ) -> List[Dict[str, str]]:
        """
        æ„å»ºæœ€ç»ˆæ¶ˆæ¯åˆ—è¡¨
        
        ç»“æ„ï¼š
        1. System message (enhanced prompt)
        2. Short-term history
        3. Current user message
        
        Args:
            system_prompt: å¢å¼ºçš„ system prompt
            short_term_history: çŸ­æœŸå†å²
            current_message: å½“å‰æ¶ˆæ¯
            
        Returns:
            å®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨
        """
        messages = []
        # 1. System prompt
        messages.append({
            "role": "system",
            "content": system_prompt
        })
        # 2. Short-term historyï¼ˆè¿‡æ»¤ï¼Œåªä¿ç•™ user/assistantï¼‰
        for msg in short_term_history:
            if msg.get("role") in ("user", "assistant"):
                messages.append(msg)
        # 3. Current message
        messages.append({
            "role": "user",
            "content": current_message
        })
        
        return messages
    
    def _estimate_tokens(self, messages: List[Dict[str, str]]) -> int:
        """
        ä¼°ç®—æ¶ˆæ¯åˆ—è¡¨çš„ token æ•°
        
        ç®€å•ä¼°ç®—ï¼šä¸­æ–‡çº¦1.5å­—ç¬¦/tokenï¼Œè‹±æ–‡çº¦4å­—ç¬¦/token
        ä½¿ç”¨ round() ä»¥é¿å…æˆªæ–­å¯¼è‡´çš„ä½ä¼°
        """
        total_tokens = 0
        
        for msg in messages:
            content = msg.get("content", "")
            
            # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦
            chinese_chars = sum(1 for c in content if '\u4e00' <= c <= '\u9fff')
            other_chars = len(content) - chinese_chars
            
            # ä¼°ç®—ï¼ˆä½¿ç”¨ round é¿å…æˆªæ–­ï¼‰
            tokens = round(chinese_chars / 1.5 + other_chars / 4)
            
            # æ¶ˆæ¯æ ¼å¼å¼€é”€
            total_tokens += tokens + 4
        
        return total_tokens
    
    def _truncate_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        æˆªæ–­æ¶ˆæ¯ä»¥é€‚åº” token é¢„ç®—
        
        ç­–ç•¥ï¼š
        1. ä¿ç•™ system prompt
        2. ä¿ç•™å½“å‰ç”¨æˆ·æ¶ˆæ¯
        3. ä»çŸ­æœŸå†å²ä¸­ç§»é™¤æœ€æ—©çš„æ¶ˆæ¯
        
        Args:
            messages: åŸå§‹æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            æˆªæ–­åçš„æ¶ˆæ¯åˆ—è¡¨
        """
        if len(messages) <= 2:
            # åªæœ‰ system å’Œå½“å‰æ¶ˆæ¯ï¼Œæ— æ³•å†æˆªæ–­
            return messages
        
        # åˆ†ç¦»ç»„ä»¶
        system_msg = messages[0] if messages and messages[0].get("role") == "system" else None
        current_msg = messages[-1] if messages and messages[-1].get("role") == "user" else None
        history = messages[1:-1] if len(messages) > 2 else []
        
        # é€æ­¥ç§»é™¤å†å²æ¶ˆæ¯ç›´åˆ°æ»¡è¶³é¢„ç®—
        budget = self.config.max_total_tokens - self.config.reserved_output_tokens
        
        # æ„å»ºæµ‹è¯•æ¶ˆæ¯åˆ—è¡¨
        test_messages = []
        if system_msg:
            test_messages.append(system_msg)
        test_messages.extend(history)
        if current_msg:
            test_messages.append(current_msg)
        
        while history and self._estimate_tokens(test_messages) > budget:
            history.pop(0)
            logger.debug(f"ç§»é™¤ä¸€æ¡å†å²æ¶ˆæ¯ï¼Œå‰©ä½™: {len(history)}")
            # é‡å»ºæµ‹è¯•æ¶ˆæ¯åˆ—è¡¨
            test_messages = []
            if system_msg:
                test_messages.append(system_msg)
            test_messages.extend(history)
            if current_msg:
                test_messages.append(current_msg)
        
        # é‡æ–°ç»„åˆ
        result = []
        if system_msg:
            result.append(system_msg)
        result.extend(history)
        if current_msg:
            result.append(current_msg)
        
        return result
    
    def get_token_budget_info(self, result: BuilderResult) -> Dict[str, Any]:
        """
        è·å– token é¢„ç®—ä½¿ç”¨æƒ…å†µ
        
        Args:
            result: æ„å»ºç»“æœ
            
        Returns:
            é¢„ç®—ä¿¡æ¯å­—å…¸
        """
        return {
            "estimated_tokens": result.token_estimate,
            "max_tokens": self.config.max_total_tokens,
            "reserved_for_output": self.config.reserved_output_tokens,
            "available_for_context": self.config.max_total_tokens - self.config.reserved_output_tokens,
            "usage_percentage": (result.token_estimate / (self.config.max_total_tokens - self.config.reserved_output_tokens)) * 100
        }

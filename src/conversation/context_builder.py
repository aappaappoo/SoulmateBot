"""
Unified Context Builder - ç»Ÿä¸€ä¸Šä¸‹æ–‡æ„å»ºå™¨

è´Ÿè´£æ„å»ºå‘é€ç»™ LLM çš„å®Œæ•´æ¶ˆæ¯ç»“æ„ï¼Œé‡‡ç”¨åˆ†å±‚æ–¹å¼ï¼š

æ¶ˆæ¯ç»“æ„ï¼š
1. System Promptï¼ˆåŒ…å«äººè®¾ + é•¿æœŸè®°å¿† + å¯¹è¯ç­–ç•¥ï¼‰
2. çŸ­æœŸå¯¹è¯å†å²ï¼ˆæœ€è¿‘ 3-5 è½®å®Œæ•´å†…å®¹ï¼‰
3. å½“å‰ç”¨æˆ·æ¶ˆæ¯

åŠŸèƒ½ï¼š
- åˆ†å‰²å†å²ï¼ˆçŸ­æœŸ vs ä¸­æœŸï¼‰
- ç”Ÿæˆä¸­æœŸæ‘˜è¦ï¼ˆæ”¯æŒç¼“å­˜ï¼‰
- æ•´åˆæ‰€æœ‰ä¸Šä¸‹æ–‡
- æ„å»ºæœ€ç»ˆæ¶ˆæ¯åˆ—è¡¨
- Token é¢„ç®—ç®¡ç†
- å†å²å¯¹è¯è¿‡æ»¤ï¼ˆURLã€ç®€å•å¯’æš„ç­‰ï¼‰
"""
from typing import List, Dict, Optional, Any, Tuple
from dataclasses import dataclass, field
from loguru import logger

from .summary_service import ConversationSummaryService, ConversationSummary
from .proactive_strategy import (
    ProactiveDialogueStrategyAnalyzer,
)
from src.utils.history_filter import HistoryFilter, get_history_filter

@dataclass
class ContextConfig:
    """
    ä¸Šä¸‹æ–‡é…ç½®
    
    ç”¨äºæ§åˆ¶ä¸Šä¸‹æ–‡æ„å»ºçš„å„é¡¹å‚æ•°
    """
    # å¯¹è¯å†å²åˆ†å±‚
    short_term_rounds: int = 5  # çŸ­æœŸå†å²è½®æ•°ï¼ˆæœ€è¿‘ N è½®ï¼‰
    mid_term_start: int = 5  # ä¸­æœŸå†å²å¼€å§‹è½®æ¬¡
    mid_term_end: int = 20  # ä¸­æœŸå†å²ç»“æŸè½®æ¬¡

    # é•¿æœŸè®°å¿†
    max_memories: int = 10  # æœ€å¤šåŒ…å«çš„é•¿æœŸè®°å¿†æ•°é‡

    # Token é¢„ç®—
    max_total_tokens: int = 8000  # æ€» token é¢„ç®—
    reserved_output_tokens: int = 1000  # ä¸ºè¾“å‡ºä¿ç•™çš„ token

    # æ‘˜è¦é€‰é¡¹
    use_llm_summary: bool = False  # æ˜¯å¦ä½¿ç”¨ LLM æ‘˜è¦ï¼ˆæ¶ˆè€— tokenï¼‰
    max_summary_length: int = 200  # æ‘˜è¦æœ€å¤§é•¿åº¦

    # ä¸»åŠ¨ç­–ç•¥
    enable_proactive_strategy: bool = True  # æ˜¯å¦å¯ç”¨ä¸»åŠ¨ç­–ç•¥

    # å†å²è¿‡æ»¤é€‰é¡¹
    enable_history_filter: bool = True  # æ˜¯å¦å¯ç”¨å†å²è¿‡æ»¤ï¼ˆè¿‡æ»¤URLã€ç®€å•å¯’æš„ç­‰ï¼‰


@dataclass
class BuilderResult:
    """
    æ„å»ºå™¨ç»“æœ
    
    åŒ…å«æ„å»ºå¥½çš„æ¶ˆæ¯åˆ—è¡¨å’Œå…ƒæ•°æ®
    """
    messages: List[Dict[str, str]]  # å®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨
    token_estimate: int  # ä¼°ç®—çš„ token æ•°
    metadata: Dict[str, Any] = field(default_factory=dict)  # å…ƒæ•°æ®


class UnifiedContextBuilder:
    """
    ç»Ÿä¸€ä¸Šä¸‹æ–‡æ„å»ºå™¨
    
    æ ¸å¿ƒèŒè´£ï¼š
    1. å°†å¯¹è¯å†å²åˆ†å±‚ï¼ˆçŸ­æœŸã€ä¸­æœŸã€é•¿æœŸï¼‰
    2. ç”Ÿæˆä¸­æœŸå¯¹è¯æ‘˜è¦
    3. æ„å»ºå¢å¼ºçš„ System Prompt
    4. æ•´åˆæ‰€æœ‰ç»„ä»¶åˆ°æœ€ç»ˆæ¶ˆæ¯åˆ—è¡¨
    5. ç®¡ç† token é¢„ç®—
    6. è¿‡æ»¤ä¸é‡è¦çš„å†å²å†…å®¹ï¼ˆURLã€ç®€å•å¯’æš„ç­‰ï¼‰
    """

    def __init__(
            self,
            summary_service: Optional[ConversationSummaryService] = None,
            proactive_analyzer: Optional[ProactiveDialogueStrategyAnalyzer] = None,
            history_filter: Optional[HistoryFilter] = None,
            config: Optional[ContextConfig] = None
    ):
        """
        åˆå§‹åŒ–æ„å»ºå™¨
        
        Args:
            summary_service: æ‘˜è¦æœåŠ¡ï¼ˆå¯é€‰ï¼Œé»˜è®¤åˆ›å»ºï¼‰
            proactive_analyzer: ä¸»åŠ¨ç­–ç•¥åˆ†æå™¨ï¼ˆå¯é€‰ï¼Œé»˜è®¤åˆ›å»ºï¼‰
            history_filter: å†å²è¿‡æ»¤å™¨ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€å®ä¾‹ï¼‰
            config: é…ç½®ï¼ˆå¯é€‰ï¼Œä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
        """
        self.summary_service = summary_service or ConversationSummaryService()
        self.proactive_analyzer = proactive_analyzer or ProactiveDialogueStrategyAnalyzer()
        self.config = config or ContextConfig()

        # åˆå§‹åŒ–å†å²è¿‡æ»¤å™¨
        if history_filter:
            self.history_filter = history_filter
        elif self.config.enable_history_filter:
            self.history_filter = get_history_filter()
        else:
            self.history_filter = None

    async def build_context(
            self,
            bot_system_prompt: str,
            conversation_history: List[Dict[str, str]],
            current_message: str,
            user_memories: Optional[List[Dict[str, Any]]] = None,
            dialogue_strategy: Optional[str] = None,
            llm_generated_summary: Optional[Dict] = None,  # æ–°å¢å‚æ•°
            chat_id: Optional[str] = None,  # ç”¨äºå†å²è¿‡æ»¤å­˜å‚¨
            user_id: Optional[str] = None,  # ç”¨äºå†å²è¿‡æ»¤å­˜å‚¨
            persona: Optional[Any] = None  # â† æ–°å¢
    ) -> BuilderResult:
        """
        æ„å»ºå®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡
        
        Args:
            bot_system_prompt: Bot çš„åŸå§‹äººè®¾
            conversation_history: å®Œæ•´å¯¹è¯å†å²ï¼ˆä¸åŒ…å« system promptï¼‰
            current_message: å½“å‰ç”¨æˆ·æ¶ˆæ¯
            user_memories: ç”¨æˆ·é•¿æœŸè®°å¿†åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            dialogue_strategy: å·²ç”Ÿæˆçš„å¯¹è¯ç­–ç•¥ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™ä¸é‡æ–°ç”Ÿæˆï¼‰
            llm_generated_summary: LLM ç”Ÿæˆçš„å¯¹è¯æ‘˜è¦ï¼ˆå¯é€‰ï¼‰
            chat_id: å¯¹è¯IDï¼ˆå¯é€‰ï¼Œç”¨äºå†å²è¿‡æ»¤å­˜å‚¨ï¼‰
            user_id: ç”¨æˆ·IDï¼ˆå¯é€‰ï¼Œç”¨äºå†å²è¿‡æ»¤å­˜å‚¨ï¼‰
            
        Returns:
            BuilderResult: åŒ…å«æ¶ˆæ¯åˆ—è¡¨å’Œå…ƒæ•°æ®
        """
        logger.debug(f"ğŸ” å¼€å§‹æ„å»ºä¸Šä¸‹æ–‡ï¼Œå†å²æ¶ˆæ¯æ•°: {len(conversation_history)}")

        # 0. åº”ç”¨å†å²è¿‡æ»¤ï¼ˆè¿‡æ»¤URLã€ç®€å•å¯’æš„ç­‰ï¼‰
        filtered_count = 0
        if self.history_filter and self.config.enable_history_filter:
            filter_result = self.history_filter.filter_history(
                conversation_history,
                chat_id=chat_id,
                user_id=user_id
            )
            conversation_history = filter_result.filtered_history
            filtered_count = len(filter_result.filtered_out)
            if filtered_count > 0:
                logger.debug(f"ğŸ” è¿‡æ»¤äº† {filtered_count} æ¡ä¸é‡è¦çš„å†å²æ¶ˆæ¯")

        # 1. åˆ†å‰²å¯¹è¯å†å²
        short_term, mid_term = self._split_history(conversation_history)
        logger.debug(f"åˆ†å‰²å¯¹è¯å†å²: çŸ­æœŸ={len(short_term)}æ¡, ä¸­æœŸ={len(mid_term)}æ¡")

        # 2. ç”Ÿæˆä¸­æœŸæ‘˜è¦ï¼ˆå¦‚æœæœ‰ä¸­æœŸå¯¹è¯ï¼‰
        mid_term_summary = None
        if mid_term:
            mid_term_summary = await self.summary_service.summarize_conversations(
                mid_term,
                use_llm=self.config.use_llm_summary,
                max_summary_length=self.config.max_summary_length
            )
            logger.debug(f"ç”Ÿæˆä¸­æœŸæ‘˜è¦: {mid_term_summary.summary_text[:50]}...")

        # 3. æ ¼å¼åŒ–é•¿æœŸè®°å¿†
        memory_context = self._format_memories(user_memories)

        # 4. ç”Ÿæˆä¸»åŠ¨ç­–ç•¥ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        proactive_guidance = ""
        if self.config.enable_proactive_strategy:
            proactive_guidance = await self._generate_proactive_guidance(
                conversation_history, user_memories
            )

        # 5. æ„å»ºå¢å¼ºçš„ System Promptï¼ˆåŒ…å«å¯¹è¯å†å²ï¼‰
        enhanced_system_prompt = self._build_enhanced_system_prompt(
            bot_system_prompt=bot_system_prompt,
            memory_context=memory_context,
            mid_term_summary=mid_term_summary,
            llm_generated_summary=llm_generated_summary,  # ä¼ é€’ LLM æ‘˜è¦
            dialogue_strategy=dialogue_strategy,
            proactive_guidance=proactive_guidance,
            short_term_history=short_term,
            persona=persona
        )
        # 6. æ„å»ºæœ€ç»ˆæ¶ˆæ¯åˆ—è¡¨ï¼ˆä»… system + user ä¸¤æ¡æ¶ˆæ¯ï¼‰
        messages = self._build_messages(
            enhanced_system_prompt,
            short_term,
            current_message
        )

        # 7. ä¼°ç®— token ä½¿ç”¨
        token_estimate = self._estimate_tokens(messages)

        # 8. æ£€æŸ¥ token é¢„ç®—
        if token_estimate > (self.config.max_total_tokens - self.config.reserved_output_tokens):
            logger.warning(f"Token ä½¿ç”¨ ({token_estimate}) è¶…è¿‡é¢„ç®—ï¼Œè¿›è¡Œæˆªæ–­")
            messages = self._truncate_messages(messages)
            token_estimate = self._estimate_tokens(messages)

        logger.info(f"ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆ: {len(messages)}æ¡æ¶ˆæ¯, ä¼°ç®—token={token_estimate}, è¿‡æ»¤äº†{filtered_count}æ¡")

        return BuilderResult(
            messages=messages,
            token_estimate=token_estimate,
            metadata={
                "short_term_count": len(short_term),
                "mid_term_count": len(mid_term),
                "has_mid_term_summary": mid_term_summary is not None,
                "memory_count": len(user_memories) if user_memories else 0,
                "has_proactive_guidance": bool(proactive_guidance),
                "filtered_history_count": filtered_count,
                "history_filter_enabled": self.config.enable_history_filter
            }
        )

    def _split_history(
            self,
            conversation_history: List[Dict[str, str]]
    ) -> Tuple[List[Dict[str, str]], List[Dict[str, str]]]:
        """
        åˆ†å‰²å¯¹è¯å†å²ä¸ºçŸ­æœŸå’Œä¸­æœŸ
        
        çŸ­æœŸï¼šæœ€è¿‘ N è½®ï¼ˆconfig.short_term_roundsï¼‰
        ä¸­æœŸï¼šç¬¬ M åˆ° N è½®ï¼ˆconfig.mid_term_start åˆ° config.mid_term_endï¼‰
        
        Returns:
            (short_term, mid_term): çŸ­æœŸå†å²å’Œä¸­æœŸå†å²
        """
        if not conversation_history:
            return [], []

        # è®¡ç®—çŸ­æœŸå†å²çš„æ¶ˆæ¯æ•°é‡
        # æ³¨æ„ï¼šä¸€è½®å¯¹è¯é€šå¸¸åŒ…å«ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯å’Œä¸€æ¡åŠ©æ‰‹æ¶ˆæ¯
        # ä½†æˆ‘ä»¬æŒ‰å®é™…æ¶ˆæ¯æ•°è®¡ç®—ï¼Œä¸å‡è®¾æ¯è½®æ°å¥½ä¸¤æ¡
        user_messages = [msg for msg in conversation_history if msg.get("role") == "user"]
        num_user_messages = len(user_messages)

        # çŸ­æœŸï¼šå–æœ€è¿‘ N è½®å¯¹è¯ï¼ˆåŸºäºç”¨æˆ·æ¶ˆæ¯æ•°ï¼‰
        if num_user_messages <= self.config.short_term_rounds:
            # æ‰€æœ‰å†å²éƒ½æ˜¯çŸ­æœŸ
            return conversation_history, []

        # æ‰¾åˆ°å€’æ•°ç¬¬ N æ¡ç”¨æˆ·æ¶ˆæ¯çš„ä½ç½®
        user_msg_indices = [i for i, msg in enumerate(conversation_history) if msg.get("role") == "user"]
        short_term_start_idx = user_msg_indices[-self.config.short_term_rounds]

        # çŸ­æœŸå†å²ä»è¯¥ä½ç½®åˆ°ç»“å°¾
        short_term = conversation_history[short_term_start_idx:]

        # å‰©ä½™çš„å†å²ï¼ˆä¸åŒ…æ‹¬çŸ­æœŸéƒ¨åˆ†ï¼‰
        remaining = conversation_history[:short_term_start_idx]

        if not remaining:
            return short_term, []
        remaining_user_indices = [i for i, msg in enumerate(remaining) if msg.get("role") == "user"]
        num_remaining_users = len(remaining_user_indices)
        # æˆ‘ä»¬åº”è¯¥ä» remaining çš„æœ«å°¾å¼€å§‹
        # ç®€åŒ–ç†è§£ï¼šä¸­æœŸå°±æ˜¯ remaining ä¸­æœ€è¿‘çš„ (mid_term_end - short_term_rounds) è½®
        mid_term_rounds = min(
            self.config.mid_term_end - self.config.short_term_rounds,
            num_remaining_users
        )

        if mid_term_rounds > 0:
            mid_term_start_idx = remaining_user_indices[-mid_term_rounds]
            mid_term = remaining[mid_term_start_idx:]
        else:
            mid_term = []
        return short_term, mid_term

    def _format_memories(self, user_memories: Optional[List[Dict[str, Any]]]) -> str:
        """
        æ ¼å¼åŒ–é•¿æœŸè®°å¿†ä¸ºæ–‡æœ¬
        
        Args:
            user_memories: ç”¨æˆ·è®°å¿†åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–çš„è®°å¿†æ–‡æœ¬
        """
        if not user_memories:
            return ""

        # é•¿æœŸè®°å¿†æå–é€»è¾‘
        memories_to_use = user_memories[:self.config.max_memories]
        memory_lines = ["ã€å…³äºè¿™ä½ç”¨æˆ·çš„è®°å¿†ã€‘"]
        for memory in memories_to_use:
            summary = memory.get("event_summary", "")
            event_date = memory.get("event_date")
            if event_date:
                event_summary = f"- ç”¨æˆ·åœ¨{event_date}æ—¶é—´è¡¨ç¤º{summary}"
            else:
                event_summary = f"- {summary}"
            if event_summary not in memory_lines:
                memory_lines.append(event_summary)
        return "\n".join(memory_lines)

    async def _generate_proactive_guidance(
            self,
            conversation_history: List[Dict[str, str]],
            user_memories: Optional[List[Dict[str, Any]]]
    ) -> str:
        """
        ç”Ÿæˆä¸»åŠ¨å¯¹è¯ç­–ç•¥æŒ‡å¯¼
        
        Args:
            conversation_history: å¯¹è¯å†å²
            user_memories: ç”¨æˆ·è®°å¿†
            
        Returns:
            ä¸»åŠ¨ç­–ç•¥æ–‡æœ¬
        """
        try:
            # æ„å»ºç”¨æˆ·ç”»åƒ
            user_profile = self.proactive_analyzer.analyze_user_profile(
                conversation_history, user_memories
            )
            # åˆ†æè¯é¢˜
            topic_analysis = self.proactive_analyzer.analyze_topic(
                conversation_history, user_profile
            )
            # ç”Ÿæˆä¸»åŠ¨ç­–ç•¥
            proactive_action = self.proactive_analyzer.generate_proactive_strategy(
                user_profile, topic_analysis, conversation_history, user_memories
            )
            # æ ¼å¼åŒ–ä¸ºæ–‡æœ¬
            guidance = self.proactive_analyzer.format_proactive_guidance(proactive_action)
            # æ·»åŠ ç”¨æˆ·ç”»åƒä¿¡æ¯
            profile_info = f"""
ã€å½“å‰å¯¹è¯æƒ…å¢ƒã€‘
- ç”¨æˆ·å‚ä¸åº¦ï¼š{user_profile.engagement_level.value}
- ç”¨æˆ·æƒ…ç»ªï¼š{user_profile.emotional_state}
- å…³ç³»æ·±åº¦ï¼š{user_profile.relationship_depth}/5
- ç”¨æˆ·å…´è¶£ï¼š{', '.join(user_profile.interests[:3]) if user_profile.interests else 'å¾…æ¢ç´¢'}
- å¯æ¢ç´¢è¯é¢˜ï¼š{', '.join(topic_analysis.topics_to_explore[:3]) if topic_analysis.topics_to_explore else 'æ— '}
"""
            return profile_info + "\n" + guidance

        except Exception as e:
            logger.warning(f"ç”Ÿæˆä¸»åŠ¨ç­–ç•¥å¤±è´¥: {e}")
            return ""

    #
    def _build_enhanced_system_prompt(
            self,
            bot_system_prompt: str,
            memory_context: str,
            mid_term_summary: Optional[ConversationSummary],
            llm_generated_summary: Optional[Dict] = None,  # æ–°å¢ï¼šLLM ç”Ÿæˆçš„æ‘˜è¦
            dialogue_strategy: Optional[str] = None,
            proactive_guidance: str = "",
            short_term_history: Optional[List[Dict[str, str]]] = None,
            persona: Optional[Any] = None
    ) -> str:
        """
        æ„å»ºå¢å¼ºçš„ System Prompt
        """
        components = [bot_system_prompt]

        # ====================  æ•´åˆæ‰€æœ‰è®°å¿†åˆ°ä¸€ä¸ªå— ====================
        memory_sections = []

        # 1. é•¿æœŸå†å²é‡è¦è®°å¿†
        if memory_context:
            # memory_context å·²ç»æ˜¯ "ã€å…³äºè¿™ä½ç”¨æˆ·çš„è®°å¿†ã€‘\n- xxx\n- xxx" æ ¼å¼
            # å»æ‰åŸæœ‰æ ‡é¢˜ï¼Œåªä¿ç•™å†…å®¹
            memory_lines = memory_context.split('\n')
            if memory_lines and memory_lines[0].startswith('ã€'):
                memory_lines = memory_lines[1:]  # å»æ‰ç¬¬ä¸€è¡Œæ ‡é¢˜
            if memory_lines:
                memory_sections.append("ã€å†å²é‡è¦è®°å¿†ã€‘\n" + '\n'.join(memory_lines))

        # 2. ä¸­æœŸæ‘˜è¦è®°å¿†
        summary_text = ""
        if llm_generated_summary and isinstance(llm_generated_summary, dict):
            key_elements = llm_generated_summary.get('key_elements', {})
            if not isinstance(key_elements, dict):
                key_elements = {}

            def format_list(items):
                return ', '.join(items) if items else 'æ— '

            summary_text = f"""ã€ä¸­æœŸæ‘˜è¦è®°å¿†ã€‘
{llm_generated_summary.get('summary_text', '')}
å…³é”®è¦ç´ ï¼šæ—¶é—´={format_list(key_elements.get('time', []))}ï¼Œåœ°ç‚¹={format_list(key_elements.get('place', []))}ï¼Œäººç‰©={format_list(key_elements.get('people', []))}
è¯é¢˜ï¼š{format_list(llm_generated_summary.get('topics', []))}
ç”¨æˆ·çŠ¶æ€ï¼š{llm_generated_summary.get('user_state', '')}"""
        elif mid_term_summary:
            summary_text = f"""ã€ä¸­æœŸæ‘˜è¦è®°å¿†ã€‘
{mid_term_summary.summary_text}
è®¨è®ºè¯é¢˜ï¼š{', '.join(mid_term_summary.key_topics[:3])}"""
            if mid_term_summary.emotion_trajectory:
                summary_text += f"\næƒ…ç»ªå˜åŒ–ï¼š{mid_term_summary.emotion_trajectory}"
        if summary_text:
            memory_sections.append(summary_text.strip())

        # 3. è¿‘æœŸå¯¹è¯è®°å½•
        if short_term_history:
            history_text = self._format_history_for_system_prompt(short_term_history)
            if history_text:
                # æ›¿æ¢åŸæœ‰æ ‡é¢˜ä¸ºç»Ÿä¸€æ ¼å¼
                history_text = history_text.replace(
                    "ã€å†å²å¯¹è¯ - ä»…å‚è€ƒï¼Œç¦æ­¢æ¨¡ä»¿æ ¼å¼ã€‘",
                    "ã€è¿‘æœŸå¯¹è¯è®°å½•ï½œç”±ä¸‹å¾€ä¸Šå¯¹åº”çš„æ—¶é—´æ˜¯ä»è¿œåˆ°è¿‘ã€‘"
                )
                memory_sections.append(history_text)

        # æ•´åˆæ‰€æœ‰è®°å¿†åˆ°ä¸€ä¸ªå—
        if memory_sections:
            unified_memory_block = """
=========================
å¯¹è¯ç›¸å…³è®°å¿†
=========================
""" + "\n\n".join(memory_sections)
            components.append(unified_memory_block)

        # ==================== å¯¹è¯ç­–ç•¥ç®¡ç†ï¼ˆæ•´åˆå—ï¼‰ ====================
        strategy_sections = []

        # 1. å½“å‰å¯¹è¯æƒ…å¢ƒï¼ˆä» proactive_guidance ä¸­æå–ï¼‰
        if proactive_guidance:
            strategy_sections.append(proactive_guidance.strip())

        if dialogue_strategy:
            strategy_sections.append(dialogue_strategy.strip())
        if persona and hasattr(persona, 'emotional_response') and persona.emotional_response:
            p = persona
            emotion_sections = []
            emotion_sections.append("ã€æƒ…ç»ªåº”å¯¹ç­–ç•¥ã€‘")
            if p.emotional_response.get("user_sad"):
                lines = '\n - '.join(p.emotional_response['user_sad'])
                emotion_sections.append(f"å½“ç”¨æˆ·éš¾è¿‡æ—¶ï¼š\n - {lines}")
            if p.emotional_response.get("user_angry"):
                lines = '\n - '.join(p.emotional_response['user_angry'])
                emotion_sections.append(f"å½“ç”¨æˆ·ç”Ÿæ°”æ—¶ï¼š\n - {lines}")
            if p.emotional_response.get("user_happy"):
                lines = '\n - '.join(p.emotional_response['user_happy'])
                emotion_sections.append(f"å½“ç”¨æˆ·å¼€å¿ƒæ—¶ï¼š\n - {lines}")
            if len(emotion_sections) > 1:
                strategy_sections.append('\n'.join(emotion_sections))

        if strategy_sections:
            unified_strategy_block = """
=========================
å¯¹è¯ç­–ç•¥ç®¡ç†
=========================
'ã€å®‰å…¨å¯¹è¯ç­–ç•¥ã€‘\n'
'**éœ€è¦ä¸»åŠ¨å›é¿çš„è¯é¢˜**ï¼š'
'-æ”¿æ²»è¯é¢˜'
'-æ­§è§†å†…å®¹'
'-æš´åŠ›å†…å®¹'
'-æœªæˆå¹´æ€§å†…å®¹'
'-äººèº«æ”»å‡»'
'**é«˜åº¦è­¦æƒ•è¦æ±‚ä¸»åŠ¨å…³é—­è¯é¢˜çš„å…³é”®è¯**ï¼š'
'-è‡ªæ€'
'-æŠ‘éƒ'
'-è°‹æ€'
'**ç‰¹æ®Šçš„å“åº”ç­–ç•¥**ï¼š'
'-é‡åˆ°ä¸¥è‚ƒé—®é¢˜æ”¶èµ·å¹½é»˜'
'-è¡¨è¾¾çœŸè¯šçš„å…³å¿ƒ'
'-ä¸ç”¨å¹½é»˜æ©ç›–ä¸¥é‡é—®é¢˜\n'
"""
            unified_strategy_block += "\n\n".join(strategy_sections)
            components.append(unified_strategy_block)
        json_format_instruction = self._get_json_format_instruction()
        components.append(json_format_instruction)
        enhanced_prompt = "\n\n".join(components)
        return enhanced_prompt

    def _format_history_for_system_prompt(
            self,
            short_term_history: List[Dict[str, str]]
    ) -> str:
        """
        å°†çŸ­æœŸå¯¹è¯å†å²æ ¼å¼åŒ–ä¸ºåµŒå…¥ system prompt çš„æ–‡æœ¬
        ä½¿ç”¨ç‰¹æ®Šæ ‡è®°é˜²æ­¢ LLM æ¨¡ä»¿æ­¤æ ¼å¼è¾“å‡º
        Args:
            short_term_history: çŸ­æœŸå¯¹è¯å†å²
        Returns:
            æ ¼å¼åŒ–çš„å†å²æ–‡æœ¬
        """
        if not short_term_history:
            return ""

        history_lines = []
        for msg in short_term_history:
            role = msg.get("role", "").lower()
            content = msg.get("content", "")
            if role == "user":
                history_lines.append(f"User: {content}")
            elif role == "assistant":
                content = content.replace("[MSG_SPLIT]", "")
                history_lines.append(f"Assistant: {content}")
        if not history_lines:
            return ""

        history_text = """
ã€å†å²å¯¹è¯ - ä»…å‚è€ƒï¼Œç¦æ­¢æ¨¡ä»¿æ ¼å¼ã€‘
<history>
""" + "\n".join(history_lines) + """
</history>
âš ï¸ æ³¨æ„ï¼šä¸Šæ–¹å†å²ä»…ç”¨äºç†è§£ä¸Šä¸‹æ–‡ï¼Œä½ çš„è¾“å‡ºå¿…é¡»æ˜¯JSON"""
        return history_text

    def _get_json_format_instruction(self) -> str:
        """
        è·å–å¼ºåˆ¶ JSON æ ¼å¼è¾“å‡ºæŒ‡ä»¤
        
        Returns:
            JSON æ ¼å¼æŒ‡ä»¤æ–‡æœ¬
        """
        return """
=========================
å¼ºåˆ¶è¾“å‡ºæ ¼å¼
=========================   
ä½ å¿…é¡»ä¸”åªèƒ½è¿”å›ä»¥ä¸‹JSONæ ¼å¼ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡æœ¬ï¼š
```json
    {{
    "intent": "direct_response" | "single_agent" | "multi_agent",
    "agents": [],
    "reasoning": "åˆ¤æ–­ç†ç”±",
    
    "conversation_summary": {{
        "summary_text": "ç»¼åˆæ•´ä¸ªå¯¹è¯çš„æ‘˜è¦æ–‡æœ¬ï¼ˆ100å­—ä»¥å†…ï¼‰",
        "key_elements": {{
            "time": ["æ—¶é—´ç‚¹1", "æ—¶é—´ç‚¹2"],
            "place": ["åœ°ç‚¹1", "åœ°ç‚¹2"],
            "people": ["äººç‰©1", "äººç‰©2"],
            "events": ["äº‹ä»¶1", "äº‹ä»¶2"],
            "emotions": ["æƒ…ç»ª1", "æƒ…ç»ª2"]
        }},
        "topics": ["è¯é¢˜1", "è¯é¢˜2", "è¯é¢˜3"],
        "user_state": "ç”¨æˆ·å½“å‰çŠ¶æ€æè¿°"
    }},
    
    "direct_reply": "çº¯æ–‡æœ¬å›å¤å†…å®¹ï¼ŒæŒ‰ç…§ä¸Šé¢å›å¤å†…å®¹æ ¼å¼è¯´æ˜è¿›è¡Œ",
    "emotion": "happy" | "gentle" | "sad" | "excited" | "angry" | "crying" | null,
    "emotion_description": "è¯¦ç»†çš„è¯­æ°”æè¿°ï¼Œå¦‚ï¼šå¼€å¿ƒã€è½»å¿«ï¼Œè¯­é€Ÿç¨å¿«ï¼Œè¯­è°ƒä¸Šæ‰¬" | null,
    "memory": {{
        "is_important": false,
        "importance_level": "low" | "medium" | "high" | null,
        "event_type": "preference" | "birthday" | "goal" | "emotion" | "life_event" | null,
        "event_summary": "äº‹ä»¶æ‘˜è¦" | null,
        "keywords": [],
        "event_date": "YYYY-MM-DD" | null,
        "raw_date_expression": "åŸå§‹æ—¶é—´è¡¨è¾¾" | null
    }}
}}
```
"""

    def _build_messages(
            self,
            system_prompt: str,
            short_term_history: List[Dict[str, str]],  # ä¿ç•™æ­¤å‚æ•°ç”¨äºå‘åå…¼å®¹å’Œæ¥å£ä¸€è‡´æ€§
            current_message: str
    ) -> List[Dict[str, str]]:
        """
        æ„å»ºæœ€ç»ˆæ¶ˆæ¯åˆ—è¡¨ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œä»…2æ¡æ¶ˆæ¯ï¼‰
        
        ç»“æ„ï¼š
        1. System message (åŒ…å«æ‰€æœ‰ä¸Šä¸‹æ–‡ï¼šäººè®¾ã€è®°å¿†ã€æ‘˜è¦ã€å¯¹è¯å†å²ã€JSONæ ¼å¼æŒ‡ä»¤)
        2. å½“å‰ user messageï¼ˆä»…å½“å‰è¾“å…¥ï¼‰
        
        æ³¨æ„ï¼šçŸ­æœŸå†å²å·²ç»åµŒå…¥åˆ° system_prompt ä¸­ï¼Œä¸å†ä½œä¸ºå•ç‹¬æ¶ˆæ¯ã€‚
        short_term_history å‚æ•°ä¿ç•™ç”¨äºï¼š
        1. å‘åå…¼å®¹ - é¿å…ä¿®æ”¹æ‰€æœ‰è°ƒç”¨æ–¹ä»£ç 
        2. æ¥å£ä¸€è‡´æ€§ - ä¸ build_context è°ƒç”¨æ¨¡å¼ä¿æŒä¸€è‡´
        
        Args:
            system_prompt: å¢å¼ºçš„ system promptï¼ˆå·²åŒ…å«å¯¹è¯å†å²ï¼‰
            short_term_history: çŸ­æœŸå†å²ï¼ˆä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼Œå†å²å·²åµŒå…¥ system_promptï¼‰
            current_message: å½“å‰æ¶ˆæ¯
            
        Returns:
            ä»…åŒ…å«2æ¡æ¶ˆæ¯çš„åˆ—è¡¨ï¼š[system, user]
        """
        # short_term_history åœ¨æ­¤ä¸ä½¿ç”¨ï¼Œå†å²å·²åµŒå…¥ system_prompt
        _ = short_term_history  # æ˜¾å¼æ ‡è®°ä¸ºå·²çŸ¥æœªä½¿ç”¨

        messages = [
            {
                "role": "system",
                "content": system_prompt
            },
            {
                "role": "user",
                "content": current_message
            }
        ]

        return messages

    def _estimate_tokens(self, messages: List[Dict[str, str]]) -> int:
        """
        ä¼°ç®—æ¶ˆæ¯åˆ—è¡¨çš„ token æ•°
        
        ç®€å•ä¼°ç®—ï¼šä¸­æ–‡çº¦1.5å­—ç¬¦/tokenï¼Œè‹±æ–‡çº¦4å­—ç¬¦/token
        ä½¿ç”¨ round() ä»¥é¿å…æˆªæ–­å¯¼è‡´çš„ä½ä¼°
        """
        total_tokens = 0

        for msg in messages:
            content = msg.get("content", "")

            # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦
            chinese_chars = sum(1 for c in content if '\u4e00' <= c <= '\u9fff')
            other_chars = len(content) - chinese_chars

            # ä¼°ç®—ï¼ˆä½¿ç”¨ round é¿å…æˆªæ–­ï¼‰
            tokens = round(chinese_chars / 1.5 + other_chars / 4)

            # æ¶ˆæ¯æ ¼å¼å¼€é”€
            total_tokens += tokens + 4

        return total_tokens

    def _truncate_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        æˆªæ–­æ¶ˆæ¯ä»¥é€‚åº” token é¢„ç®—
        
        æ³¨æ„ï¼šåœ¨ç®€åŒ–ç»“æ„ï¼ˆä»… system + user ä¸¤æ¡æ¶ˆæ¯ï¼‰ä¸‹ï¼Œå†å²å·²åµŒå…¥ system promptï¼Œ
        æ— æ³•åœ¨æ¶ˆæ¯å±‚é¢è¿›è¡Œæˆªæ–­ã€‚å¦‚éœ€æ›´ä¸¥æ ¼çš„ token æ§åˆ¶ï¼Œè¯·è°ƒæ•´ short_term_rounds é…ç½®ã€‚
        
        Args:
            messages: åŸå§‹æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            æˆªæ–­åçš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆåœ¨ç®€åŒ–ç»“æ„ä¸‹è¿”å›åŸå§‹æ¶ˆæ¯ï¼‰
        """
        # ç®€åŒ–ç»“æ„ä¸‹ï¼Œåªæœ‰ system å’Œå½“å‰æ¶ˆæ¯ï¼Œæ— æ³•åœ¨æ¶ˆæ¯å±‚é¢æˆªæ–­
        # å†å²å·²åµŒå…¥ system promptï¼Œéœ€è¦é€šè¿‡è°ƒæ•´ short_term_rounds é…ç½®æ¥æ§åˆ¶ token
        if len(messages) <= 2:
            logger.debug("ç®€åŒ–ç»“æ„ä¸‹æ— æ³•æˆªæ–­æ¶ˆæ¯ï¼Œè¯·é€šè¿‡è°ƒæ•´ short_term_rounds é…ç½®æ¥æ§åˆ¶ token")
        return messages

    def get_token_budget_info(self, result: BuilderResult) -> Dict[str, Any]:
        """
        è·å– token é¢„ç®—ä½¿ç”¨æƒ…å†µ
        
        Args:
            result: æ„å»ºç»“æœ
            
        Returns:
            é¢„ç®—ä¿¡æ¯å­—å…¸
        """
        return {
            "estimated_tokens": result.token_estimate,
            "max_tokens": self.config.max_total_tokens,
            "reserved_for_output": self.config.reserved_output_tokens,
            "available_for_context": self.config.max_total_tokens - self.config.reserved_output_tokens,
            "usage_percentage": (result.token_estimate / (
                    self.config.max_total_tokens - self.config.reserved_output_tokens)) * 100
        }

"""
Unified Context Builder - ç»Ÿä¸€ä¸Šä¸‹æ–‡æ„å»ºå™¨

è´Ÿè´£æ„å»ºå‘é€ç»™ LLM çš„å®Œæ•´æ¶ˆæ¯ç»“æ„ï¼Œé‡‡ç”¨åˆ†å±‚æ–¹å¼ï¼š

æ¶ˆæ¯ç»“æ„ï¼š
1. System Promptï¼ˆåŒ…å«äººè®¾ + é•¿æœŸè®°å¿† + å¯¹è¯ç­–ç•¥ï¼‰
2. çŸ­æœŸå¯¹è¯å†å²ï¼ˆæœ€è¿‘ 3-5 è½®å®Œæ•´å†…å®¹ï¼‰
3. å½“å‰ç”¨æˆ·æ¶ˆæ¯

åŠŸèƒ½ï¼š
- åˆ†å‰²å†å²ï¼ˆçŸ­æœŸ vs ä¸­æœŸï¼‰
- ç”Ÿæˆä¸­æœŸæ‘˜è¦ï¼ˆæ”¯æŒç¼“å­˜ï¼‰
- æ•´åˆæ‰€æœ‰ä¸Šä¸‹æ–‡
- æ„å»ºæœ€ç»ˆæ¶ˆæ¯åˆ—è¡¨
- Token é¢„ç®—ç®¡ç†
- å†å²å¯¹è¯è¿‡æ»¤ï¼ˆURLã€ç®€å•å¯’æš„ç­‰ï¼‰
"""
from typing import List, Dict, Optional, Any, Tuple
from dataclasses import dataclass, field
from loguru import logger

from .summary_service import ConversationSummaryService, ConversationSummary
from .proactive_strategy import (
    ProactiveDialogueStrategyAnalyzer,
    ProactiveAction,
    UserProfile,
    TopicAnalysis
)
from src.utils.history_filter import HistoryFilter, get_history_filter


@dataclass
class ContextConfig:
    """
    ä¸Šä¸‹æ–‡é…ç½®
    
    ç”¨äºæ§åˆ¶ä¸Šä¸‹æ–‡æ„å»ºçš„å„é¡¹å‚æ•°
    """
    # å¯¹è¯å†å²åˆ†å±‚
    short_term_rounds: int = 5  # çŸ­æœŸå†å²è½®æ•°ï¼ˆæœ€è¿‘ N è½®ï¼‰
    mid_term_start: int = 3  # ä¸­æœŸå†å²å¼€å§‹è½®æ¬¡
    mid_term_end: int = 20  # ä¸­æœŸå†å²ç»“æŸè½®æ¬¡
    
    # é•¿æœŸè®°å¿†
    max_memories: int = 8  # æœ€å¤šåŒ…å«çš„é•¿æœŸè®°å¿†æ•°é‡
    
    # Token é¢„ç®—
    max_total_tokens: int = 8000  # æ€» token é¢„ç®—
    reserved_output_tokens: int = 1000  # ä¸ºè¾“å‡ºä¿ç•™çš„ token
    
    # æ‘˜è¦é€‰é¡¹
    use_llm_summary: bool = False  # æ˜¯å¦ä½¿ç”¨ LLM æ‘˜è¦ï¼ˆæ¶ˆè€— tokenï¼‰
    max_summary_length: int = 200  # æ‘˜è¦æœ€å¤§é•¿åº¦
    
    # ä¸»åŠ¨ç­–ç•¥
    enable_proactive_strategy: bool = True  # æ˜¯å¦å¯ç”¨ä¸»åŠ¨ç­–ç•¥
    
    # å†å²è¿‡æ»¤é€‰é¡¹
    enable_history_filter: bool = True  # æ˜¯å¦å¯ç”¨å†å²è¿‡æ»¤ï¼ˆè¿‡æ»¤URLã€ç®€å•å¯’æš„ç­‰ï¼‰
    filter_urls: bool = True  # æ˜¯å¦è¿‡æ»¤URLä¸»å¯¼çš„å†…å®¹
    filter_trivial: bool = True  # æ˜¯å¦è¿‡æ»¤ç®€å•å¯’æš„


@dataclass
class BuilderResult:
    """
    æ„å»ºå™¨ç»“æœ
    
    åŒ…å«æ„å»ºå¥½çš„æ¶ˆæ¯åˆ—è¡¨å’Œå…ƒæ•°æ®
    """
    messages: List[Dict[str, str]]  # å®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨
    token_estimate: int  # ä¼°ç®—çš„ token æ•°
    metadata: Dict[str, Any] = field(default_factory=dict)  # å…ƒæ•°æ®


class UnifiedContextBuilder:
    """
    ç»Ÿä¸€ä¸Šä¸‹æ–‡æ„å»ºå™¨
    
    æ ¸å¿ƒèŒè´£ï¼š
    1. å°†å¯¹è¯å†å²åˆ†å±‚ï¼ˆçŸ­æœŸã€ä¸­æœŸã€é•¿æœŸï¼‰
    2. ç”Ÿæˆä¸­æœŸå¯¹è¯æ‘˜è¦
    3. æ„å»ºå¢å¼ºçš„ System Prompt
    4. æ•´åˆæ‰€æœ‰ç»„ä»¶åˆ°æœ€ç»ˆæ¶ˆæ¯åˆ—è¡¨
    5. ç®¡ç† token é¢„ç®—
    6. è¿‡æ»¤ä¸é‡è¦çš„å†å²å†…å®¹ï¼ˆURLã€ç®€å•å¯’æš„ç­‰ï¼‰
    """
    
    def __init__(
        self,
        summary_service: Optional[ConversationSummaryService] = None,
        proactive_analyzer: Optional[ProactiveDialogueStrategyAnalyzer] = None,
        history_filter: Optional[HistoryFilter] = None,
        config: Optional[ContextConfig] = None
    ):
        """
        åˆå§‹åŒ–æ„å»ºå™¨
        
        Args:
            summary_service: æ‘˜è¦æœåŠ¡ï¼ˆå¯é€‰ï¼Œé»˜è®¤åˆ›å»ºï¼‰
            proactive_analyzer: ä¸»åŠ¨ç­–ç•¥åˆ†æå™¨ï¼ˆå¯é€‰ï¼Œé»˜è®¤åˆ›å»ºï¼‰
            history_filter: å†å²è¿‡æ»¤å™¨ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€å®ä¾‹ï¼‰
            config: é…ç½®ï¼ˆå¯é€‰ï¼Œä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
        """
        self.summary_service = summary_service or ConversationSummaryService()
        self.proactive_analyzer = proactive_analyzer or ProactiveDialogueStrategyAnalyzer()
        self.config = config or ContextConfig()
        
        # åˆå§‹åŒ–å†å²è¿‡æ»¤å™¨
        if history_filter:
            self.history_filter = history_filter
        elif self.config.enable_history_filter:
            self.history_filter = get_history_filter()
        else:
            self.history_filter = None


    async def build_context(
        self,
        bot_system_prompt: str,
        conversation_history: List[Dict[str, str]],
        current_message: str,
        user_memories: Optional[List[Dict[str, Any]]] = None,
        dialogue_strategy: Optional[str] = None,
        llm_generated_summary: Optional[Dict] = None,  # æ–°å¢å‚æ•°
        chat_id: Optional[str] = None,  # ç”¨äºå†å²è¿‡æ»¤å­˜å‚¨
        user_id: Optional[str] = None  # ç”¨äºå†å²è¿‡æ»¤å­˜å‚¨
    ) -> BuilderResult:
        """
        æ„å»ºå®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡
        
        Args:
            bot_system_prompt: Bot çš„åŸå§‹äººè®¾
            conversation_history: å®Œæ•´å¯¹è¯å†å²ï¼ˆä¸åŒ…å« system promptï¼‰
            current_message: å½“å‰ç”¨æˆ·æ¶ˆæ¯
            user_memories: ç”¨æˆ·é•¿æœŸè®°å¿†åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            dialogue_strategy: å·²ç”Ÿæˆçš„å¯¹è¯ç­–ç•¥ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™ä¸é‡æ–°ç”Ÿæˆï¼‰
            llm_generated_summary: LLM ç”Ÿæˆçš„å¯¹è¯æ‘˜è¦ï¼ˆå¯é€‰ï¼‰
            chat_id: å¯¹è¯IDï¼ˆå¯é€‰ï¼Œç”¨äºå†å²è¿‡æ»¤å­˜å‚¨ï¼‰
            user_id: ç”¨æˆ·IDï¼ˆå¯é€‰ï¼Œç”¨äºå†å²è¿‡æ»¤å­˜å‚¨ï¼‰
            
        Returns:
            BuilderResult: åŒ…å«æ¶ˆæ¯åˆ—è¡¨å’Œå…ƒæ•°æ®
        """
        logger.debug(f"ğŸ” å¼€å§‹æ„å»ºä¸Šä¸‹æ–‡ï¼Œå†å²æ¶ˆæ¯æ•°: {len(conversation_history)}")
        
        # 0. åº”ç”¨å†å²è¿‡æ»¤ï¼ˆè¿‡æ»¤URLã€ç®€å•å¯’æš„ç­‰ï¼‰
        filtered_count = 0
        if self.history_filter and self.config.enable_history_filter:
            filter_result = self.history_filter.filter_history(
                conversation_history,
                chat_id=chat_id,
                user_id=user_id
            )
            conversation_history = filter_result.filtered_history
            filtered_count = len(filter_result.filtered_out)
            if filtered_count > 0:
                logger.debug(f"ğŸ” è¿‡æ»¤äº† {filtered_count} æ¡ä¸é‡è¦çš„å†å²æ¶ˆæ¯")
        
        # 1. åˆ†å‰²å¯¹è¯å†å²
        short_term, mid_term = self._split_history(conversation_history)
        logger.debug(f"åˆ†å‰²å¯¹è¯å†å²: çŸ­æœŸ={len(short_term)}æ¡, ä¸­æœŸ={len(mid_term)}æ¡")
        
        # 2. ç”Ÿæˆä¸­æœŸæ‘˜è¦ï¼ˆå¦‚æœæœ‰ä¸­æœŸå¯¹è¯ï¼‰
        mid_term_summary = None
        if mid_term:
            mid_term_summary = await self.summary_service.summarize_conversations(
                mid_term,
                use_llm=self.config.use_llm_summary,
                max_summary_length=self.config.max_summary_length
            )
            logger.debug(f"ç”Ÿæˆä¸­æœŸæ‘˜è¦: {mid_term_summary.summary_text[:50]}...")
        
        # 3. æ ¼å¼åŒ–é•¿æœŸè®°å¿†
        memory_context = self._format_memories(user_memories)
        
        # 4. ç”Ÿæˆä¸»åŠ¨ç­–ç•¥ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        proactive_guidance = ""
        if self.config.enable_proactive_strategy:
            proactive_guidance = await self._generate_proactive_guidance(
                conversation_history, user_memories
            )
        
        # 5. æ„å»ºå¢å¼ºçš„ System Promptï¼ˆåŒ…å«å¯¹è¯å†å²ï¼‰
        enhanced_system_prompt = self._build_enhanced_system_prompt(
            bot_system_prompt=bot_system_prompt,
            memory_context=memory_context,
            mid_term_summary=mid_term_summary,
            llm_generated_summary=llm_generated_summary,  # ä¼ é€’ LLM æ‘˜è¦
            dialogue_strategy=dialogue_strategy,
            proactive_guidance=proactive_guidance,
            short_term_history=short_term  # ä¼ é€’çŸ­æœŸå†å²ä»¥åµŒå…¥ system prompt
        )
        
        # 6. æ„å»ºæœ€ç»ˆæ¶ˆæ¯åˆ—è¡¨ï¼ˆä»… system + user ä¸¤æ¡æ¶ˆæ¯ï¼‰
        messages = self._build_messages(
            enhanced_system_prompt,
            short_term,
            current_message
        )
        
        # 7. ä¼°ç®— token ä½¿ç”¨
        token_estimate = self._estimate_tokens(messages)
        
        # 8. æ£€æŸ¥ token é¢„ç®—
        if token_estimate > (self.config.max_total_tokens - self.config.reserved_output_tokens):
            logger.warning(f"Token ä½¿ç”¨ ({token_estimate}) è¶…è¿‡é¢„ç®—ï¼Œè¿›è¡Œæˆªæ–­")
            messages = self._truncate_messages(messages)
            token_estimate = self._estimate_tokens(messages)
        
        logger.info(f"ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆ: {len(messages)}æ¡æ¶ˆæ¯, ä¼°ç®—token={token_estimate}, è¿‡æ»¤äº†{filtered_count}æ¡")
        
        return BuilderResult(
            messages=messages,
            token_estimate=token_estimate,
            metadata={
                "short_term_count": len(short_term),
                "mid_term_count": len(mid_term),
                "has_mid_term_summary": mid_term_summary is not None,
                "memory_count": len(user_memories) if user_memories else 0,
                "has_proactive_guidance": bool(proactive_guidance),
                "filtered_history_count": filtered_count,
                "history_filter_enabled": self.config.enable_history_filter
            }
        )
    
    def _split_history(
        self,
        conversation_history: List[Dict[str, str]]
    ) -> Tuple[List[Dict[str, str]], List[Dict[str, str]]]:
        """
        åˆ†å‰²å¯¹è¯å†å²ä¸ºçŸ­æœŸå’Œä¸­æœŸ
        
        çŸ­æœŸï¼šæœ€è¿‘ N è½®ï¼ˆconfig.short_term_roundsï¼‰
        ä¸­æœŸï¼šç¬¬ M åˆ° N è½®ï¼ˆconfig.mid_term_start åˆ° config.mid_term_endï¼‰
        
        Returns:
            (short_term, mid_term): çŸ­æœŸå†å²å’Œä¸­æœŸå†å²
        """
        if not conversation_history:
            return [], []
        
        # è®¡ç®—çŸ­æœŸå†å²çš„æ¶ˆæ¯æ•°é‡
        # æ³¨æ„ï¼šä¸€è½®å¯¹è¯é€šå¸¸åŒ…å«ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯å’Œä¸€æ¡åŠ©æ‰‹æ¶ˆæ¯
        # ä½†æˆ‘ä»¬æŒ‰å®é™…æ¶ˆæ¯æ•°è®¡ç®—ï¼Œä¸å‡è®¾æ¯è½®æ°å¥½ä¸¤æ¡
        user_messages = [msg for msg in conversation_history if msg.get("role") == "user"]
        num_user_messages = len(user_messages)
        
        # çŸ­æœŸï¼šå–æœ€è¿‘ N è½®å¯¹è¯ï¼ˆåŸºäºç”¨æˆ·æ¶ˆæ¯æ•°ï¼‰
        if num_user_messages <= self.config.short_term_rounds:
            # æ‰€æœ‰å†å²éƒ½æ˜¯çŸ­æœŸ
            return conversation_history, []
        
        # æ‰¾åˆ°å€’æ•°ç¬¬ N æ¡ç”¨æˆ·æ¶ˆæ¯çš„ä½ç½®
        user_msg_indices = [i for i, msg in enumerate(conversation_history) if msg.get("role") == "user"]
        short_term_start_idx = user_msg_indices[-self.config.short_term_rounds]
        
        # çŸ­æœŸå†å²ä»è¯¥ä½ç½®åˆ°ç»“å°¾
        short_term = conversation_history[short_term_start_idx:]
        
        # å‰©ä½™çš„å†å²ï¼ˆä¸åŒ…æ‹¬çŸ­æœŸéƒ¨åˆ†ï¼‰
        remaining = conversation_history[:short_term_start_idx]
        
        if not remaining:
            return short_term, []
        
        # è®¡ç®—ä¸­æœŸèŒƒå›´ï¼ˆåŸºäºç”¨æˆ·æ¶ˆæ¯è½®æ•°ï¼‰
        # æ‰¾åˆ°ç¬¬ mid_term_start è½®åˆ° mid_term_end è½®çš„æ¶ˆæ¯
        remaining_user_indices = [i for i, msg in enumerate(remaining) if msg.get("role") == "user"]
        
        # å¦‚æœæœ‰è¶³å¤Ÿçš„å†å²ï¼Œæå–ä¸­æœŸ
        if len(remaining_user_indices) >= self.config.mid_term_start and self.config.mid_term_start > 0:
            start_idx = remaining_user_indices[self.config.mid_term_start - 1]
            end_user_idx = min(self.config.mid_term_end - 1, len(remaining_user_indices) - 1)
            if end_user_idx >= 0 and end_user_idx < len(remaining_user_indices):
                end_idx = remaining_user_indices[end_user_idx]
                mid_term = remaining[start_idx:end_idx + 1]
            else:
                mid_term = []
        else:
            mid_term = []
        
        return short_term, mid_term
    
    def _format_memories(self, user_memories: Optional[List[Dict[str, Any]]]) -> str:
        """
        æ ¼å¼åŒ–é•¿æœŸè®°å¿†ä¸ºæ–‡æœ¬
        
        Args:
            user_memories: ç”¨æˆ·è®°å¿†åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–çš„è®°å¿†æ–‡æœ¬
        """
        if not user_memories:
            return ""
        
        # æœ€å¤šå– max_memories æ¡
        memories_to_use = user_memories[:self.config.max_memories]
        
        memory_lines = ["ã€å…³äºè¿™ä½ç”¨æˆ·çš„è®°å¿†ã€‘"]
        for memory in memories_to_use:
            summary = memory.get("event_summary", "")
            event_date = memory.get("event_date")
            if event_date:
                event_summary = f"- ç”¨æˆ·åœ¨{event_date}è¡¨ç¤º{summary}"
            else:
                event_summary = f"- {summary}"
            if event_summary not in memory_lines:
                memory_lines.append(event_summary)
        return "\n".join(memory_lines)
    
    async def _generate_proactive_guidance(
        self,
        conversation_history: List[Dict[str, str]],
        user_memories: Optional[List[Dict[str, Any]]]
    ) -> str:
        """
        ç”Ÿæˆä¸»åŠ¨å¯¹è¯ç­–ç•¥æŒ‡å¯¼
        
        Args:
            conversation_history: å¯¹è¯å†å²
            user_memories: ç”¨æˆ·è®°å¿†
            
        Returns:
            ä¸»åŠ¨ç­–ç•¥æ–‡æœ¬
        """
        try:
            # æ„å»ºç”¨æˆ·ç”»åƒ
            user_profile = self.proactive_analyzer.analyze_user_profile(
                conversation_history, user_memories
            )
            
            # åˆ†æè¯é¢˜
            topic_analysis = self.proactive_analyzer.analyze_topic(
                conversation_history, user_profile
            )
            
            # ç”Ÿæˆä¸»åŠ¨ç­–ç•¥
            proactive_action = self.proactive_analyzer.generate_proactive_strategy(
                user_profile, topic_analysis, conversation_history, user_memories
            )
            
            # æ ¼å¼åŒ–ä¸ºæ–‡æœ¬
            guidance = self.proactive_analyzer.format_proactive_guidance(proactive_action)
            
            # æ·»åŠ ç”¨æˆ·ç”»åƒä¿¡æ¯
            profile_info = f"""
ã€å½“å‰å¯¹è¯æƒ…å¢ƒã€‘
- ç”¨æˆ·å‚ä¸åº¦ï¼š{user_profile.engagement_level.value}
- ç”¨æˆ·æƒ…ç»ªï¼š{user_profile.emotional_state}
- å…³ç³»æ·±åº¦ï¼š{user_profile.relationship_depth}/5
- ç”¨æˆ·å…´è¶£ï¼š{', '.join(user_profile.interests[:3]) if user_profile.interests else 'å¾…æ¢ç´¢'}
- å¯æ¢ç´¢è¯é¢˜ï¼š{', '.join(topic_analysis.topics_to_explore[:3]) if topic_analysis.topics_to_explore else 'æ— '}
"""
            
            return profile_info + "\n" + guidance
            
        except Exception as e:
            logger.warning(f"ç”Ÿæˆä¸»åŠ¨ç­–ç•¥å¤±è´¥: {e}")
            return ""
    
    def _build_enhanced_system_prompt(
        self,
        bot_system_prompt: str,
        memory_context: str,
        mid_term_summary: Optional[ConversationSummary],
        llm_generated_summary: Optional[Dict] = None,  # æ–°å¢ï¼šLLM ç”Ÿæˆçš„æ‘˜è¦
        dialogue_strategy: Optional[str] = None,
        proactive_guidance: str = "",
        short_term_history: Optional[List[Dict[str, str]]] = None
    ) -> str:
        """
        æ„å»ºå¢å¼ºçš„ System Prompt
        
        ç®€åŒ–ç»“æ„ï¼ˆä»…åŒ…å« system + user ä¸¤æ¡æ¶ˆæ¯ï¼‰:
        1. åŸå§‹äººè®¾ï¼ˆè§’è‰²è®¾å®šï¼‰
        2. é•¿æœŸè®°å¿†ï¼ˆé‡è¦äº‹ä»¶ï¼‰
        3. ä¸­æœŸå¯¹è¯æ‘˜è¦
        4. ä¸»åŠ¨ç­–ç•¥
        5. å¯¹è¯ç­–ç•¥
        6. ä»…5è½®å¯¹è¯å†å²ï¼ˆåµŒå…¥åœ¨ system prompt ä¸­ï¼Œå¸¦ç‰¹æ®Šæ ‡è®°é˜²æ­¢ LLM æ¨¡ä»¿æ ¼å¼ï¼‰
        7. å¼ºåˆ¶ JSON æ ¼å¼è¾“å‡ºæŒ‡ä»¤
        
        Args:
            bot_system_prompt: åŸå§‹äººè®¾
            memory_context: é•¿æœŸè®°å¿†æ–‡æœ¬
            mid_term_summary: ä¸­æœŸæ‘˜è¦
            llm_generated_summary: LLM ç”Ÿæˆçš„å¯¹è¯æ‘˜è¦ï¼ˆå¯é€‰ï¼‰
            dialogue_strategy: å¯¹è¯ç­–ç•¥
            proactive_guidance: ä¸»åŠ¨ç­–ç•¥
            short_term_history: çŸ­æœŸå¯¹è¯å†å²ï¼ˆæœ€è¿‘5è½®ï¼Œå¯é€‰ï¼‰
            
        Returns:
            å¢å¼ºåçš„ system prompt
        """
        components = [bot_system_prompt]
        
        # æ·»åŠ é•¿æœŸè®°å¿†ï¼ˆé‡è¦äº‹ä»¶ï¼‰
        if memory_context:
            components.append(memory_context)
        
        # æ·»åŠ å¯¹è¯æ‘˜è¦ï¼ˆä¼˜å…ˆä½¿ç”¨ LLM ç”Ÿæˆçš„ï¼‰
        if llm_generated_summary:
            # éªŒè¯æ‘˜è¦ç»“æ„
            if not isinstance(llm_generated_summary, dict):
                logger.warning("llm_generated_summary should be a dict, skipping")
            else:
                key_elements = llm_generated_summary.get('key_elements', {})
                if not isinstance(key_elements, dict):
                    key_elements = {}
                
                # è¾…åŠ©å‡½æ•°ï¼šå¤„ç†ç©ºåˆ—è¡¨æ˜¾ç¤º
                def format_list(items):
                    return ', '.join(items) if items else 'æ— '
                
                summary_text = f"""ã€å¯¹è¯å›é¡¾ã€‘
{llm_generated_summary.get('summary_text', '')}
å…³é”®è¦ç´ ï¼š
- æ—¶é—´={format_list(key_elements.get('time', []))}
- åœ°ç‚¹={format_list(key_elements.get('place', []))}
- äººç‰©={format_list(key_elements.get('people', []))}
- äº‹ä»¶={format_list(key_elements.get('events', []))}
- æƒ…ç»ª={format_list(key_elements.get('emotions', []))}
è¯é¢˜ï¼š{format_list(llm_generated_summary.get('topics', []))}
ç”¨æˆ·çŠ¶æ€ï¼š{llm_generated_summary.get('user_state', '')}"""
                components.append(summary_text.strip())
            
        elif mid_term_summary:
            # å›é€€åˆ°è§„åˆ™æ‘˜è¦
            summary_text = f"""ã€å¯¹è¯å›é¡¾ã€‘
{mid_term_summary.summary_text}
è®¨è®ºè¯é¢˜ï¼š{', '.join(mid_term_summary.key_topics[:3])}"""
            if mid_term_summary.emotion_trajectory:
                summary_text += f"\næƒ…ç»ªå˜åŒ–ï¼š{mid_term_summary.emotion_trajectory}"
            
            components.append(summary_text.strip())
        
        # æ·»åŠ ä¸»åŠ¨ç­–ç•¥ï¼ˆåœ¨å¯¹è¯ç­–ç•¥ä¹‹å‰ï¼‰
        if proactive_guidance:
            components.append(proactive_guidance)
        
        # æ·»åŠ å¯¹è¯ç­–ç•¥ï¼ˆå¦‚æœæä¾›ï¼‰
        if dialogue_strategy:
            components.append(dialogue_strategy)

<<<<<<< Updated upstream
        # æ·»åŠ å¯¹è¯å†å²ï¼ˆåµŒå…¥åœ¨ system prompt ä¸­ï¼Œå¸¦ç‰¹æ®Šæ ‡è®°ï¼‰
        if short_term_history:
            history_text = self._format_history_for_system_prompt(short_term_history)
            if history_text:
                components.append(history_text)
        
        # æ·»åŠ å¼ºåˆ¶ JSON æ ¼å¼è¾“å‡ºæŒ‡ä»¤
        json_format_instruction = self._get_json_format_instruction()
        components.append(json_format_instruction)

=======
>>>>>>> Stashed changes
        # ç”¨åŒæ¢è¡Œç¬¦è¿æ¥æ‰€æœ‰ç»„ä»¶
        enhanced_prompt = "\n\n".join(components)
        
        return enhanced_prompt
    
    def _format_history_for_system_prompt(
        self,
        short_term_history: List[Dict[str, str]]
    ) -> str:
        """
        å°†çŸ­æœŸå¯¹è¯å†å²æ ¼å¼åŒ–ä¸ºåµŒå…¥ system prompt çš„æ–‡æœ¬
        
        ä½¿ç”¨ç‰¹æ®Šæ ‡è®°é˜²æ­¢ LLM æ¨¡ä»¿æ­¤æ ¼å¼è¾“å‡º
        
        Args:
            short_term_history: çŸ­æœŸå¯¹è¯å†å²
            
        Returns:
            æ ¼å¼åŒ–çš„å†å²æ–‡æœ¬
        """
        if not short_term_history:
            return ""
        
        history_lines = []
        for msg in short_term_history:
            role = msg.get("role", "")
            content = msg.get("content", "")
            if role == "user":
                history_lines.append(f"User: {content}")
            elif role == "assistant":
                # ä¸ºåŠ©æ‰‹å›å¤æ·»åŠ ç®€çŸ­æ‘˜è¦ï¼Œä¿ç•™ä¸Šä¸‹æ–‡ä½†é˜²æ­¢ LLM æ¨¡ä»¿å®Œæ•´æ ¼å¼
                # æˆªå–å‰30å­—ç¬¦ä½œä¸ºæ‘˜è¦ï¼Œé¿å… token æµªè´¹
                summary = content[:30] + "..." if len(content) > 30 else content
                history_lines.append(f"Assistant: {summary}")
        
        if not history_lines:
            return ""
        
        history_text = """ã€å†å²å¯¹è¯ - ä»…å‚è€ƒï¼Œç¦æ­¢æ¨¡ä»¿æ ¼å¼ã€‘
<history>
""" + "\n".join(history_lines) + """
</history>

âš ï¸ æ³¨æ„ï¼šä¸Šæ–¹å†å²ä»…ç”¨äºç†è§£ä¸Šä¸‹æ–‡ï¼Œä½ çš„è¾“å‡ºå¿…é¡»æ˜¯JSON"""
        
        return history_text
    
    def _get_json_format_instruction(self) -> str:
        """
        è·å–å¼ºåˆ¶ JSON æ ¼å¼è¾“å‡ºæŒ‡ä»¤
        
        Returns:
            JSON æ ¼å¼æŒ‡ä»¤æ–‡æœ¬
        """
        return """ã€å¼ºåˆ¶JSONæ ¼å¼ã€‘
ä½ å¿…é¡»ä¸”åªèƒ½è¿”å›ä»¥ä¸‹JSONæ ¼å¼ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡æœ¬ï¼š
{
    "response": "ä½ çš„å›å¤å†…å®¹",
    "emotion_info": {
        "emotion_type": "æƒ…ç»ªç±»å‹(happy/gentle/sad/excited/angry/crying/neutral)",
        "intensity": "å¼ºåº¦(high/medium/low)",
        "tone_description": "è¯­æ°”æè¿°"
    }
}"""
    
    def _build_messages(
        self,
        system_prompt: str,
        short_term_history: List[Dict[str, str]],  # ä¿ç•™æ­¤å‚æ•°ç”¨äºå‘åå…¼å®¹å’Œæ¥å£ä¸€è‡´æ€§
        current_message: str
    ) -> List[Dict[str, str]]:
        """
        æ„å»ºæœ€ç»ˆæ¶ˆæ¯åˆ—è¡¨ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œä»…2æ¡æ¶ˆæ¯ï¼‰
        
        ç»“æ„ï¼š
        1. System message (åŒ…å«æ‰€æœ‰ä¸Šä¸‹æ–‡ï¼šäººè®¾ã€è®°å¿†ã€æ‘˜è¦ã€å¯¹è¯å†å²ã€JSONæ ¼å¼æŒ‡ä»¤)
        2. å½“å‰ user messageï¼ˆä»…å½“å‰è¾“å…¥ï¼‰
        
        æ³¨æ„ï¼šçŸ­æœŸå†å²å·²ç»åµŒå…¥åˆ° system_prompt ä¸­ï¼Œä¸å†ä½œä¸ºå•ç‹¬æ¶ˆæ¯ã€‚
        short_term_history å‚æ•°ä¿ç•™ç”¨äºï¼š
        1. å‘åå…¼å®¹ - é¿å…ä¿®æ”¹æ‰€æœ‰è°ƒç”¨æ–¹ä»£ç 
        2. æ¥å£ä¸€è‡´æ€§ - ä¸ build_context è°ƒç”¨æ¨¡å¼ä¿æŒä¸€è‡´
        
        Args:
            system_prompt: å¢å¼ºçš„ system promptï¼ˆå·²åŒ…å«å¯¹è¯å†å²ï¼‰
            short_term_history: çŸ­æœŸå†å²ï¼ˆä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼Œå†å²å·²åµŒå…¥ system_promptï¼‰
            current_message: å½“å‰æ¶ˆæ¯
            
        Returns:
            ä»…åŒ…å«2æ¡æ¶ˆæ¯çš„åˆ—è¡¨ï¼š[system, user]
        """
        # short_term_history åœ¨æ­¤ä¸ä½¿ç”¨ï¼Œå†å²å·²åµŒå…¥ system_prompt
        _ = short_term_history  # æ˜¾å¼æ ‡è®°ä¸ºå·²çŸ¥æœªä½¿ç”¨
        
        messages = [
            {
                "role": "system",
                "content": system_prompt
            },
            {
                "role": "user",
                "content": current_message
            }
        ]
        
        return messages
    
    def _estimate_tokens(self, messages: List[Dict[str, str]]) -> int:
        """
        ä¼°ç®—æ¶ˆæ¯åˆ—è¡¨çš„ token æ•°
        
        ç®€å•ä¼°ç®—ï¼šä¸­æ–‡çº¦1.5å­—ç¬¦/tokenï¼Œè‹±æ–‡çº¦4å­—ç¬¦/token
        ä½¿ç”¨ round() ä»¥é¿å…æˆªæ–­å¯¼è‡´çš„ä½ä¼°
        """
        total_tokens = 0
        
        for msg in messages:
            content = msg.get("content", "")
            
            # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦
            chinese_chars = sum(1 for c in content if '\u4e00' <= c <= '\u9fff')
            other_chars = len(content) - chinese_chars
            
            # ä¼°ç®—ï¼ˆä½¿ç”¨ round é¿å…æˆªæ–­ï¼‰
            tokens = round(chinese_chars / 1.5 + other_chars / 4)
            
            # æ¶ˆæ¯æ ¼å¼å¼€é”€
            total_tokens += tokens + 4
        
        return total_tokens
    
    def _truncate_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        æˆªæ–­æ¶ˆæ¯ä»¥é€‚åº” token é¢„ç®—
        
        æ³¨æ„ï¼šåœ¨ç®€åŒ–ç»“æ„ï¼ˆä»… system + user ä¸¤æ¡æ¶ˆæ¯ï¼‰ä¸‹ï¼Œå†å²å·²åµŒå…¥ system promptï¼Œ
        æ— æ³•åœ¨æ¶ˆæ¯å±‚é¢è¿›è¡Œæˆªæ–­ã€‚å¦‚éœ€æ›´ä¸¥æ ¼çš„ token æ§åˆ¶ï¼Œè¯·è°ƒæ•´ short_term_rounds é…ç½®ã€‚
        
        Args:
            messages: åŸå§‹æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            æˆªæ–­åçš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆåœ¨ç®€åŒ–ç»“æ„ä¸‹è¿”å›åŸå§‹æ¶ˆæ¯ï¼‰
        """
        # ç®€åŒ–ç»“æ„ä¸‹ï¼Œåªæœ‰ system å’Œå½“å‰æ¶ˆæ¯ï¼Œæ— æ³•åœ¨æ¶ˆæ¯å±‚é¢æˆªæ–­
        # å†å²å·²åµŒå…¥ system promptï¼Œéœ€è¦é€šè¿‡è°ƒæ•´ short_term_rounds é…ç½®æ¥æ§åˆ¶ token
        if len(messages) <= 2:
            logger.debug("ç®€åŒ–ç»“æ„ä¸‹æ— æ³•æˆªæ–­æ¶ˆæ¯ï¼Œè¯·é€šè¿‡è°ƒæ•´ short_term_rounds é…ç½®æ¥æ§åˆ¶ token")
        return messages
    
    def get_token_budget_info(self, result: BuilderResult) -> Dict[str, Any]:
        """
        è·å– token é¢„ç®—ä½¿ç”¨æƒ…å†µ
        
        Args:
            result: æ„å»ºç»“æœ
            
        Returns:
            é¢„ç®—ä¿¡æ¯å­—å…¸
        """
        return {
            "estimated_tokens": result.token_estimate,
            "max_tokens": self.config.max_total_tokens,
            "reserved_for_output": self.config.reserved_output_tokens,
            "available_for_context": self.config.max_total_tokens - self.config.reserved_output_tokens,
            "usage_percentage": (result.token_estimate / (self.config.max_total_tokens - self.config.reserved_output_tokens)) * 100
        }

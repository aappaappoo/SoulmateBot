"""
Conversation Memory Service - å¯¹è¯è®°å¿†æœåŠ¡

æä¾›RAGæŠ€æœ¯é©±åŠ¨çš„å¯¹è¯è®°å¿†åŠŸèƒ½ï¼š
1. åˆ†æå¯¹è¯ï¼Œåˆ¤æ–­æ˜¯å¦åŒ…å«é‡è¦äº‹ä»¶ï¼ˆè¿‡æ»¤æ—¥å¸¸å¯’æš„ï¼‰
2. æå–å¹¶ä¿å­˜é‡è¦äº‹ä»¶åˆ°æ•°æ®åº“ï¼ˆæ”¯æŒå‘é‡åµŒå…¥ï¼‰
3. ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢ä¸å½“å‰å¯¹è¯ç›¸å…³çš„å†å²è®°å¿†
4. ç”¨äºå¢å¼ºBotçš„ä¸ªæ€§åŒ–å¯¹è¯èƒ½åŠ›

å‘é‡æ£€ç´¢è¯´æ˜ï¼š
- ä½¿ç”¨Embeddingå‘é‡åŒ–å­˜å‚¨è®°å¿†æ‘˜è¦
- æ”¯æŒè¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢ï¼Œæå‡æ£€ç´¢ç²¾åº¦
- å‘åå…¼å®¹ï¼šå¯¹äºæ²¡æœ‰embeddingçš„è®°å¿†ï¼Œå›é€€åˆ°å…³é”®è¯åŒ¹é…
- æ”¯æŒæ··åˆæ£€ç´¢ï¼šå‘é‡ç›¸ä¼¼åº¦ + å…ƒæ•°æ®è¿‡æ»¤

ä½¿ç”¨æ–¹æ³•ï¼š
    from src.services.conversation_memory_service import ConversationMemoryService

    service = ConversationMemoryService(db_session, llm_provider, embedding_service)

    # ä¿å­˜é‡è¦å¯¹è¯äº‹ä»¶ï¼ˆè‡ªåŠ¨ç”Ÿæˆå‘é‡åµŒå…¥ï¼‰
    await service.extract_and_save_important_events(
        user_id=123,
        bot_id=456,
        user_message="æˆ‘ä¸‹ä¸ªæœˆ15å·ç”Ÿæ—¥",
        bot_response="å¤ªæ£’äº†ï¼æˆ‘è®°ä½äº†ï¼Œä¸‹ä¸ªæœˆ15å·æ˜¯ä½ çš„ç”Ÿæ—¥..."
    )

    # æ£€ç´¢ç›¸å…³è®°å¿†ï¼ˆä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦ï¼‰
    memories = await service.retrieve_memories(
        user_id=123,
        bot_id=456,
        current_message="ä½ è¿˜è®°å¾—æˆ‘çš„ç”Ÿæ—¥å—ï¼Ÿ"
    )
"""
import json
import time
import uuid
import re
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timezone, timedelta
from dateutil.relativedelta import relativedelta
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, update, and_, or_, func
from loguru import logger
import numpy as np

from src.models.database import UserMemory, MemoryImportance


class DateParser:
    """
    æ™ºèƒ½æ—¥æœŸè§£æå™¨

    æ”¯æŒè§£æå„ç§ä¸­æ–‡å’Œè‹±æ–‡çš„ç›¸å¯¹æ—¶é—´è¡¨è¾¾ï¼Œç»“åˆå½“å‰ç³»ç»Ÿæ—¶é—´è®¡ç®—å‡ºå‡†ç¡®æ—¥æœŸã€‚

    æ”¯æŒçš„è¡¨è¾¾å¼ç¤ºä¾‹ï¼š
    - ç›¸å¯¹æ—¥æœŸï¼šæ˜¨å¤©ã€ä»Šå¤©ã€æ˜å¤©ã€å‰å¤©ã€åå¤©ã€å¤§å‰å¤©ã€å¤§åå¤©
    - ç›¸å¯¹å‘¨ï¼šä¸Šå‘¨ã€è¿™å‘¨ã€ä¸‹å‘¨ã€ä¸Šä¸Šå‘¨ã€ä¸‹ä¸‹å‘¨
    - ç›¸å¯¹æœˆï¼šä¸Šä¸ªæœˆã€è¿™ä¸ªæœˆã€ä¸‹ä¸ªæœˆã€ä¸Šä¸Šä¸ªæœˆ
    - ç›¸å¯¹å¹´ï¼šå»å¹´ã€ä»Šå¹´ã€æ˜å¹´ã€å‰å¹´ã€åå¹´
    - å…·ä½“æ—¥æœŸï¼š15å·ã€3æœˆ15æ—¥ã€2026å¹´3æœˆ15æ—¥
    - ç»„åˆè¡¨è¾¾ï¼šä¸‹ä¸ªæœˆ15å·ã€æ˜å¹´3æœˆã€å»å¹´12æœˆ25æ—¥
    - æ˜ŸæœŸè¡¨è¾¾ï¼šå‘¨ä¸€ã€æ˜ŸæœŸä¸‰ã€ä¸‹å‘¨äº”ã€ä¸Šå‘¨æ—¥
    - ç‰¹æ®Šè¡¨è¾¾ï¼šæœˆåº•ã€æœˆåˆã€å¹´åº•ã€å¹´åˆ
    """

    # ä¸­æ–‡æ•°å­—æ˜ å°„
    CN_NUMBERS = {
        'é›¶': 0, 'ä¸€': 1, 'äºŒ': 2, 'ä¸¤': 2, 'ä¸‰': 3, 'å››': 4,
        'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9, 'å': 10,
        'åä¸€': 11, 'åäºŒ': 12, 'åä¸‰': 13, 'åå››': 14, 'åäº”': 15,
        'åå…­': 16, 'åä¸ƒ': 17, 'åå…«': 18, 'åä¹': 19, 'äºŒå': 20,
        'äºŒåä¸€': 21, 'äºŒåäºŒ': 22, 'äºŒåä¸‰': 23, 'äºŒåå››': 24,
        'äºŒåäº”': 25, 'äºŒåå…­': 26, 'äºŒåä¸ƒ': 27, 'äºŒåå…«': 28,
        'äºŒåä¹': 29, 'ä¸‰å': 30, 'ä¸‰åä¸€': 31
    }

    # æ˜ŸæœŸæ˜ å°„
    WEEKDAY_MAP = {
        'ä¸€': 0, 'äºŒ': 1, 'ä¸‰': 2, 'å››': 3, 'äº”': 4, 'å…­': 5, 'æ—¥': 6, 'å¤©': 6,
        '1': 0, '2': 1, '3': 2, '4': 3, '5': 4, '6': 5, '7': 6
    }

    def __init__(self, reference_time: Optional[datetime] = None):
        """
        åˆå§‹åŒ–æ—¥æœŸè§£æå™¨

        Args:
            reference_time: å‚è€ƒæ—¶é—´ï¼Œé»˜è®¤ä½¿ç”¨å½“å‰ç³»ç»Ÿæ—¶é—´
        """
        self.reference_time = reference_time or datetime.now()
        self.today = self.reference_time.replace(hour=0, minute=0, second=0, microsecond=0)

    def parse(self, text: str) -> Optional[datetime]:
        """
        è§£ææ–‡æœ¬ä¸­çš„æ—¥æœŸè¡¨è¾¾

        Args:
            text: åŒ…å«æ—¥æœŸè¡¨è¾¾çš„æ–‡æœ¬

        Returns:
            è§£æå‡ºçš„æ—¥æœŸï¼Œå¦‚æœæ— æ³•è§£æåˆ™è¿”å›None
        """
        if not text:
            return None

        text = text.strip()

        # å°è¯•å„ç§è§£æç­–ç•¥
        result = None

        # 1. å°è¯•è§£ææ ‡å‡†æ—¥æœŸæ ¼å¼ (YYYY-MM-DD, YYYY/MM/DD, YYYYå¹´MMæœˆDDæ—¥)
        result = self._parse_standard_date(text)
        if result:
            return result

        # 2. å°è¯•è§£æç›¸å¯¹æ—¥æœŸè¡¨è¾¾
        result = self._parse_relative_date(text)
        if result:
            return result

        # 3. å°è¯•è§£æç»„åˆæ—¥æœŸè¡¨è¾¾ï¼ˆå¦‚"ä¸‹ä¸ªæœˆ15å·"ï¼‰
        result = self._parse_combined_date(text)
        if result:
            return result

        # 4. å°è¯•è§£ææ˜ŸæœŸè¡¨è¾¾
        result = self._parse_weekday(text)
        if result:
            return result

        # 5. å°è¯•è§£æç‰¹æ®Šè¡¨è¾¾ï¼ˆæœˆåº•ã€å¹´åˆç­‰ï¼‰
        result = self._parse_special_date(text)
        if result:
            return result

        return None

    def _parse_standard_date(self, text: str) -> Optional[datetime]:
        """è§£ææ ‡å‡†æ—¥æœŸæ ¼å¼"""
        patterns = [
            # YYYY-MM-DD æˆ– YYYY/MM/DD
            (r'(\d{4})[-/](\d{1,2})[-/](\d{1,2})',
             lambda m: datetime(int(m.group(1)), int(m.group(2)), int(m.group(3)))),
            # YYYYå¹´MMæœˆDDæ—¥
            (r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})[æ—¥å·]?',
             lambda m: datetime(int(m.group(1)), int(m.group(2)), int(m.group(3)))),
            # MM-DD æˆ– MM/DD (ä½¿ç”¨å½“å‰å¹´ä»½)
            (r'^(\d{1,2})[-/](\d{1,2})$', lambda m: datetime(self.today.year, int(m.group(1)), int(m.group(2)))),
            # MMæœˆDDæ—¥ (ä½¿ç”¨å½“å‰å¹´ä»½)
            (r'^(\d{1,2})æœˆ(\d{1,2})[æ—¥å·]$', lambda m: datetime(self.today.year, int(m.group(1)), int(m.group(2)))),
            # DDæ—¥ æˆ– DDå· (ä½¿ç”¨å½“å‰å¹´æœˆ)
            (r'^(\d{1,2})[æ—¥å·]$', lambda m: datetime(self.today.year, self.today.month, int(m.group(1)))),
        ]

        for pattern, handler in patterns:
            match = re.search(pattern, text)
            if match:
                try:
                    return handler(match)
                except ValueError:
                    continue

        return None

    def _parse_relative_date(self, text: str) -> Optional[datetime]:
        """è§£æç›¸å¯¹æ—¥æœŸè¡¨è¾¾"""

        # ==================== æ¨¡ç³Šæ—¶é—´è¡¨è¾¾ ====================
        # è¿™äº›è¡¨è¾¾éƒ½æŒ‡å‘"ä»Šå¤©"
        today_expressions = [
            r'åˆšåˆš', r'åˆšæ‰', r'æ–¹æ‰',
            r'ä»Šå¤©æ—©ä¸Š', r'ä»Šå¤©ä¸Šåˆ', r'ä»Šå¤©ä¸­åˆ', r'ä»Šå¤©ä¸‹åˆ', r'ä»Šå¤©æ™šä¸Š', r'ä»Šå¤©å‡Œæ™¨',
            r'ä»Šæ—©', r'ä»Šæ™š',
            r'ä¸Šåˆ', r'ä¸‹åˆ', r'æ™šä¸Š', r'å‡Œæ™¨', r'ä¸­åˆ',  # ä¸å¸¦"ä»Šå¤©"é»˜è®¤æŒ‡ä»Šå¤©
            r'è¿™ä¼šå„¿', r'ç°åœ¨', r'æ­¤åˆ»',
            r'ä»Šæ—¥',
        ]

        for pattern in today_expressions:
            if re.search(pattern, text):
                return self.today

        # è¿™äº›è¡¨è¾¾æŒ‡å‘"æ˜¨å¤©"
        yesterday_expressions = [
            r'æ˜¨æ™š', r'æ˜¨å¤©æ™šä¸Š', r'æ˜¨å¤©ä¸‹åˆ', r'æ˜¨å¤©ä¸Šåˆ', r'æ˜¨å¤©æ—©ä¸Š',
            r'æ˜¨æ—¥',
        ]

        for pattern in yesterday_expressions:
            if re.search(pattern, text):
                return self.today + timedelta(days=-1)

        # æ¨¡ç³Šçš„"æœ€è¿‘"è¡¨è¾¾ - é»˜è®¤æŒ‡å‘ä»Šå¤©
        recent_expressions = [
            r'æœ€è¿‘', r'è¿™å‡ å¤©', r'å‰å‡ å¤©', r'è¿™æ®µæ—¶é—´', r'è¿‘æœŸ', r'è¿‘æ¥',
        ]

        for pattern in recent_expressions:
            if re.search(pattern, text):
                return self.today

        # ==================== ç²¾ç¡®çš„ç›¸å¯¹å¤©æ•° ====================
        day_patterns = {
            r'å¤§å‰å¤©': -3,
            r'å‰å¤©': -2,
            r'æ˜¨å¤©': -1,
            r'ä»Šå¤©': 0,
            r'æ˜å¤©': 1,
            r'åå¤©': 2,
            r'å¤§åå¤©': 3,
        }

        for pattern, days in day_patterns.items():
            if pattern in text:
                return self.today + timedelta(days=days)

        # ==================== ç›¸å¯¹å‘¨ ====================
        week_patterns = {
            r'ä¸Šä¸Šå‘¨': -2,
            r'ä¸Šå‘¨': -1,
            r'è¿™å‘¨|æœ¬å‘¨': 0,
            r'ä¸‹å‘¨': 1,
            r'ä¸‹ä¸‹å‘¨': 2,
        }

        for pattern, weeks in week_patterns.items():
            if re.search(pattern, text):
                # è¿”å›è¯¥å‘¨çš„å‘¨ä¸€
                target = self.today + timedelta(weeks=weeks)
                days_since_monday = target.weekday()
                return target - timedelta(days=days_since_monday)

        # ==================== ç›¸å¯¹æœˆ ====================
        month_patterns = {
            r'ä¸Šä¸Šä¸ª?æœˆ': -2,
            r'ä¸Šä¸ª?æœˆ': -1,
            r'è¿™ä¸ª?æœˆ|æœ¬æœˆ': 0,
            r'ä¸‹ä¸ª?æœˆ': 1,
            r'ä¸‹ä¸‹ä¸ª?æœˆ': 2,
        }

        for pattern, months in month_patterns.items():
            if re.search(pattern, text):
                result = self.today + relativedelta(months=months)
                # åªåŒ¹é…çº¯æœˆä»½è¡¨è¾¾ï¼Œè¿”å›è¯¥æœˆ1æ—¥
                if not re.search(r'\d+[æ—¥å·]', text):
                    return result.replace(day=1)
                return result

        # ==================== ç›¸å¯¹å¹´ ====================
        year_patterns = {
            r'å‰å¹´': -2,
            r'å»å¹´': -1,
            r'ä»Šå¹´': 0,
            r'æ˜å¹´': 1,
            r'åå¹´': 2,
        }

        for pattern, years in year_patterns.items():
            if re.search(pattern, text):
                result = self.today + relativedelta(years=years)
                # åªåŒ¹é…çº¯å¹´ä»½è¡¨è¾¾ï¼Œè¿”å›è¯¥å¹´1æœˆ1æ—¥
                if not re.search(r'\d+æœˆ|\d+[æ—¥å·]', text):
                    return result.replace(month=1, day=1)
                return result

        return None

    def _parse_combined_date(self, text: str) -> Optional[datetime]:
        """è§£æç»„åˆæ—¥æœŸè¡¨è¾¾ï¼ˆå¦‚"ä¸‹ä¸ªæœˆ15å·"ã€"æ˜å¹´3æœˆ15æ—¥"ï¼‰"""

        # åŸºå‡†æ—¥æœŸ
        base_date = self.today

        # è§£æå¹´ä»½ä¿®é¥°
        year_offset = 0
        if 'å‰å¹´' in text:
            year_offset = -2
        elif 'å»å¹´' in text:
            year_offset = -1
        elif 'æ˜å¹´' in text:
            year_offset = 1
        elif 'åå¹´' in text:
            year_offset = 2

        # è§£ææœˆä»½ä¿®é¥°
        month_offset = 0
        if re.search(r'ä¸Šä¸Šä¸ª?æœˆ', text):
            month_offset = -2
        elif re.search(r'ä¸Šä¸ª?æœˆ', text):
            month_offset = -1
        elif re.search(r'ä¸‹ä¸‹ä¸ª?æœˆ', text):
            month_offset = 2
        elif re.search(r'ä¸‹ä¸ª?æœˆ', text):
            month_offset = 1

        # åº”ç”¨å¹´æœˆåç§»
        if year_offset != 0 or month_offset != 0:
            base_date = base_date + relativedelta(years=year_offset, months=month_offset)

        # è§£æå…·ä½“æœˆä»½ (å¦‚ "3æœˆ", "12æœˆ")
        month_match = re.search(r'(\d{1,2})æœˆ', text)
        if month_match:
            month = int(month_match.group(1))
            if 1 <= month <= 12:
                base_date = base_date.replace(month=month)

        # è§£æå…·ä½“æ—¥æœŸ (å¦‚ "15å·", "15æ—¥")
        day_match = re.search(r'(\d{1,2})[æ—¥å·]', text)
        if day_match:
            day = int(day_match.group(1))
            if 1 <= day <= 31:
                try:
                    base_date = base_date.replace(day=day)
                except ValueError:
                    # æ—¥æœŸæ— æ•ˆï¼ˆå¦‚2æœˆ30æ—¥ï¼‰ï¼Œå°è¯•ä½¿ç”¨è¯¥æœˆæœ€åä¸€å¤©
                    next_month = base_date.replace(day=1) + relativedelta(months=1)
                    base_date = next_month - timedelta(days=1)

        # è§£æä¸­æ–‡æ—¥æœŸ (å¦‚ "åäº”å·", "äºŒåæ—¥")
        for cn_num, num in self.CN_NUMBERS.items():
            if f'{cn_num}[æ—¥å·]' in text or re.search(f'{cn_num}[æ—¥å·]', text):
                try:
                    base_date = base_date.replace(day=num)
                except ValueError:
                    pass
                break

        # å¦‚æœæœ‰ä»»ä½•åç§»æˆ–å…·ä½“æ—¥æœŸï¼Œè¿”å›ç»“æœ
        if year_offset != 0 or month_offset != 0 or month_match or day_match:
            return base_date

        return None

    def _parse_weekday(self, text: str) -> Optional[datetime]:
        """è§£ææ˜ŸæœŸè¡¨è¾¾ï¼ˆå¦‚"å‘¨ä¸€"ã€"ä¸‹å‘¨äº”"ã€"ä¸Šå‘¨æ—¥"ï¼‰"""

        # ç¡®å®šå‘¨åç§»
        week_offset = 0
        if re.search(r'ä¸Šä¸Šå‘¨', text):
            week_offset = -2
        elif re.search(r'ä¸Šå‘¨', text):
            week_offset = -1
        elif re.search(r'ä¸‹ä¸‹å‘¨', text):
            week_offset = 2
        elif re.search(r'ä¸‹å‘¨', text):
            week_offset = 1
        elif re.search(r'è¿™å‘¨|æœ¬å‘¨', text):
            week_offset = 0

        # è§£ææ˜ŸæœŸå‡ 
        weekday_match = re.search(r'(?:å‘¨|æ˜ŸæœŸ)([ä¸€äºŒä¸‰å››äº”å…­æ—¥å¤©1-7])', text)
        if weekday_match:
            weekday_str = weekday_match.group(1)
            target_weekday = self.WEEKDAY_MAP.get(weekday_str)

            if target_weekday is not None:
                # è®¡ç®—ç›®æ ‡æ—¥æœŸ
                current_weekday = self.today.weekday()
                days_diff = target_weekday - current_weekday + (week_offset * 7)

                # å¦‚æœæ²¡æœ‰å‘¨åç§»ä¸”ç›®æ ‡æ˜ŸæœŸå·²è¿‡ï¼Œé»˜è®¤æŒ‡å‘ä¸‹å‘¨
                if week_offset == 0 and days_diff < 0 and 'è¿™å‘¨' not in text and 'æœ¬å‘¨' not in text:
                    days_diff += 7

                return self.today + timedelta(days=days_diff)

        return None

    def _parse_special_date(self, text: str) -> Optional[datetime]:
        """è§£æç‰¹æ®Šæ—¥æœŸè¡¨è¾¾ï¼ˆæœˆåº•ã€å¹´åˆç­‰ï¼‰"""

        base_date = self.today

        # å…ˆå¤„ç†å¹´æœˆåç§»
        if 'å»å¹´' in text:
            base_date = base_date + relativedelta(years=-1)
        elif 'æ˜å¹´' in text:
            base_date = base_date + relativedelta(years=1)

        if re.search(r'ä¸Šä¸ª?æœˆ', text):
            base_date = base_date + relativedelta(months=-1)
        elif re.search(r'ä¸‹ä¸ª?æœˆ', text):
            base_date = base_date + relativedelta(months=1)

        # æœˆåˆ
        if 'æœˆåˆ' in text:
            return base_date.replace(day=1)

        # æœˆåº•/æœˆæœ«
        if 'æœˆåº•' in text or 'æœˆæœ«' in text:
            next_month = base_date.replace(day=1) + relativedelta(months=1)
            return next_month - timedelta(days=1)

        # å¹´åˆ
        if 'å¹´åˆ' in text:
            return base_date.replace(month=1, day=1)

        # å¹´åº•/å¹´æœ«
        if 'å¹´åº•' in text or 'å¹´æœ«' in text:
            return base_date.replace(month=12, day=31)

        return None

    def parse_from_message(self, message: str) -> Optional[datetime]:
        """
        ä»ç”¨æˆ·æ¶ˆæ¯ä¸­æ™ºèƒ½æå–æ—¥æœŸ

        ä¼šå°è¯•è¯†åˆ«æ¶ˆæ¯ä¸­çš„å„ç§æ—¥æœŸè¡¨è¾¾å¹¶è§£æ

        Args:
            message: ç”¨æˆ·æ¶ˆæ¯

        Returns:
            è§£æå‡ºçš„æ—¥æœŸï¼Œå¦‚æœæ— æ³•è§£æåˆ™è¿”å›None
        """
        if not message:
            return None

        # å¸¸è§çš„æ—¥æœŸç›¸å…³å…³é”®è¯æ¨¡å¼
        date_patterns = [
            # ==================== æ–°å¢ï¼šæ¨¡ç³Šæ—¶é—´è¡¨è¾¾ ====================
            r'åˆšåˆš|åˆšæ‰|æ–¹æ‰',
            r'æœ€è¿‘|è¿™å‡ å¤©|å‰å‡ å¤©|è¿™æ®µæ—¶é—´|è¿‘æœŸ|è¿‘æ¥|è¿™ä¸¤å¤©',
            r'ä»Šå¤©?æ—©ä¸Š|ä»Šå¤©?ä¸Šåˆ|ä»Šå¤©?ä¸­åˆ|ä»Šå¤©?ä¸‹åˆ|ä»Šå¤©?æ™šä¸Š|ä»Šå¤©?å‡Œæ™¨',
            r'ä»Šæ—©|ä»Šæ™š|ä»Šæ—¥',
            r'æ˜¨å¤©?æ™šä¸Š|æ˜¨å¤©?ä¸‹åˆ|æ˜¨å¤©?ä¸Šåˆ|æ˜¨å¤©?æ—©ä¸Š|æ˜¨æ™š|æ˜¨æ—©|æ˜¨æ—¥',
            r'å‰å¤©æ™šä¸Š|å‰æ™š',
            r'è¿™ä¼šå„¿|ç°åœ¨|æ­¤åˆ»',
            # ==================== å®Œæ•´æ—¥æœŸè¡¨è¾¾ ====================
            r'\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}[æ—¥å·]?',
            r'\d{1,2}[-/æœˆ]\d{1,2}[æ—¥å·]?',
            r'\d{1,2}[æ—¥å·]',
            # ç›¸å¯¹æ—¥æœŸ
            r'å¤§?å‰å¤©|æ˜¨å¤©|ä»Šå¤©|æ˜å¤©|å¤§?åå¤©',
            # ç›¸å¯¹å‘¨
            r'ä¸Š{1,2}å‘¨|è¿™å‘¨|æœ¬å‘¨|ä¸‹{1,2}å‘¨',
            # ç›¸å¯¹æœˆ
            r'ä¸Š{1,2}ä¸ª?æœˆ|è¿™ä¸ª?æœˆ|æœ¬æœˆ|ä¸‹{1,2}ä¸ª?æœˆ',
            # ç›¸å¯¹å¹´
            r'å‰å¹´|å»å¹´|ä»Šå¹´|æ˜å¹´|åå¹´',
            # æ˜ŸæœŸ
            r'(?:ä¸Š{1,2}å‘¨|è¿™å‘¨|æœ¬å‘¨|ä¸‹{1,2}å‘¨)?(?:å‘¨|æ˜ŸæœŸ)[ä¸€äºŒä¸‰å››äº”å…­æ—¥å¤©1-7]',
            # ç‰¹æ®Šè¡¨è¾¾
            r'æœˆ[åˆåº•æœ«]|å¹´[åˆåº•æœ«]',
            # ç»„åˆè¡¨è¾¾ (å¦‚ "ä¸‹ä¸ªæœˆ15å·")
            r'(?:ä¸Š{1,2}ä¸ª?æœˆ|ä¸‹{1,2}ä¸ª?æœˆ)\d{1,2}[æ—¥å·]',
            r'(?:å»å¹´|æ˜å¹´|åå¹´)\d{1,2}æœˆ(?:\d{1,2}[æ—¥å·])?',
        ]

        # æ‰¾åˆ°æ‰€æœ‰å¯èƒ½çš„æ—¥æœŸè¡¨è¾¾
        for pattern in date_patterns:
            matches = re.findall(pattern, message)
            for match in matches:
                result = self.parse(match)
                if result:
                    return result

        # å°è¯•ç›´æ¥è§£ææ•´ä¸ªæ¶ˆæ¯
        return self.parse(message)


class ConversationMemoryService:
    """
    å¯¹è¯è®°å¿†æœåŠ¡

    ä½¿ç”¨LLMåˆ†æå¯¹è¯é‡è¦æ€§ï¼Œå­˜å‚¨é‡è¦äº‹ä»¶ï¼Œå¹¶åœ¨éœ€è¦æ—¶æ£€ç´¢ç›¸å…³è®°å¿†ã€‚
    """

    # ç”¨äºåˆ¤æ–­é‡è¦æ€§çš„ç³»ç»Ÿæç¤ºè¯ï¼ˆæ›´æ–°ä»¥æ”¯æŒæ›´çµæ´»çš„æ—¥æœŸæå–ï¼‰
    IMPORTANCE_ANALYSIS_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½è®°å¿†åˆ†æåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æç”¨æˆ·å’ŒAIåŠ©æ‰‹ä¹‹é—´çš„å¯¹è¯ï¼Œåˆ¤æ–­æ˜¯å¦åŒ…å«å€¼å¾—è®°ä½çš„é‡è¦äº‹ä»¶ã€‚

    é‡è¦äº‹ä»¶åŒ…æ‹¬ï¼ˆä»¥ä¸‹çš„äº‹ä»¶é‡è¦åº¦ä¸ºä¸­ç­‰ä»¥åŠä»¥ä¸Šï¼‰ï¼š
    - ä¸ªäººä¿¡æ¯ï¼šç”Ÿæ—¥ã€å¹´é¾„ã€èŒä¸šã€å®¶åº­æˆå‘˜ã€å±…ä½åœ°ç­‰
    - é‡è¦åå¥½ï¼šå–œæ¬¢/ä¸å–œæ¬¢çš„äº‹ç‰©ã€å…´è¶£çˆ±å¥½ã€ä¹ æƒ¯ç­‰
    - é‡è¦ç›®æ ‡ï¼šå­¦ä¹ è®¡åˆ’ã€å·¥ä½œç›®æ ‡ã€äººç”Ÿè§„åˆ’ç­‰
    - æƒ…æ„Ÿäº‹ä»¶ï¼šé‡è¦çš„æƒ…æ„Ÿè¡¨è¾¾ã€å¿ƒç†çŠ¶æ€å˜åŒ–ç­‰
    - ç”Ÿæ´»äº‹ä»¶ï¼šæ¯•ä¸šã€æ±‚èŒã€ç»“å©šã€ç”Ÿç—…ã€æ¬å®¶ç­‰é‡å¤§äº‹ä»¶
    - äººé™…å…³ç³»ï¼šæœ‹å‹ã€å®¶äººã€åŒäº‹ç­‰é‡è¦å…³ç³»
    - ç”Ÿæ´»å°æ’æ›²ï¼šè®°å½•å…·æœ‰æƒ…ç»ªä»·å€¼å’Œè¿ç»­æ„ä¹‰çš„æ—¥å¸¸å°äº‹ä»¶æ¯”å¦‚ï¼šæ¡åˆ°é’±ã€å¤±æ‹äº†ã€å‡èŒåŠ è–ªäº†ã€åæ§½æœ‹å‹è€æ¿åŒäº‹ç­‰ç­‰ï¼Œç”¨äºè¿½è¸ªçŠ¶æ€å˜åŒ–ä¸åç»­å…³æ€€ã€‚

    ä¸é‡è¦çš„äº‹ä»¶ï¼ˆåº”è¯¥è¿‡æ»¤ï¼‰ï¼š
    - æ—¥å¸¸å¯’æš„ï¼šä½ å¥½ã€å†è§ã€è°¢è°¢ã€æ—©ä¸Šå¥½ç­‰
    - ç®€å•é—®ç­”ï¼šä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ã€ç°åœ¨å‡ ç‚¹äº†ç­‰
    - æ— ä¸ªäººä¿¡æ¯çš„æŠ€æœ¯é—®é¢˜ï¼šå¦‚ä½•å†™ä»£ç ã€è§£é‡Šæ¦‚å¿µç­‰
    - ä¸€æ¬¡æ€§è¯é¢˜ï¼šæ— éœ€é•¿æœŸè®°å¿†çš„ä¸´æ—¶è¯é¢˜

    å…³äºæ—¥æœŸæå–ï¼š
    - å¦‚æœç”¨æˆ·æåˆ°äº†å…·ä½“æ—¥æœŸï¼Œè¯·æå–ä¸º YYYY-MM-DD æ ¼å¼
    - å¦‚æœç”¨æˆ·ä½¿ç”¨ç›¸å¯¹æ—¶é—´è¡¨è¾¾ï¼ˆå¦‚"æ˜¨å¤©"ã€"ä¸‹ä¸ªæœˆ15å·"ã€"æ˜å¹´3æœˆ"ï¼‰ï¼Œè¯·åŸæ ·ä¿ç•™åœ¨ raw_date_expression å­—æ®µ
    - å¦‚æœæ²¡æœ‰æåˆ°æ—¥æœŸç›¸å…³ä¿¡æ¯ï¼Œevent_date å’Œ raw_date_expression éƒ½è®¾ä¸º null

    è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
    {
        "is_important": true/false,
        "importance_level": "low/medium/high/critical",
        "event_type": "preference/birthday/goal/emotion/life_event/relationship/other",
        "event_summary": "ç®€æ´çš„äº‹ä»¶æ‘˜è¦ï¼ˆå¦‚æœé‡è¦çš„è¯ï¼‰",
        "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
        "event_date": "YYYY-MM-DDæ ¼å¼çš„æ—¥æœŸï¼ˆå¦‚æœæ˜¯å…·ä½“æ—¥æœŸï¼‰æˆ– null",
        "raw_date_expression": "ç”¨æˆ·åŸå§‹çš„æ—¶é—´è¡¨è¾¾ï¼ˆå¦‚'æ˜¨å¤©'ã€'ä¸‹ä¸ªæœˆ15å·'ï¼‰æˆ– null"
    }

    åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

    # ç”¨äºæ£€ç´¢ç›¸å…³è®°å¿†çš„ç³»ç»Ÿæç¤ºè¯
    MEMORY_RETRIEVAL_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½è®°å¿†æ£€ç´¢åŠ©æ‰‹ã€‚æ ¹æ®ç”¨æˆ·å½“å‰çš„æ¶ˆæ¯ï¼Œä»å†å²è®°å¿†ä¸­æ‰¾å‡ºæœ€ç›¸å…³çš„è®°å¿†ã€‚

    è¯·åˆ†æå½“å‰æ¶ˆæ¯å¯èƒ½éœ€è¦å›å¿†çš„å†…å®¹ç±»å‹ï¼Œä¾‹å¦‚ï¼š
    - ç”¨æˆ·è¯¢é—®"ä½ è¿˜è®°å¾—æˆ‘å—"æ—¶ï¼Œéœ€è¦å›å¿†ç”¨æˆ·çš„åŸºæœ¬ä¿¡æ¯
    - ç”¨æˆ·æåˆ°ç”Ÿæ—¥ç›¸å…³è¯é¢˜æ—¶ï¼Œéœ€è¦å›å¿†ç”Ÿæ—¥ç›¸å…³çš„è®°å¿†
    - ç”¨æˆ·è®¨è®ºå·¥ä½œæ—¶ï¼Œéœ€è¦å›å¿†èŒä¸šå’Œå·¥ä½œç›®æ ‡ç›¸å…³çš„è®°å¿†

    è¯·ä»¥JSONæ ¼å¼è¿”å›æ£€ç´¢å»ºè®®ï¼š
    {
        "should_retrieve": true/false,
        "relevance_keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
        "event_types": ["preference", "goal", "emotion"]
    }

    åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

    def __init__(
            self,
            db: AsyncSession,
            llm_provider=None,
            embedding_service=None,
            max_memories_per_query: int = 5,
            importance_threshold: str = "medium",
            similarity_threshold: float = 0.5
    ):
        """
        åˆå§‹åŒ–å¯¹è¯è®°å¿†æœåŠ¡

        Args:
            db: å¼‚æ­¥æ•°æ®åº“ä¼šè¯
            llm_provider: LLMæä¾›è€…ï¼ˆç”¨äºé‡è¦æ€§åˆ†æï¼‰
            embedding_service: å‘é‡åµŒå…¥æœåŠ¡ï¼ˆç”¨äºç”Ÿæˆå’Œæ£€ç´¢å‘é‡ï¼‰
            max_memories_per_query: æ¯æ¬¡æ£€ç´¢è¿”å›çš„æœ€å¤§è®°å¿†æ•°é‡
            importance_threshold: ä¿å­˜è®°å¿†çš„æœ€ä½é‡è¦æ€§é˜ˆå€¼
            similarity_threshold: å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢çš„æœ€ä½é˜ˆå€¼
        """
        self.db = db
        self.llm_provider = llm_provider
        self.embedding_service = embedding_service
        self.max_memories_per_query = max_memories_per_query
        self.importance_threshold = importance_threshold
        self.similarity_threshold = similarity_threshold

        # é‡è¦æ€§çº§åˆ«æ’åº
        self._importance_order = {
            MemoryImportance.LOW.value: 0,
            MemoryImportance.MEDIUM.value: 1,
            MemoryImportance.HIGH.value: 2,
            MemoryImportance.CRITICAL.value: 3
        }

        logger.info(
            f"ğŸ§  [Memory] ConversationMemoryService initialized | "
            f"embedding_enabled={embedding_service is not None} | "
            f"llm_enabled={llm_provider is not None} | "
            f"importance_threshold={importance_threshold} | "
            f"similarity_threshold={similarity_threshold}"
        )

    def _generate_trace_id(self) -> str:
        """ç”Ÿæˆè¿½è¸ªIDç”¨äºå…³è”æ—¥å¿—"""
        return str(uuid.uuid4())[:8]

    def _parse_event_date(
            self,
            analysis: Dict[str, Any],
            user_message: str,
            reference_time: Optional[datetime] = None,
            trace_id: str = ""
    ) -> Optional[datetime]:
        """
        æ™ºèƒ½è§£æäº‹ä»¶æ—¥æœŸ

        ä¼˜å…ˆä½¿ç”¨LLMæå–çš„æ—¥æœŸï¼Œå¦‚æœæ˜¯ç›¸å¯¹æ—¶é—´è¡¨è¾¾åˆ™ä½¿ç”¨DateParserè§£æã€‚
        å¦‚æœLLMæ²¡æœ‰æå–åˆ°æ—¥æœŸï¼Œåˆ™å°è¯•ä»ç”¨æˆ·æ¶ˆæ¯ä¸­ç›´æ¥æå–ã€‚

        Args:
            analysis: LLMåˆ†æç»“æœ
            user_message: ç”¨æˆ·åŸå§‹æ¶ˆæ¯
            reference_time: å‚è€ƒæ—¶é—´ï¼ˆé»˜è®¤ä½¿ç”¨å½“å‰ç³»ç»Ÿæ—¶é—´ï¼‰
            trace_id: è¿½è¸ªIDç”¨äºæ—¥å¿—

        Returns:
            è§£æå‡ºçš„æ—¥æœŸï¼Œå¦‚æœæ— æ³•è§£æåˆ™è¿”å›None
        """
        reference_time = reference_time or datetime.now()
        parser = DateParser(reference_time)

        logger.debug(
            f"ğŸ“… [Memory-DateParse][{trace_id}] START date parsing | "
            f"reference_time={reference_time.strftime('%Y-%m-%d %H:%M:%S')}"
        )

        # 1. å°è¯•ä½¿ç”¨LLMæå–çš„å…·ä½“æ—¥æœŸ (YYYY-MM-DDæ ¼å¼)
        llm_date = analysis.get("event_date")
        if llm_date and isinstance(llm_date, str):
            try:
                parsed = datetime.strptime(llm_date, "%Y-%m-%d")
                logger.debug(
                    f"ğŸ“… [Memory-DateParse][{trace_id}] Parsed from LLM event_date | "
                    f"input={llm_date} | result={parsed.strftime('%Y-%m-%d')}"
                )
                return parsed
            except ValueError:
                logger.debug(
                    f"ğŸ“… [Memory-DateParse][{trace_id}] Failed to parse LLM event_date as YYYY-MM-DD | "
                    f"input={llm_date}"
                )

        # 2. å°è¯•ä½¿ç”¨LLMæå–çš„åŸå§‹æ—¶é—´è¡¨è¾¾
        raw_expression = analysis.get("raw_date_expression")
        if raw_expression and isinstance(raw_expression, str):
            parsed = parser.parse(raw_expression)
            if parsed:
                logger.debug(
                    f"ğŸ“… [Memory-DateParse][{trace_id}] Parsed from raw_date_expression | "
                    f"expression='{raw_expression}' | result={parsed.strftime('%Y-%m-%d')}"
                )
                return parsed
            else:
                logger.debug(
                    f"ğŸ“… [Memory-DateParse][{trace_id}] Failed to parse raw_date_expression | "
                    f"expression='{raw_expression}'"
                )

        # 3. å°è¯•ä»ç”¨æˆ·æ¶ˆæ¯ä¸­ç›´æ¥æå–æ—¥æœŸ
        parsed = parser.parse_from_message(user_message)
        if parsed:
            logger.debug(
                f"ğŸ“… [Memory-DateParse][{trace_id}] Parsed from user_message | "
                f"result={parsed.strftime('%Y-%m-%d')}"
            )
            return parsed

        logger.debug(f"ğŸ“… [Memory-DateParse][{trace_id}] No date found in message")
        return None

    async def analyze_importance(
            self,
            user_message: str,
            bot_response: str
    ) -> Dict[str, Any]:
        """
        åˆ†æå¯¹è¯çš„é‡è¦æ€§

        ä½¿ç”¨LLMåˆ†æç”¨æˆ·æ¶ˆæ¯å’ŒBotå›å¤ï¼Œåˆ¤æ–­æ˜¯å¦åŒ…å«é‡è¦äº‹ä»¶ã€‚

        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            bot_response: Botå›å¤

        Returns:
            DictåŒ…å«åˆ†æç»“æœï¼šis_important, importance_level, event_typeç­‰
        """
        trace_id = self._generate_trace_id()
        start_time = time.perf_counter()

        # è®°å½•è¾“å…¥
        user_msg_preview = user_message[:100] + "..." if len(user_message) > 100 else user_message
        bot_resp_preview = bot_response[:100] + "..." if len(bot_response) > 100 else bot_response

        logger.debug(
            f"ğŸ” [Memory-Analyze][{trace_id}] START importance analysis | "
            f"user_message_length={len(user_message)} | bot_response_length={len(bot_response)}"
        )
        logger.debug(f"ğŸ” [Memory-Analyze][{trace_id}] user_message_preview: {user_msg_preview}")
        logger.debug(f"ğŸ” [Memory-Analyze][{trace_id}] bot_response_preview: {bot_resp_preview}")

        if not self.llm_provider:
            # å¦‚æœæ²¡æœ‰LLMï¼Œä½¿ç”¨ç®€å•çš„è§„åˆ™åˆ¤æ–­
            logger.debug(f"ğŸ” [Memory-Analyze][{trace_id}] No LLM provider, using rule-based analysis")
            result = self._analyze_importance_rule_based(user_message, trace_id)
            latency_ms = (time.perf_counter() - start_time) * 1000
            logger.debug(
                f"ğŸ” [Memory-Analyze][{trace_id}] END rule-based analysis | "
                f"latency={latency_ms:.1f}ms | result={json.dumps(result, ensure_ascii=False)}"
            )
            return result

        try:
            # åœ¨promptä¸­åŒ…å«å½“å‰æ—¶é—´ï¼Œå¸®åŠ©LLMç†è§£ç›¸å¯¹æ—¶é—´
            current_time_str = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")
            analysis_prompt = f"""å½“å‰æ—¶é—´: {current_time_str}

ç”¨æˆ·æ¶ˆæ¯: {user_message}
AIå›å¤: {bot_response}

è¯·åˆ†æè¿™æ®µå¯¹è¯æ˜¯å¦åŒ…å«å€¼å¾—è®°ä½çš„é‡è¦äº‹ä»¶ã€‚"""

            logger.debug(f"ğŸ” [Memory-Analyze][{trace_id}] Calling LLM for importance analysis...")
            llm_start_time = time.perf_counter()

            response = await self.llm_provider.generate_response(
                [{"role": "user", "content": analysis_prompt}],
                context=self.IMPORTANCE_ANALYSIS_PROMPT
            )

            llm_latency_ms = (time.perf_counter() - llm_start_time) * 1000
            logger.debug(
                f"ğŸ” [Memory-Analyze][{trace_id}] LLM response received | "
                f"llm_latency={llm_latency_ms:.1f}ms | response_length={len(response)}"
            )
            logger.debug(f"ğŸ” [Memory-Analyze][{trace_id}] LLM raw response: {response[:500]}")

            # è§£æJSONå“åº”
            response_text = response.strip()
            if response_text.startswith("```"):
                response_text = response_text.split("```")[1]
                if response_text.startswith("json"):
                    response_text = response_text[4:]

            result = json.loads(response_text.strip())

            latency_ms = (time.perf_counter() - start_time) * 1000
            logger.debug(
                f"ğŸ” [Memory-Analyze][{trace_id}] END LLM analysis | "
                f"total_latency={latency_ms:.1f}ms | "
                f"is_important={result.get('is_important')} | "
                f"importance_level={result.get('importance_level')} | "
                f"event_type={result.get('event_type')} | "
                f"event_date={result.get('event_date')} | "
                f"raw_date_expression={result.get('raw_date_expression')}"
            )
            logger.debug(f"ğŸ” [Memory-Analyze][{trace_id}] Full result: {json.dumps(result, ensure_ascii=False)}")

            return result

        except json.JSONDecodeError as e:
            latency_ms = (time.perf_counter() - start_time) * 1000
            logger.warning(
                f"âš ï¸ [Memory-Analyze][{trace_id}] Failed to parse LLM response as JSON | "
                f"latency={latency_ms:.1f}ms | error={e}"
            )
            return {"is_important": False}
        except Exception as e:
            latency_ms = (time.perf_counter() - start_time) * 1000
            logger.error(
                f"âŒ [Memory-Analyze][{trace_id}] Error in importance analysis | "
                f"latency={latency_ms:.1f}ms | error_type={type(e).__name__} | error={e}"
            )
            return {"is_important": False}

    def _analyze_importance_rule_based(self, user_message: str, trace_id: str = "") -> Dict[str, Any]:
        """
        åŸºäºè§„åˆ™çš„é‡è¦æ€§åˆ†æï¼ˆå½“æ²¡æœ‰LLMæ—¶ä½¿ç”¨ï¼‰

        ä½¿ç”¨å…³é”®è¯åŒ¹é…è¿›è¡Œç®€å•çš„é‡è¦æ€§åˆ¤æ–­ã€‚
        """
        message_lower = user_message.lower()

        # æ—¥å¸¸å¯’æš„å…³é”®è¯ï¼ˆä½é‡è¦æ€§ï¼‰
        greetings = ["ä½ å¥½", "hello", "hi", "å†è§", "bye", "è°¢è°¢", "thanks",
                     "æ—©ä¸Šå¥½", "æ™šä¸Šå¥½", "æ—©å®‰", "æ™šå®‰", "good morning", "good night"]

        if any(greeting in message_lower for greeting in greetings) and len(user_message) < 20:
            logger.debug(
                f"ğŸ” [Memory-Analyze][{trace_id}] Rule-based: detected greeting, marking as not important"
            )
            return {"is_important": False}

        # é‡è¦äº‹ä»¶å…³é”®è¯
        important_keywords = {
            "birthday": ["ç”Ÿæ—¥", "birthday", "å‡ºç”Ÿ"],
            "preference": ["å–œæ¬¢", "ä¸å–œæ¬¢", "çˆ±å¥½", "å…´è¶£", "å–œå¥½", "favorite", "prefer"],
            "goal": ["ç›®æ ‡", "è®¡åˆ’", "æ‰“ç®—", "æƒ³è¦", "å¸Œæœ›", "goal", "plan"],
            "life_event": ["æ¯•ä¸š", "å·¥ä½œ", "ç»“å©š", "æ¬å®¶", "ç”Ÿç—…", "æ‹çˆ±"],
            "emotion": ["éš¾è¿‡", "å¼€å¿ƒ", "ç„¦è™‘", "å‹åŠ›", "æ‹…å¿ƒ", "å®³æ€•"],
            "relationship": ["æœ‹å‹", "å®¶äºº", "çˆ¶æ¯", "å­©å­", "ç”·æœ‹å‹", "å¥³æœ‹å‹"]
        }

        for event_type, keywords in important_keywords.items():
            matched_keywords = [kw for kw in keywords if kw in message_lower]
            if matched_keywords:
                # å°è¯•ä»æ¶ˆæ¯ä¸­æå–æ—¥æœŸ
                parser = DateParser()
                parsed_date = parser.parse_from_message(user_message)
                raw_date_expr = None

                # å°è¯•æ‰¾åˆ°åŸç»„åˆçš„æ—¶é—´è¡¨è¾¾
                date_patterns = [
                    r'å¤§?å‰å¤©|æ˜¨å¤©|ä»Šå¤©|æ˜å¤©|å¤§?åå¤©',
                    r'ä¸Š{1,2}å‘¨|è¿™å‘¨|æœ¬å‘¨|ä¸‹{1,2}å‘¨',
                    r'ä¸Š{1,2}ä¸ª?æœˆ|è¿™ä¸ª?æœˆ|æœ¬æœˆ|ä¸‹{1,2}ä¸ª?æœˆ',
                    r'å‰å¹´|å»å¹´|ä»Šå¹´|æ˜å¹´|åå¹´',
                    r'(?:ä¸Š{1,2}å‘¨|è¿™å‘¨|æœ¬å‘¨|ä¸‹{1,2}å‘¨)?(?:å‘¨|æ˜ŸæœŸ)[ä¸€äºŒä¸‰å››äº”å…­æ—¥å¤©]',
                    r'\d{1,2}æœˆ\d{1,2}[æ—¥å·]',
                    r'\d{1,2}[æ—¥å·]',
                ]
                for pattern in date_patterns:
                    match = re.search(pattern, user_message)
                    if match:
                        raw_date_expr = match.group()
                        break

                logger.debug(
                    f"ğŸ” [Memory-Analyze][{trace_id}] Rule-based: matched keywords {matched_keywords} | "
                    f"event_type={event_type} | "
                    f"parsed_date={parsed_date.strftime('%Y-%m-%d') if parsed_date else None} | "
                    f"raw_date_expression={raw_date_expr}"
                )

                return {
                    "is_important": True,
                    "importance_level": MemoryImportance.MEDIUM.value,
                    "event_type": event_type,
                    "event_summary": user_message[:100],
                    "keywords": matched_keywords,
                    "event_date": parsed_date.strftime("%Y-%m-%d") if parsed_date else None,
                    "raw_date_expression": raw_date_expr
                }

        logger.debug(f"ğŸ” [Memory-Analyze][{trace_id}] Rule-based: no important keywords found")
        return {"is_important": False}

    async def extract_and_save_important_events(
            self,
            user_id: int,
            bot_id: Optional[int],
            user_message: str,
            bot_response: str
    ) -> Optional[UserMemory]:
        """
        æå–å¹¶ä¿å­˜é‡è¦å¯¹è¯äº‹ä»¶

        åˆ†æå¯¹è¯å†…å®¹ï¼Œå¦‚æœåŒ…å«é‡è¦äº‹ä»¶åˆ™ä¿å­˜åˆ°æ•°æ®åº“ã€‚
        å¦‚æœé…ç½®äº†embedding_serviceï¼Œä¼šè‡ªåŠ¨ç”Ÿæˆå‘é‡åµŒå…¥ã€‚

        Args:
            user_id: ç”¨æˆ·ID
            bot_id: Bot ID
            user_message: ç”¨æˆ·æ¶ˆæ¯
            bot_response: Botå›å¤

        Returns:
            ä¿å­˜çš„UserMemoryå¯¹è±¡ï¼Œå¦‚æœä¸é‡è¦åˆ™è¿”å›None
        """
        trace_id = self._generate_trace_id()
        start_time = time.perf_counter()
        reference_time = datetime.now()  # è®°å½•å¤„ç†æ—¶çš„å‚è€ƒæ—¶é—´

        logger.debug(
            f"ğŸ“ [Memory-Extract][{trace_id}] START extract_and_save_important_events | "
            f"user_id={user_id} | bot_id={bot_id} | "
            f"reference_time={reference_time.strftime('%Y-%m-%d %H:%M:%S')}"
        )
        logger.debug(
            f"ğŸ“ [Memory-Extract][{trace_id}] Input lengths: "
            f"user_message={len(user_message)} chars | bot_response={len(bot_response)} chars"
        )

        # ==================== Step 1: åˆ†æé‡è¦æ€§ ====================
        logger.debug(f"ğŸ“ [Memory-Extract][{trace_id}] Step 1: Analyzing importance...")
        analysis_start = time.perf_counter()

        analysis = await self.analyze_importance(user_message, bot_response)

        analysis_latency = (time.perf_counter() - analysis_start) * 1000
        logger.debug(
            f"ğŸ“ [Memory-Extract][{trace_id}] Step 1 completed | "
            f"latency={analysis_latency:.1f}ms | "
            f"is_important={analysis.get('is_important')} | "
            f"importance_level={analysis.get('importance_level')}"
        )

        # ==================== Step 2: æ£€æŸ¥æ˜¯å¦é‡è¦ ====================
        if not analysis.get("is_important", False):
            latency_ms = (time.perf_counter() - start_time) * 1000
            logger.debug(
                f"ğŸ“ [Memory-Extract][{trace_id}] END - Message not important | "
                f"total_latency={latency_ms:.1f}ms | action=skipped"
            )
            return None

        # ==================== Step 3: æ£€æŸ¥é‡è¦æ€§çº§åˆ«æ˜¯å¦è¾¾åˆ°é˜ˆå€¼ ====================
        importance_level = analysis.get("importance_level", MemoryImportance.MEDIUM.value)
        current_level_order = self._importance_order.get(importance_level, 0)
        threshold_order = self._importance_order.get(self.importance_threshold, 1)

        logger.debug(
            f"ğŸ“ [Memory-Extract][{trace_id}] Step 2: Checking threshold | "
            f"importance_level={importance_level} (order={current_level_order}) | "
            f"threshold={self.importance_threshold} (order={threshold_order})"
        )

        if current_level_order < threshold_order:
            latency_ms = (time.perf_counter() - start_time) * 1000
            logger.debug(
                f"ğŸ“ [Memory-Extract][{trace_id}] END - Importance level {importance_level} "
                f"below threshold {self.importance_threshold} | "
                f"total_latency={latency_ms:.1f}ms | action=skipped"
            )
            return None

        logger.debug(f"ğŸ“ [Memory-Extract][{trace_id}] Threshold check passed, continuing to save...")

        # ==================== Step 4: æ™ºèƒ½è§£æäº‹ä»¶æ—¥æœŸ ====================
        logger.debug(f"ğŸ“ [Memory-Extract][{trace_id}] Step 3: Parsing event date...")

        event_date = self._parse_event_date(
            analysis=analysis,
            user_message=user_message,
            reference_time=reference_time,
            trace_id=trace_id
        )

        if event_date:
            logger.debug(
                f"ğŸ“ [Memory-Extract][{trace_id}] Event date parsed: {event_date.strftime('%Y-%m-%d')} | "
                f"raw_expression={analysis.get('raw_date_expression')}"
            )
        else:
            logger.debug(f"ğŸ“ [Memory-Extract][{trace_id}] No event date found")

        # ==================== Step 5: è·å–äº‹ä»¶æ‘˜è¦ ====================
        event_summary = analysis.get("event_summary", user_message[:200])
        logger.debug(
            f"ğŸ“ [Memory-Extract][{trace_id}] Step 4: Event summary | "
            f"length={len(event_summary)} | preview={event_summary[:80]}..."
        )

        # ==================== Step 6: ç”Ÿæˆå‘é‡åµŒå…¥ ====================
        embedding = None
        embedding_model = None
        embedding_dim = 0

        logger.debug(
            f"ğŸ“ [Memory-Extract][{trace_id}] Step 5: Generating embedding | "
            f"embedding_service_available={self.embedding_service is not None} | "
            f"provider_available={self.embedding_service.provider is not None if self.embedding_service else False}"
        )

        if self.embedding_service and self.embedding_service.provider:
            try:
                embedding_start = time.perf_counter()
                logger.debug(
                    f"ğŸ”¢ [Memory-Embedding][{trace_id}] START embedding generation | "
                    f"text_length={len(event_summary)}"
                )

                result = await self.embedding_service.embed_text(event_summary)

                embedding = result.embedding
                embedding_model = result.model
                embedding_dim = len(embedding) if embedding else 0
                embedding_latency = (time.perf_counter() - embedding_start) * 1000

                # è®¡ç®—åµŒå…¥å‘é‡çš„ä¸€äº›ç»Ÿè®¡ä¿¡æ¯ç”¨äºè°ƒè¯•
                if embedding:
                    embedding_array = np.array(embedding)
                    embedding_stats = {
                        "dim": embedding_dim,
                        "min": float(np.min(embedding_array)),
                        "max": float(np.max(embedding_array)),
                        "mean": float(np.mean(embedding_array)),
                        "std": float(np.std(embedding_array)),
                        "norm": float(np.linalg.norm(embedding_array))
                    }
                else:
                    embedding_stats = {}

                logger.debug(
                    f"ğŸ”¢ [Memory-Embedding][{trace_id}] END embedding generation | "
                    f"latency={embedding_latency:.1f}ms | "
                    f"model={embedding_model} | dim={embedding_dim}"
                )
                logger.debug(
                    f"ğŸ”¢ [Memory-Embedding][{trace_id}] Embedding stats: {json.dumps(embedding_stats)}"
                )

            except Exception as e:
                embedding_latency = (time.perf_counter() - embedding_start) * 1000
                logger.warning(
                    f"âš ï¸ [Memory-Embedding][{trace_id}] Failed to generate embedding | "
                    f"latency={embedding_latency:.1f}ms | error_type={type(e).__name__} | error={e}"
                )
        else:
            logger.debug(f"ğŸ“ [Memory-Extract][{trace_id}] Skipping embedding generation (service not available)")

        # ==================== Step 7: åˆ›å»ºå¹¶ä¿å­˜è®°å¿†å¯¹è±¡ ====================
        logger.debug(f"ğŸ“ [Memory-Extract][{trace_id}] Step 6: Creating UserMemory object...")

        memory = UserMemory(
            user_id=user_id,
            bot_id=bot_id,
            event_summary=event_summary,
            user_message=user_message,
            bot_response=bot_response,
            importance=importance_level,
            event_type=analysis.get("event_type"),
            keywords=analysis.get("keywords", []),
            event_date=event_date,
            embedding=embedding,
            embedding_model=embedding_model
        )

        logger.debug(
            f"ğŸ“ [Memory-Extract][{trace_id}] UserMemory object created | "
            f"event_type={analysis.get('event_type')} | "
            f"keywords={analysis.get('keywords', [])} | "
            f"event_date={event_date.strftime('%Y-%m-%d') if event_date else None} | "
            f"has_embedding={embedding is not None} | "
            f"embedding_dim={embedding_dim}"
        )

        # ==================== Step 8: ä¿å­˜åˆ°æ•°æ®åº“ ====================
        db_start = time.perf_counter()
        logger.debug(f"ğŸ’¾ [Memory-DB][{trace_id}] START database save...")

        try:
            self.db.add(memory)
            await self.db.commit()
            await self.db.refresh(memory)

            db_latency = (time.perf_counter() - db_start) * 1000
            logger.debug(
                f"ğŸ’¾ [Memory-DB][{trace_id}] END database save | "
                f"latency={db_latency:.1f}ms | memory_id={memory.id} | uuid={memory.uuid}"
            )
        except Exception as e:
            db_latency = (time.perf_counter() - db_start) * 1000
            logger.error(
                f"âŒ [Memory-DB][{trace_id}] Database save failed | "
                f"latency={db_latency:.1f}ms | error_type={type(e).__name__} | error={e}"
            )
            raise

        # ==================== å®Œæˆ ====================
        total_latency = (time.perf_counter() - start_time) * 1000
        logger.info(
            f"âœ… [Memory-Extract][{trace_id}] END extract_and_save_important_events | "
            f"total_latency={total_latency:.1f}ms | "
            f"user_id={user_id} | memory_id={memory.id} | "
            f"importance={importance_level} | event_type={analysis.get('event_type')} | "
            f"event_date={event_date.strftime('%Y-%m-%d') if event_date else None} | "
            f"has_embedding={embedding is not None}"
        )
        logger.debug(
            f"ğŸ“ [Memory-Extract][{trace_id}] Saved memory summary: {event_summary[:80]}..."
        )

        return memory

    async def retrieve_memories(
            self,
            user_id: int,
            bot_id: Optional[int] = None,
            current_message: Optional[str] = None,
            event_types: Optional[List[str]] = None,
            limit: Optional[int] = None,
            use_vector_search: bool = True,
            skip_llm_analysis: bool = False
    ) -> List[UserMemory]:
        """
        æ£€ç´¢ç”¨æˆ·çš„ç›¸å…³è®°å¿†

        ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢ä¸å½“å‰æ¶ˆæ¯æœ€ç›¸å…³çš„å†å²è®°å¿†ã€‚
        å¦‚æœæ²¡æœ‰embeddingæˆ–current_messageä¸ºç©ºï¼Œåˆ™å›é€€åˆ°ä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„æ£€ç´¢ã€‚

        Args:
            user_id: ç”¨æˆ·ID
            bot_id: Bot IDï¼ˆå¯é€‰ï¼ŒæŒ‡å®šåˆ™åªæ£€ç´¢è¯¥Botç›¸å…³çš„è®°å¿†ï¼‰
            current_message: å½“å‰æ¶ˆæ¯ï¼ˆç”¨äºå‘é‡ç›¸ä¼¼åº¦åŒ¹é…ï¼‰
            event_types: äº‹ä»¶ç±»å‹è¿‡æ»¤åˆ—è¡¨
            limit: è¿”å›æ•°é‡é™åˆ¶
            use_vector_search: æ˜¯å¦ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æœç´¢
            skip_llm_analysis: æ˜¯å¦è·³è¿‡LLMåˆ†æï¼ˆé¿å…é¢å¤–çš„LLMè°ƒç”¨ï¼‰

        Returns:
            ç›¸å…³è®°å¿†åˆ—è¡¨ï¼ˆæŒ‰ç›¸ä¼¼åº¦/é‡è¦æ€§æ’åºï¼‰
        """
        trace_id = self._generate_trace_id()
        start_time = time.perf_counter()
        limit = limit or self.max_memories_per_query

        logger.debug(
            f"ğŸ” [Memory-Retrieve][{trace_id}] START retrieve_memories | "
            f"user_id={user_id} | bot_id={bot_id} | limit={limit} | "
            f"use_vector_search={use_vector_search} | "
            f"has_current_message={current_message is not None}"
        )

        if current_message:
            logger.debug(
                f"ğŸ” [Memory-Retrieve][{trace_id}] current_message preview: "
                f"{current_message[:100]}{'...' if len(current_message) > 100 else ''}"
            )

        # å°è¯•ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢
        if (use_vector_search and
                current_message and
                self.embedding_service and
                self.embedding_service.provider):
            try:
                logger.debug(f"ğŸ” [Memory-Retrieve][{trace_id}] Attempting vector similarity search...")

                memories = await self._retrieve_by_vector_similarity(
                    user_id=user_id,
                    bot_id=bot_id,
                    current_message=current_message,
                    event_types=event_types,
                    limit=limit,
                    trace_id=trace_id
                )
                if memories:
                    latency_ms = (time.perf_counter() - start_time) * 1000
                    logger.info(
                        f"âœ… [Memory-Retrieve][{trace_id}] END vector search | "
                        f"latency={latency_ms:.1f}ms | retrieved={len(memories)} memories"
                    )
                    return memories

                logger.debug(f"ğŸ” [Memory-Retrieve][{trace_id}] Vector search returned no results, falling back...")

            except Exception as e:
                logger.warning(
                    f"âš ï¸ [Memory-Retrieve][{trace_id}] Vector search failed, falling back to metadata retrieval | "
                    f"error_type={type(e).__name__} | error={e}"
                )
        else:
            reasons = []
            if not use_vector_search:
                reasons.append("vector_search_disabled")
            if not current_message:
                reasons.append("no_current_message")
            if not self.embedding_service:
                reasons.append("no_embedding_service")
            elif not self.embedding_service.provider:
                reasons.append("no_embedding_provider")

            logger.debug(
                f"ğŸ” [Memory-Retrieve][{trace_id}] Skipping vector search | reasons={reasons}"
            )

        # å›é€€åˆ°ä¼ ç»Ÿæ£€ç´¢
        logger.debug(f"ğŸ” [Memory-Retrieve][{trace_id}] Using metadata-based retrieval...")
        memories = await self._retrieve_by_metadata(
            user_id=user_id,
            bot_id=bot_id,
            current_message=current_message,
            event_types=event_types,
            limit=limit,
            skip_llm_analysis=skip_llm_analysis,
            trace_id=trace_id
        )

        latency_ms = (time.perf_counter() - start_time) * 1000
        logger.info(
            f"âœ… [Memory-Retrieve][{trace_id}] END metadata search | "
            f"latency={latency_ms:.1f}ms | retrieved={len(memories)} memories"
        )

        return memories

    async def _retrieve_by_vector_similarity(
            self,
            user_id: int,
            bot_id: Optional[int],
            current_message: str,
            event_types: Optional[List[str]],
            limit: int,
            trace_id: str = ""
    ) -> List[UserMemory]:
        """
        ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢è®°å¿†

        1. ç”ŸæˆæŸ¥è¯¢æ¶ˆæ¯çš„å‘é‡åµŒå…¥
        2. ä»æ•°æ®åº“è·å–ç”¨æˆ·çš„æ‰€æœ‰æœ‰embeddingçš„è®°å¿†
        3. è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ’åº
        4. è¿”å›æœ€ç›¸å…³çš„è®°å¿†
        """
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        logger.debug(f"ğŸ”¢ [Memory-VectorSearch][{trace_id}] Generating query embedding...")
        embedding_start = time.perf_counter()

        query_result = await self.embedding_service.embed_text(current_message)
        query_embedding = np.array(query_result.embedding, dtype=np.float32)

        embedding_latency = (time.perf_counter() - embedding_start) * 1000
        logger.debug(
            f"ğŸ”¢ [Memory-VectorSearch][{trace_id}] Query embedding generated | "
            f"latency={embedding_latency:.1f}ms | dim={len(query_embedding)} | "
            f"model={query_result.model}"
        )

        # æ„å»ºåŸºç¡€æŸ¥è¯¢ - è·å–æ‰€æœ‰æœ‰embeddingçš„è®°å¿†
        query = select(UserMemory).where(
            and_(
                UserMemory.user_id == user_id,
                UserMemory.is_active == True,
                UserMemory.embedding.isnot(None)
            )
        )

        # å¦‚æœæŒ‡å®šäº†Bot IDï¼Œæ·»åŠ è¿‡æ»¤
        if bot_id is not None:
            query = query.where(
                or_(
                    UserMemory.bot_id == bot_id,
                    UserMemory.bot_id.is_(None)
                )
            )

        # å¦‚æœæŒ‡å®šäº†äº‹ä»¶ç±»å‹ï¼Œæ·»åŠ è¿‡æ»¤
        if event_types:
            query = query.where(UserMemory.event_type.in_(event_types))
            logger.debug(f"ğŸ”¢ [Memory-VectorSearch][{trace_id}] Filtering by event_types: {event_types}")

        db_start = time.perf_counter()
        result = await self.db.execute(query)
        memories = list(result.scalars().all())
        db_latency = (time.perf_counter() - db_start) * 1000

        logger.debug(
            f"ğŸ”¢ [Memory-VectorSearch][{trace_id}] Fetched {len(memories)} memories with embeddings | "
            f"db_latency={db_latency:.1f}ms"
        )

        if not memories:
            return []

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        logger.debug(f"ğŸ”¢ [Memory-VectorSearch][{trace_id}] Computing cosine similarities...")
        similarity_start = time.perf_counter()

        scored_memories: List[Tuple[UserMemory, float]] = []
        for memory in memories:
            if memory.embedding:
                memory_embedding = np.array(memory.embedding, dtype=np.float32)
                similarity = self._cosine_similarity(query_embedding, memory_embedding)

                logger.debug(
                    f"ğŸ”¢ [Memory-VectorSearch][{trace_id}] Memory {memory.id}: "
                    f"similarity={similarity:.4f} | threshold={self.similarity_threshold} | "
                    f"preview={memory.event_summary[:50]}..."
                )

                if similarity >= self.similarity_threshold:
                    scored_memories.append((memory, similarity))

        similarity_latency = (time.perf_counter() - similarity_start) * 1000
        logger.debug(
            f"ğŸ”¢ [Memory-VectorSearch][{trace_id}] Similarity computation done | "
            f"latency={similarity_latency:.1f}ms | "
            f"above_threshold={len(scored_memories)}/{len(memories)}"
        )

        # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶å–top_k
        scored_memories.sort(key=lambda x: x[1], reverse=True)
        top_memories = [m for m, _ in scored_memories[:limit]]

        if top_memories:
            logger.debug(f"ğŸ”¢ [Memory-VectorSearch][{trace_id}] Top {len(top_memories)} memories selected:")
            for i, (memory, score) in enumerate(scored_memories[:limit]):
                logger.debug(
                    f"  [{i + 1}] id={memory.id} | similarity={score:.4f} | "
                    f"type={memory.event_type} | summary={memory.event_summary[:60]}..."
                )

        # æ›´æ–°è®¿é—®è®¡æ•°å’Œæ—¶é—´
        if top_memories:
            memory_ids = [m.id for m in top_memories]
            await self.db.execute(
                update(UserMemory)
                .where(UserMemory.id.in_(memory_ids))
                .values(
                    access_count=UserMemory.access_count + 1,
                    last_accessed_at=datetime.utcnow()
                )
            )
            await self.db.commit()
            logger.debug(f"ğŸ”¢ [Memory-VectorSearch][{trace_id}] Updated access count for {len(memory_ids)} memories")

        return top_memories

    async def _retrieve_by_metadata(
            self,
            user_id: int,
            bot_id: Optional[int],
            current_message: Optional[str],
            event_types: Optional[List[str]],
            limit: int,
            skip_llm_analysis: bool = False,
            trace_id: str = ""
    ) -> List[UserMemory]:
        """
        ä½¿ç”¨å…ƒæ•°æ®ï¼ˆå…³é”®è¯ã€äº‹ä»¶ç±»å‹ç­‰ï¼‰æ£€ç´¢è®°å¿†

        è¿™æ˜¯å‘é‡æ£€ç´¢ä¸å¯ç”¨æ—¶çš„å›é€€æ–¹æ¡ˆã€‚
        """
        logger.debug(f"ğŸ“‹ [Memory-MetadataSearch][{trace_id}] Building metadata query...")

        # æ„å»ºåŸºç¡€æŸ¥è¯¢
        query = select(UserMemory).where(
            and_(
                UserMemory.user_id == user_id,
                UserMemory.is_active == True
            )
        )

        # å¦‚æœæŒ‡å®šäº†Bot IDï¼Œæ·»åŠ è¿‡æ»¤
        if bot_id is not None:
            query = query.where(
                or_(
                    UserMemory.bot_id == bot_id,
                    UserMemory.bot_id.is_(None)  # ä¹ŸåŒ…æ‹¬é€šç”¨è®°å¿†
                )
            )

        # å¦‚æœæŒ‡å®šäº†äº‹ä»¶ç±»å‹ï¼Œæ·»åŠ è¿‡æ»¤
        if event_types:
            query = query.where(UserMemory.event_type.in_(event_types))
            logger.debug(f"ğŸ“‹ [Memory-MetadataSearch][{trace_id}] Filtering by event_types: {event_types}")

        # å¦‚æœæœ‰å½“å‰æ¶ˆæ¯ä¸”æœ‰LLMï¼Œä¸”æœªè®¾ç½®è·³è¿‡æ ‡å¿—ï¼Œå°è¯•æ™ºèƒ½åŒ¹é…
        if current_message and self.llm_provider and not skip_llm_analysis:
            try:
                logger.debug(f"ğŸ“‹ [Memory-MetadataSearch][{trace_id}] Analyzing retrieval needs with LLM...")
                retrieval_analysis = await self._analyze_retrieval_needs(current_message, trace_id)

                if retrieval_analysis.get("should_retrieve", False):
                    if retrieval_analysis.get("event_types"):
                        query = query.where(
                            UserMemory.event_type.in_(retrieval_analysis["event_types"])
                        )
                        logger.debug(
                            f"ğŸ“‹ [Memory-MetadataSearch][{trace_id}] LLM suggested event_types: "
                            f"{retrieval_analysis['event_types']}"
                        )
            except Exception as e:
                logger.warning(
                    f"âš ï¸ [Memory-MetadataSearch][{trace_id}] Error in retrieval analysis | error={e}"
                )
        elif skip_llm_analysis:
            logger.debug(f"ğŸ“‹ [Memory-MetadataSearch][{trace_id}] Skipping LLM analysis (skip_llm_analysis=True)")

        # æŒ‰é‡è¦æ€§å’Œè®¿é—®æ—¶é—´æ’åº
        query = query.order_by(
            UserMemory.importance.desc(),
            UserMemory.last_accessed_at.desc().nullsfirst(),
            UserMemory.created_at.desc()
        ).limit(limit)

        db_start = time.perf_counter()
        result = await self.db.execute(query)
        memories = list(result.scalars().all())
        db_latency = (time.perf_counter() - db_start) * 1000

        logger.debug(
            f"ğŸ“‹ [Memory-MetadataSearch][{trace_id}] Query executed | "
            f"db_latency={db_latency:.1f}ms | retrieved={len(memories)} memories"
        )

        if memories:
            logger.debug(f"ğŸ“‹ [Memory-MetadataSearch][{trace_id}] Retrieved memories:")
            for i, memory in enumerate(memories):
                logger.debug(
                    f"  [{i + 1}] id={memory.id} | importance={memory.importance} | "
                    f"type={memory.event_type} | summary={memory.event_summary[:60]}..."
                )

        # æ›´æ–°è®¿é—®è®¡æ•°å’Œæ—¶é—´
        if memories:
            memory_ids = [m.id for m in memories]
            await self.db.execute(
                update(UserMemory)
                .where(UserMemory.id.in_(memory_ids))
                .values(
                    access_count=UserMemory.access_count + 1,
                    last_accessed_at=datetime.utcnow()
                )
            )
            await self.db.commit()
            logger.debug(f"ğŸ“‹ [Memory-MetadataSearch][{trace_id}] Updated access count for {len(memory_ids)} memories")

        return memories

    @staticmethod
    def _cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
        """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)

        if norm1 == 0 or norm2 == 0:
            return 0.0

        return float(dot_product / (norm1 * norm2))

    async def _analyze_retrieval_needs(self, current_message: str, trace_id: str = "") -> Dict[str, Any]:
        """
        åˆ†æå½“å‰æ¶ˆæ¯çš„è®°å¿†æ£€ç´¢éœ€æ±‚

        ä½¿ç”¨LLMåˆ¤æ–­å½“å‰æ¶ˆæ¯æ˜¯å¦éœ€è¦å›å¿†å†å²è®°å¿†ã€‚
        """
        start_time = time.perf_counter()
        logger.debug(f"ğŸ” [Memory-RetrievalAnalysis][{trace_id}] START analyzing retrieval needs...")

        try:
            response = await self.llm_provider.generate_response(
                [{"role": "user", "content": f"ç”¨æˆ·æ¶ˆæ¯: {current_message}"}],
                context=self.MEMORY_RETRIEVAL_PROMPT
            )

            latency_ms = (time.perf_counter() - start_time) * 1000
            logger.debug(
                f"ğŸ” [Memory-RetrievalAnalysis][{trace_id}] LLM response received | "
                f"latency={latency_ms:.1f}ms"
            )

            response_text = response.strip()
            if response_text.startswith("```"):
                response_text = response_text.split("```")[1]
                if response_text.startswith("json"):
                    response_text = response_text[4:]

            result = json.loads(response_text.strip())
            logger.debug(
                f"ğŸ” [Memory-RetrievalAnalysis][{trace_id}] END analysis | "
                f"should_retrieve={result.get('should_retrieve')} | "
                f"event_types={result.get('event_types')}"
            )

            return result

        except Exception as e:
            latency_ms = (time.perf_counter() - start_time) * 1000
            logger.warning(
                f"âš ï¸ [Memory-RetrievalAnalysis][{trace_id}] Error in retrieval analysis | "
                f"latency={latency_ms:.1f}ms | error={e}"
            )
            return {"should_retrieve": False}

    async def format_memories_for_context(
            self,
            memories: List[UserMemory],
            max_chars: int = 1000
    ) -> str:
        """
        å°†è®°å¿†æ ¼å¼åŒ–ä¸ºå¯æ³¨å…¥åˆ°å¯¹è¯ä¸Šä¸‹æ–‡çš„å­—ç¬¦ä¸²

        Args:
            memories: è®°å¿†åˆ—è¡¨
            max_chars: æœ€å¤§å­—ç¬¦æ•°é™åˆ¶

        Returns:
            æ ¼å¼åŒ–çš„è®°å¿†å­—ç¬¦ä¸²
        """
        if not memories:
            return ""

        memory_texts = []
        current_length = 0

        for memory in memories:
            memory_text = f"- {memory.event_summary}"
            if memory.event_date:
                memory_text += f" (æ—¥æœŸ: {memory.event_date.strftime('%Y-%m-%d')})"

            if current_length + len(memory_text) > max_chars:
                break

            memory_texts.append(memory_text)
            current_length += len(memory_text)

        if not memory_texts:
            return ""

        return "ã€å…³äºè¿™ä½ç”¨æˆ·çš„è®°å¿†ã€‘\n" + "\n".join(memory_texts)

    async def delete_memory(self, memory_id: int) -> bool:
        """
        è½¯åˆ é™¤æŒ‡å®šè®°å¿†

        Args:
            memory_id: è®°å¿†ID

        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        trace_id = self._generate_trace_id()
        logger.debug(f"ğŸ—‘ï¸ [Memory-Delete][{trace_id}] Deleting memory_id={memory_id}")

        result = await self.db.execute(
            update(UserMemory)
            .where(UserMemory.id == memory_id)
            .values(is_active=False, updated_at=datetime.utcnow())
        )
        await self.db.commit()

        success = result.rowcount > 0
        logger.debug(
            f"ğŸ—‘ï¸ [Memory-Delete][{trace_id}] Delete {'succeeded' if success else 'failed'} | "
            f"memory_id={memory_id} | rows_affected={result.rowcount}"
        )

        return success

    async def get_user_memory_stats(self, user_id: int) -> Dict[str, Any]:
        """
        è·å–ç”¨æˆ·è®°å¿†ç»Ÿè®¡ä¿¡æ¯

        Args:
            user_id: ç”¨æˆ·ID

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        trace_id = self._generate_trace_id()
        logger.debug(f"ğŸ“Š [Memory-Stats][{trace_id}] Getting stats for user_id={user_id}")

        # æ€»è®°å¿†æ•°
        total_query = select(func.count(UserMemory.id)).where(
            and_(UserMemory.user_id == user_id, UserMemory.is_active == True)
        )
        total_result = await self.db.execute(total_query)
        total_count = total_result.scalar() or 0

        # æœ‰embeddingçš„è®°å¿†æ•°
        embedded_query = select(func.count(UserMemory.id)).where(
            and_(
                UserMemory.user_id == user_id,
                UserMemory.is_active == True,
                UserMemory.embedding.isnot(None)
            )
        )
        embedded_result = await self.db.execute(embedded_query)
        embedded_count = embedded_result.scalar() or 0

        # æŒ‰äº‹ä»¶ç±»å‹åˆ†ç»„ç»Ÿè®¡
        type_query = select(
            UserMemory.event_type,
            func.count(UserMemory.id)
        ).where(
            and_(UserMemory.user_id == user_id, UserMemory.is_active == True)
        ).group_by(UserMemory.event_type)

        type_result = await self.db.execute(type_query)
        type_counts = {row[0]: row[1] for row in type_result.all()}

        stats = {
            "total_memories": total_count,
            "embedded_memories": embedded_count,
            "embedding_coverage": embedded_count / total_count if total_count > 0 else 0,
            "by_event_type": type_counts
        }

        logger.debug(f"ğŸ“Š [Memory-Stats][{trace_id}] Stats: {json.dumps(stats)}")

        return stats

    async def backfill_embeddings(
            self,
            user_id: Optional[int] = None,
            batch_size: int = 50
    ) -> Dict[str, int]:
        """
        ä¸ºæ²¡æœ‰embeddingçš„è®°å¿†ç”Ÿæˆå‘é‡åµŒå…¥

        ç”¨äºè¿ç§»ç°æœ‰æ•°æ®æˆ–é‡æ–°ç”Ÿæˆä¸¢å¤±çš„embeddingã€‚

        Args:
            user_id: æŒ‡å®šç”¨æˆ·IDï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™å¤„ç†æ‰€æœ‰ç”¨æˆ·ï¼‰
            batch_size: æ¯æ‰¹å¤„ç†çš„è®°å¿†æ•°é‡

        Returns:
            å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        trace_id = self._generate_trace_id()
        logger.info(
            f"ğŸ”„ [Memory-Backfill][{trace_id}] START backfill_embeddings | "
            f"user_id={user_id} | batch_size={batch_size}"
        )

        if not self.embedding_service or not self.embedding_service.provider:
            logger.warning(
                f"âš ï¸ [Memory-Backfill][{trace_id}] No embedding service configured, cannot backfill"
            )
            return {"processed": 0, "failed": 0, "skipped": 0}

        # æ„å»ºæŸ¥è¯¢ - è·å–æ²¡æœ‰embeddingçš„è®°å¿†
        query = select(UserMemory).where(
            and_(
                UserMemory.is_active == True,
                UserMemory.embedding.is_(None)
            )
        )

        if user_id is not None:
            query = query.where(UserMemory.user_id == user_id)

        query = query.limit(batch_size)

        result = await self.db.execute(query)
        memories = list(result.scalars().all())

        logger.debug(f"ğŸ”„ [Memory-Backfill][{trace_id}] Found {len(memories)} memories without embeddings")

        processed = 0
        failed = 0

        for memory in memories:
            try:
                logger.debug(
                    f"ğŸ”„ [Memory-Backfill][{trace_id}] Processing memory_id={memory.id} | "
                    f"summary={memory.event_summary[:50]}..."
                )

                # ç”Ÿæˆembedding
                embed_result = await self.embedding_service.embed_text(memory.event_summary)

                # æ›´æ–°è®°å¿†
                await self.db.execute(
                    update(UserMemory)
                    .where(UserMemory.id == memory.id)
                    .values(
                        embedding=embed_result.embedding,
                        embedding_model=embed_result.model,
                        updated_at=datetime.utcnow()
                    )
                )
                processed += 1

                logger.debug(
                    f"ğŸ”„ [Memory-Backfill][{trace_id}] memory_id={memory.id} embedding generated | "
                    f"dim={len(embed_result.embedding)}"
                )

            except Exception as e:
                logger.warning(
                    f"âš ï¸ [Memory-Backfill][{trace_id}] Failed to generate embedding for memory_id={memory.id} | "
                    f"error={e}"
                )
                failed += 1

        await self.db.commit()

        remaining = await self._count_memories_without_embedding(user_id)

        result_stats = {
            "processed": processed,
            "failed": failed,
            "remaining": remaining
        }

        logger.info(
            f"âœ… [Memory-Backfill][{trace_id}] END backfill_embeddings | "
            f"processed={processed} | failed={failed} | remaining={remaining}"
        )

        return result_stats

    async def _count_memories_without_embedding(self, user_id: Optional[int] = None) -> int:
        """ç»Ÿè®¡æ²¡æœ‰embeddingçš„è®°å¿†æ•°é‡"""
        query = select(func.count(UserMemory.id)).where(
            and_(
                UserMemory.is_active == True,
                UserMemory.embedding.is_(None)
            )
        )

        if user_id is not None:
            query = query.where(UserMemory.user_id == user_id)

        result = await self.db.execute(query)
        return result.scalar() or 0


# å…¨å±€æœåŠ¡å®ä¾‹è·å–å‡½æ•°
_memory_service_cache: Dict[str, ConversationMemoryService] = {}


def get_conversation_memory_service(
        db: AsyncSession,
        llm_provider=None,
        embedding_service=None
) -> ConversationMemoryService:
    """
    è·å–å¯¹è¯è®°å¿†æœåŠ¡å®ä¾‹

    Args:
        db: æ•°æ®åº“ä¼šè¯
        llm_provider: LLMæä¾›è€…
        embedding_service: å‘é‡åµŒå…¥æœåŠ¡ï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™è‡ªåŠ¨è·å–ï¼‰

    Returns:
        ConversationMemoryServiceå®ä¾‹
    """
    # å¦‚æœæ²¡æœ‰ä¼ å…¥embedding_serviceï¼Œå°è¯•è‡ªåŠ¨è·å–
    if embedding_service is None:
        try:
            from .embedding_service import get_embedding_service
            embedding_service = get_embedding_service()
        except Exception as e:
            logger.warning(f"Could not auto-configure embedding service: {e}")

    return ConversationMemoryService(db, llm_provider, embedding_service)

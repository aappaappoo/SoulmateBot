"""
è¯­éŸ³è¯†åˆ«æœåŠ¡ - Voice Recognition Service
ä½¿ç”¨é˜¿é‡Œäº‘ DashScope çš„ ASR (Automatic Speech Recognition) API
å°†ç”¨æˆ·å‘é€çš„è¯­éŸ³æ¶ˆæ¯è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œå¹¶åˆ†æè¯­éŸ³ä¸­çš„æƒ…ç»ª

Usage:
    from src.services.voice_recognition_service import voice_recognition_service

    result = await voice_recognition_service.recognize_voice(audio_file_path)
    # result.text  -> è¯†åˆ«å‡ºçš„æ–‡æœ¬
    # result.emotion -> è¯†åˆ«å‡ºçš„æƒ…ç»ª (å¦‚ "happy", "sad" ç­‰)
"""
import os
import asyncio
import tempfile
from typing import Optional
from dataclasses import dataclass, field
from loguru import logger

from config import settings

try:
    import dashscope
    from dashscope.audio.asr import Recognition

    DASHSCOPE_ASR_AVAILABLE = True
except ImportError:
    DASHSCOPE_ASR_AVAILABLE = False
    logger.warning("dashscope ASR package not available. Voice recognition will not work.")


# æƒ…ç»ªå…³é”®è¯æ˜ å°„ - ä»è¯­éŸ³è¯†åˆ«æ–‡æœ¬ä¸­æ¨æ–­æƒ…ç»ª
EMOTION_KEYWORDS = {
    "happy": ["å“ˆå“ˆ", "å˜»å˜»", "å¼€å¿ƒ", "é«˜å…´", "å¤ªå¥½äº†", "æ£’", "å¥½å¼€å¿ƒ", "å¤ªæ£’äº†", "è€¶", "å¥½çš„å‘€"],
    "excited": ["å¤ªå‰å®³äº†", "å“‡", "å¤©å‘", "çœŸçš„å—", "ä¸ä¼šå§", "å¤ªæ¿€åŠ¨"],
    "sad": ["å”‰", "éš¾è¿‡", "ä¼¤å¿ƒ", "ä¸å¼€å¿ƒ", "å¥½éš¾è¿‡", "å¿ƒç—›", "å‘œå‘œ", "å‘œ"],
    "angry": ["æ°”æ­»äº†", "çƒ¦æ­»äº†", "è®¨åŒ", "å¤ªè¿‡åˆ†", "ç”Ÿæ°”", "æ€’"],
    "gentle": ["å—¯", "å¥½å§", "å¥½çš„", "è°¢è°¢", "æ„Ÿè°¢", "è¾›è‹¦äº†"],
    "crying": ["å‘œå‘œå‘œ", "å“­äº†", "å¥½å§”å±ˆ", "å§”å±ˆ"],
}


@dataclass
class VoiceRecognitionResult:
    """
    è¯­éŸ³è¯†åˆ«ç»“æœ

    Attributes:
        text: è¯†åˆ«å‡ºçš„æ–‡æœ¬å†…å®¹
        emotion: æ¨æ–­çš„æƒ…ç»ªç±»å‹ (happy, sad, angry, excited, gentle, crying, None)
        duration_ms: éŸ³é¢‘æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰ï¼Œå¦‚æœå¯ç”¨
        confidence: è¯†åˆ«ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰ï¼Œå¦‚æœå¯ç”¨
        raw_response: DashScope API åŸå§‹å“åº”ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    """
    text: str = ""
    emotion: Optional[str] = None
    duration_ms: Optional[int] = None
    confidence: Optional[float] = None
    raw_response: Optional[dict] = field(default=None, repr=False)


class VoiceRecognitionService:
    """
    è¯­éŸ³è¯†åˆ«æœåŠ¡

    ä½¿ç”¨é˜¿é‡Œäº‘ DashScope ASR API å°†è¯­éŸ³è½¬ä¸ºæ–‡æœ¬ï¼Œ
    å¹¶ä»è¯†åˆ«å‡ºçš„æ–‡æœ¬ä¸­æ¨æ–­æƒ…ç»ªã€‚
    """

    # é»˜è®¤ ASR æ¨¡å‹
    DEFAULT_MODEL = "qwen3-asr-flash"

    def __init__(self):
        self.api_key = getattr(settings, 'dashscope_api_key', None)
        self.model = getattr(settings, 'asr_model', self.DEFAULT_MODEL)

        # ä»ç¯å¢ƒå˜é‡è·å– API keyï¼ˆå¦‚æœæœªåœ¨é…ç½®ä¸­è®¾ç½®ï¼‰
        if not self.api_key and 'DASHSCOPE_API_KEY' in os.environ:
            self.api_key = os.environ['DASHSCOPE_API_KEY']

    async def recognize_voice(
        self,
        audio_file_path: str,
    ) -> VoiceRecognitionResult:
        """
        è¯†åˆ«è¯­éŸ³æ–‡ä»¶ä¸­çš„å†…å®¹

        Args:
            audio_file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„ (æ”¯æŒ wav/mp3/ogg/m4a/flac æ ¼å¼)

        Returns:
            VoiceRecognitionResult: åŒ…å«è¯†åˆ«æ–‡æœ¬å’Œæƒ…ç»ªçš„ç»“æœå¯¹è±¡
        """
        logger.info(
            f"ğŸ™ï¸ [ASR] recognize_voice called: file={audio_file_path}, model={self.model}"
        )

        if not DASHSCOPE_ASR_AVAILABLE:
            logger.error("ğŸ™ï¸ [ASR] dashscope package not installed, cannot recognize voice")
            return VoiceRecognitionResult(text="", emotion=None)

        if not self.api_key:
            logger.error("ğŸ™ï¸ [ASR] DashScope API key not configured")
            return VoiceRecognitionResult(text="", emotion=None)

        if not os.path.exists(audio_file_path):
            logger.error(f"ğŸ™ï¸ [ASR] Audio file not found: {audio_file_path}")
            return VoiceRecognitionResult(text="", emotion=None)

        try:
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥ ASR è°ƒç”¨
            result = await asyncio.get_event_loop().run_in_executor(
                None,
                self._sync_recognize,
                audio_file_path,
            )
            return result

        except Exception as e:
            logger.error(f"ğŸ™ï¸ [ASR] Voice recognition error: {e}", exc_info=True)
            return VoiceRecognitionResult(text="", emotion=None)

    def _sync_recognize(
        self,
        audio_file_path: str,
    ) -> VoiceRecognitionResult:
        """
        åŒæ­¥æ–¹å¼æ‰§è¡Œè¯­éŸ³è¯†åˆ«ï¼ˆç”¨äºåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰

        Args:
            audio_file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„

        Returns:
            VoiceRecognitionResult
        """
        try:
            # è®¾ç½® API key
            if self.api_key:
                dashscope.api_key = self.api_key

            logger.info(f"ğŸ™ï¸ [ASR] Calling DashScope Recognition API: model={self.model}")

            # è°ƒç”¨ DashScope Recognition API
            response = Recognition.call(
                model=self.model,
                audio_file=audio_file_path,
            )

            logger.debug(f"ğŸ™ï¸ [ASR] Raw response status: {response.status_code}")

            if response.status_code == 200:
                # æå–è¯†åˆ«æ–‡æœ¬
                output = response.output or {}
                recognized_text = ""

                # DashScope ASR è¿”å›æ ¼å¼ï¼šoutput.sentence æˆ– output.text
                if isinstance(output, dict):
                    recognized_text = output.get("sentence", {}).get("text", "") if isinstance(
                        output.get("sentence"), dict
                    ) else output.get("sentence", "")
                    if not recognized_text:
                        recognized_text = output.get("text", "")
                elif isinstance(output, str):
                    recognized_text = output

                recognized_text = recognized_text.strip()
                logger.info(
                    f"ğŸ™ï¸ [ASR] Recognition successful: text_length={len(recognized_text)}, "
                    f"text='{recognized_text[:100]}...'" if len(recognized_text) > 100
                    else f"ğŸ™ï¸ [ASR] Recognition successful: text='{recognized_text}'"
                )

                # ä»æ–‡æœ¬ä¸­æ¨æ–­æƒ…ç»ª
                emotion = self._infer_emotion_from_text(recognized_text)
                if emotion:
                    logger.info(f"ğŸ™ï¸ [ASR] Inferred emotion: {emotion}")

                return VoiceRecognitionResult(
                    text=recognized_text,
                    emotion=emotion,
                    raw_response=output,
                )
            else:
                logger.error(
                    f"ğŸ™ï¸ [ASR] Recognition failed: status={response.status_code}, "
                    f"message={response.message}"
                )
                return VoiceRecognitionResult(text="", emotion=None)

        except Exception as e:
            logger.error(f"ğŸ™ï¸ [ASR] Error in sync recognition: {e}", exc_info=True)
            return VoiceRecognitionResult(text="", emotion=None)

    @staticmethod
    def _infer_emotion_from_text(text: str) -> Optional[str]:
        """
        ä»è¯†åˆ«å‡ºçš„æ–‡æœ¬ä¸­æ¨æ–­æƒ…ç»ª

        é€šè¿‡å…³é”®è¯åŒ¹é…æ¥æ¨æ–­ç”¨æˆ·è¯­éŸ³ä¸­çš„æƒ…ç»ªå€¾å‘ã€‚

        Args:
            text: è¯†åˆ«å‡ºçš„æ–‡æœ¬

        Returns:
            æƒ…ç»ªæ ‡ç­¾ (happy, sad, angry, excited, gentle, crying) æˆ– None
        """
        if not text:
            return None

        # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥æƒ…ç»ªå…³é”®è¯
        # Priority: angry > crying > sad > excited > happy > gentle
        priority_order = ["angry", "crying", "sad", "excited", "happy", "gentle"]
        for emotion in priority_order:
            keywords = EMOTION_KEYWORDS.get(emotion, [])
            for keyword in keywords:
                if keyword in text:
                    return emotion

        return None


# å…¨å±€è¯­éŸ³è¯†åˆ«æœåŠ¡å®ä¾‹
voice_recognition_service = VoiceRecognitionService()

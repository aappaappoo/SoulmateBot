"""
Embedding Service - å‘é‡åµŒå…¥æœåŠ¡

æä¾›ç»Ÿä¸€çš„æ–‡æœ¬å‘é‡åŒ–æ¥å£ï¼Œæ”¯æŒå¤šç§EmbeddingæœåŠ¡æä¾›å•†ï¼š
1. OpenAI Embedding API
2. DashScope (é€šä¹‰åƒé—®) Text Embedding API

è®¾è®¡ç›®æ ‡ï¼š
- ä¸ºå¯¹è¯è®°å¿†RAGå’Œæœªæ¥çš„æ–‡ä»¶RAGæä¾›å‘é‡åŒ–æ”¯æŒ
- æ”¯æŒå¤šProvideråˆ‡æ¢
- æ”¯æŒæ‰¹é‡å‘é‡åŒ–ä»¥æé«˜æ•ˆç‡
- ç¼“å­˜å¸¸ç”¨embeddingä»¥å‡å°‘APIè°ƒç”¨
"""
import asyncio
from abc import ABC, abstractmethod
from typing import List, Optional, Dict, Any, Union
from dataclasses import dataclass
import numpy as np
from loguru import logger

# Optional imports for providers
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logger.warning("openai package not installed. OpenAI Embedding will not be available.")

try:
    import dashscope
    from dashscope import TextEmbedding
    DASHSCOPE_AVAILABLE = True
except ImportError:
    DASHSCOPE_AVAILABLE = False
    logger.warning("dashscope package not installed. DashScope Embedding will not be available.")


@dataclass
class EmbeddingResult:
    """
    å‘é‡åµŒå…¥ç»“æœ
    
    Attributes:
        embedding: åµŒå…¥å‘é‡
        text: åŸå§‹æ–‡æœ¬
        model: ä½¿ç”¨çš„æ¨¡å‹åç§°
        tokens_used: æ¶ˆè€—çš„tokenæ•°é‡
    """
    embedding: List[float]
    text: str
    model: str
    tokens_used: int = 0
    
    def to_numpy(self) -> np.ndarray:
        """è½¬æ¢ä¸ºnumpyæ•°ç»„"""
        return np.array(self.embedding, dtype=np.float32)


class EmbeddingProvider(ABC):
    """å‘é‡åµŒå…¥ProvideræŠ½è±¡åŸºç±»"""
    
    @property
    @abstractmethod
    def name(self) -> str:
        """Provideråç§°"""
        pass
    
    @property
    @abstractmethod
    def dimension(self) -> int:
        """å‘é‡ç»´åº¦"""
        pass
    
    @abstractmethod
    async def embed_text(self, text: str) -> EmbeddingResult:
        """
        å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œå‘é‡åŒ–
        
        Args:
            text: å¾…å‘é‡åŒ–çš„æ–‡æœ¬
            
        Returns:
            EmbeddingResultå¯¹è±¡
        """
        pass
    
    @abstractmethod
    async def embed_batch(self, texts: List[str]) -> List[EmbeddingResult]:
        """
        æ‰¹é‡å‘é‡åŒ–æ–‡æœ¬
        
        Args:
            texts: å¾…å‘é‡åŒ–çš„æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            EmbeddingResultåˆ—è¡¨
        """
        pass


class OpenAIEmbeddingProvider(EmbeddingProvider):
    """OpenAI Embedding Provider"""
    
    # æ¨¡å‹ç»´åº¦æ˜ å°„
    MODEL_DIMENSIONS = {
        "text-embedding-3-small": 1536,
        "text-embedding-3-large": 3072,
        "text-embedding-ada-002": 1536,
    }
    
    def __init__(
        self,
        api_key: str,
        model: str = "text-embedding-3-small",
        dimensions: Optional[int] = None
    ):
        """
        åˆå§‹åŒ–OpenAI Embedding Provider
        
        Args:
            api_key: OpenAI APIå¯†é’¥
            model: æ¨¡å‹åç§°
            dimensions: è‡ªå®šä¹‰ç»´åº¦ï¼ˆä»…text-embedding-3æ”¯æŒï¼‰
        """
        if not OPENAI_AVAILABLE:
            raise RuntimeError("openai package is not installed")
        
        self.client = openai.AsyncOpenAI(api_key=api_key)
        self.model = model
        self._dimensions = dimensions or self.MODEL_DIMENSIONS.get(model, 1536)
        logger.info(f"OpenAI Embedding Provider initialized with model: {model}")
    
    @property
    def name(self) -> str:
        return "openai"
    
    @property
    def dimension(self) -> int:
        return self._dimensions
    
    async def embed_text(self, text: str) -> EmbeddingResult:
        """å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œå‘é‡åŒ–"""
        try:
            response = await self.client.embeddings.create(
                model=self.model,
                input=text,
                dimensions=self._dimensions if "text-embedding-3" in self.model else None
            )
            
            return EmbeddingResult(
                embedding=response.data[0].embedding,
                text=text,
                model=self.model,
                tokens_used=response.usage.total_tokens
            )
        except Exception as e:
            logger.error(f"OpenAI Embedding error: {e}")
            raise
    
    async def embed_batch(self, texts: List[str]) -> List[EmbeddingResult]:
        """æ‰¹é‡å‘é‡åŒ–æ–‡æœ¬"""
        if not texts:
            return []
        
        try:
            response = await self.client.embeddings.create(
                model=self.model,
                input=texts,
                dimensions=self._dimensions if "text-embedding-3" in self.model else None
            )
            
            results = []
            for i, data in enumerate(response.data):
                results.append(EmbeddingResult(
                    embedding=data.embedding,
                    text=texts[i],
                    model=self.model,
                    tokens_used=response.usage.total_tokens // len(texts)
                ))
            
            return results
        except Exception as e:
            logger.error(f"OpenAI Batch Embedding error: {e}")
            raise


class DashScopeEmbeddingProvider(EmbeddingProvider):
    """
    é˜¿é‡Œäº‘DashScope Embedding Provider
    é€šä¹‰åƒé—®æ–‡æœ¬å‘é‡åŒ–æœåŠ¡
    """
    
    # æ¨¡å‹ç»´åº¦æ˜ å°„
    MODEL_DIMENSIONS = {
        "text-embedding-v1": 1536,
        "text-embedding-v2": 1536,
        "text-embedding-v3": 1024,
    }
    
    def __init__(
        self,
        api_key: str,
        model: str = "text-embedding-v3"
    ):
        """
        åˆå§‹åŒ–DashScope Embedding Provider
        
        Args:
            api_key: DashScope APIå¯†é’¥
            model: æ¨¡å‹åç§°
        """
        if not DASHSCOPE_AVAILABLE:
            raise RuntimeError("dashscope package is not installed")
        
        dashscope.api_key = api_key
        self.model = model
        self._dimensions = self.MODEL_DIMENSIONS.get(model, 1024)
        logger.info(f"DashScope Embedding Provider initialized with model: {model}")
    
    @property
    def name(self) -> str:
        return "dashscope"
    
    @property
    def dimension(self) -> int:
        return self._dimensions
    
    async def embed_text(self, text: str) -> EmbeddingResult:
        """å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œå‘é‡åŒ–"""
        try:
            # DashScopeåŒæ­¥APIï¼Œä½¿ç”¨asyncio.to_threadåŒ…è£…
            def _embed():
                return TextEmbedding.call(
                    model=self.model,
                    input=text
                )
            
            response = await asyncio.to_thread(_embed)
            
            if response.status_code != 200:
                raise Exception(f"DashScope API error: {response.code} - {response.message}")
            
            embedding = response.output['embeddings'][0]['embedding']
            
            return EmbeddingResult(
                embedding=embedding,
                text=text,
                model=self.model,
                tokens_used=response.usage.get('total_tokens', 0) if response.usage else 0
            )
        except Exception as e:
            logger.error(f"DashScope Embedding error: {e}")
            raise
    
    async def embed_batch(self, texts: List[str]) -> List[EmbeddingResult]:
        """æ‰¹é‡å‘é‡åŒ–æ–‡æœ¬"""
        if not texts:
            return []
        
        try:
            # DashScopeæ‰¹é‡å¤„ç†ï¼Œæ¯æ‰¹æœ€å¤š25ä¸ª
            batch_size = 25
            all_results = []
            
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i + batch_size]
                
                def _embed_batch():
                    return TextEmbedding.call(
                        model=self.model,
                        input=batch
                    )
                
                response = await asyncio.to_thread(_embed_batch)
                
                if response.status_code != 200:
                    raise Exception(f"DashScope API error: {response.code} - {response.message}")
                
                for j, emb_data in enumerate(response.output['embeddings']):
                    all_results.append(EmbeddingResult(
                        embedding=emb_data['embedding'],
                        text=batch[j],
                        model=self.model,
                        tokens_used=response.usage.get('total_tokens', 0) // len(batch) if response.usage else 0
                    ))
            
            return all_results
        except Exception as e:
            logger.error(f"DashScope Batch Embedding error: {e}")
            raise


class EmbeddingService:
    """
    ç»Ÿä¸€çš„å‘é‡åµŒå…¥æœåŠ¡
    
    æä¾›å‘é‡ç”Ÿæˆã€ç›¸ä¼¼åº¦è®¡ç®—ç­‰åŠŸèƒ½ã€‚
    æ”¯æŒå¤šProviderå’Œç¼“å­˜æœºåˆ¶ã€‚
    
    Usage:
        service = EmbeddingService()
        
        # ç”Ÿæˆå•ä¸ªå‘é‡
        result = await service.embed_text("ç”¨æˆ·çš„æ¶ˆæ¯å†…å®¹")
        
        # æ‰¹é‡ç”Ÿæˆå‘é‡
        results = await service.embed_batch(["æ–‡æœ¬1", "æ–‡æœ¬2"])
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity = service.cosine_similarity(vec1, vec2)
    """

    def __init__(
            self,
            provider: Optional[EmbeddingProvider] = None,
            enable_cache: bool = True,
            cache_size: int = 1000
    ):
        """
        åˆå§‹åŒ–å‘é‡åµŒå…¥æœåŠ¡

        Args:
            provider: å‘é‡åµŒå…¥Provider
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            cache_size: ç¼“å­˜å¤§å°
        """
        self.provider = provider
        self.enable_cache = enable_cache
        self._cache: Dict[str, EmbeddingResult] = {}
        self._cache_size = cache_size

        if provider:
            logger.info(
                f"ğŸ”¢ EmbeddingService initialized | "
                f"provider={provider.name} | "
                f"dimension={provider.dimension} | "
                f"cache_enabled={enable_cache} | "
                f"cache_size={cache_size}"
            )
    def set_provider(self, provider: EmbeddingProvider) -> None:
        """è®¾ç½®Provider"""
        self.provider = provider
        logger.info(f"EmbeddingService provider set to: {provider.name}")
    
    @property
    def dimension(self) -> int:
        """è·å–å‘é‡ç»´åº¦"""
        if not self.provider:
            raise ValueError("No embedding provider configured")
        return self.provider.dimension
    
    async def embed_text(self, text: str, use_cache: bool = True) -> EmbeddingResult:
        """
        å¯¹æ–‡æœ¬è¿›è¡Œå‘é‡åŒ–
        
        Args:
            text: å¾…å‘é‡åŒ–çš„æ–‡æœ¬
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            EmbeddingResultå¯¹è±¡
        """
        if not self.provider:
            raise ValueError("No embedding provider configured")
        
        # æ£€æŸ¥ç¼“å­˜
        if use_cache and self.enable_cache and text in self._cache:
            logger.debug(f"Embedding cache hit for text: {text[:50]}...")
            return self._cache[text]
        
        # ç”Ÿæˆå‘é‡
        result = await self.provider.embed_text(text)
        
        # ç¼“å­˜ç»“æœ
        if use_cache and self.enable_cache:
            if len(self._cache) >= self._cache_size:
                # ç§»é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
                oldest_key = next(iter(self._cache))
                del self._cache[oldest_key]
            self._cache[text] = result
        
        return result
    
    async def embed_batch(
        self,
        texts: List[str],
        use_cache: bool = True
    ) -> List[EmbeddingResult]:
        """
        æ‰¹é‡å‘é‡åŒ–æ–‡æœ¬
        
        Args:
            texts: å¾…å‘é‡åŒ–çš„æ–‡æœ¬åˆ—è¡¨
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            EmbeddingResultåˆ—è¡¨
        """
        if not self.provider:
            raise ValueError("No embedding provider configured")
        
        if not texts:
            return []
        
        # åˆ†ç¦»ç¼“å­˜å‘½ä¸­å’Œæœªå‘½ä¸­çš„æ–‡æœ¬
        results = [None] * len(texts)
        texts_to_embed = []
        indices_to_embed = []
        
        if use_cache and self.enable_cache:
            for i, text in enumerate(texts):
                if text in self._cache:
                    results[i] = self._cache[text]
                else:
                    texts_to_embed.append(text)
                    indices_to_embed.append(i)
        else:
            texts_to_embed = texts
            indices_to_embed = list(range(len(texts)))
        
        # æ‰¹é‡ç”Ÿæˆæœªå‘½ä¸­çš„å‘é‡
        if texts_to_embed:
            new_results = await self.provider.embed_batch(texts_to_embed)
            
            # Use enumerate to avoid inefficient index lookup
            for idx, (original_idx, result) in enumerate(zip(indices_to_embed, new_results)):
                results[original_idx] = result
                
                # ç¼“å­˜ç»“æœ
                if use_cache and self.enable_cache:
                    if len(self._cache) >= self._cache_size:
                        oldest_key = next(iter(self._cache))
                        del self._cache[oldest_key]
                    self._cache[texts_to_embed[idx]] = result
        
        return results
    
    @staticmethod
    def cosine_similarity(
        vec1: Union[List[float], np.ndarray],
        vec2: Union[List[float], np.ndarray]
    ) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
        
        Args:
            vec1: å‘é‡1
            vec2: å‘é‡2
            
        Returns:
            ä½™å¼¦ç›¸ä¼¼åº¦ (-1 åˆ° 1)
        """
        if isinstance(vec1, list):
            vec1 = np.array(vec1, dtype=np.float32)
        if isinstance(vec2, list):
            vec2 = np.array(vec2, dtype=np.float32)
        
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return float(dot_product / (norm1 * norm2))
    
    @staticmethod
    def batch_cosine_similarity(
        query_vec: Union[List[float], np.ndarray],
        candidate_vecs: List[Union[List[float], np.ndarray]]
    ) -> List[float]:
        """
        è®¡ç®—æŸ¥è¯¢å‘é‡ä¸å¤šä¸ªå€™é€‰å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
        
        Args:
            query_vec: æŸ¥è¯¢å‘é‡
            candidate_vecs: å€™é€‰å‘é‡åˆ—è¡¨
            
        Returns:
            ç›¸ä¼¼åº¦åˆ—è¡¨
        """
        if isinstance(query_vec, list):
            query_vec = np.array(query_vec, dtype=np.float32)
        
        # è½¬æ¢å€™é€‰å‘é‡ä¸ºçŸ©é˜µ
        if not candidate_vecs:
            return []
        
        candidate_matrix = np.array(candidate_vecs, dtype=np.float32)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        query_norm = np.linalg.norm(query_vec)
        if query_norm == 0:
            return [0.0] * len(candidate_vecs)
        
        query_normalized = query_vec / query_norm
        
        # å½’ä¸€åŒ–å€™é€‰å‘é‡
        candidate_norms = np.linalg.norm(candidate_matrix, axis=1, keepdims=True)
        # é¿å…é™¤é›¶
        candidate_norms[candidate_norms == 0] = 1
        candidate_normalized = candidate_matrix / candidate_norms
        
        # æ‰¹é‡è®¡ç®—ç›¸ä¼¼åº¦
        similarities = np.dot(candidate_normalized, query_normalized)
        
        return similarities.tolist()
    
    def clear_cache(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        self._cache.clear()
        logger.info("Embedding cache cleared")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "cache_size": len(self._cache),
            "max_cache_size": self._cache_size,
            "cache_enabled": self.enable_cache
        }


# å…¨å±€æœåŠ¡å®ä¾‹
_embedding_service: Optional[EmbeddingService] = None


def get_embedding_service() -> EmbeddingService:
    """
    è·å–å…¨å±€å‘é‡åµŒå…¥æœåŠ¡å®ä¾‹

    è‡ªåŠ¨æ ¹æ®é…ç½®åˆå§‹åŒ–åˆé€‚çš„Provider
    """
    global _embedding_service

    if _embedding_service is None:
        provider = None

        # å…ˆå°è¯•è‡ªåŠ¨é…ç½®Provider
        try:
            from config import settings

            # ä¼˜å…ˆä½¿ç”¨DashScopeï¼ˆå¦‚æœï¿½ï¿½é…ç½®ï¼‰
            if hasattr(settings, 'dashscope_api_key') and settings.dashscope_api_key and DASHSCOPE_AVAILABLE:
                provider = DashScopeEmbeddingProvider(
                    api_key=settings.dashscope_api_key,
                    model=getattr(settings, 'dashscope_embedding_model', 'text-embedding-v3')
                )
                logger.info("Auto-configured DashScope embedding provider")
            elif hasattr(settings, 'openai_api_key') and settings.openai_api_key and OPENAI_AVAILABLE:
                provider = OpenAIEmbeddingProvider(
                    api_key=settings.openai_api_key,
                    model=getattr(settings, 'openai_embedding_model', 'text-embedding-3-small')
                )
                logger.info("Auto-configured OpenAI embedding provider")
            else:
                logger.warning(
                    "No embedding provider configured. "
                    "Please set DASHSCOPE_API_KEY or OPENAI_API_KEY in your .env file."
                )
        except ImportError:
            logger.warning("Could not import config settings for embedding service")
        except Exception as e:
            logger.error(f"Error auto-configuring embedding provider: {e}")

        # ä½¿ç”¨å·²é…ç½®çš„provideråˆ›å»ºæœåŠ¡ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        _embedding_service = EmbeddingService(provider=provider)

    return _embedding_service
"""
task_engine å•å…ƒæµ‹è¯•

æµ‹è¯•å†…å®¹ï¼š
- æ•°æ®æ¨¡å‹ï¼ˆTask / Step / StepResultï¼‰
- Plannerï¼ˆæ¡Œé¢ä»»åŠ¡è¯†åˆ«ï¼‰
- Guardï¼ˆä¸‰å±‚å®‰å…¨å®ˆå«ï¼‰
- Platformï¼ˆå¹³å°æ£€æµ‹ï¼‰
- Verifierï¼ˆç»“æœéªŒè¯ï¼‰
- Reporterï¼ˆæŠ¥å‘Šç”Ÿæˆï¼‰
- ShellExecutorï¼ˆå®‰å…¨ shell æ‰§è¡Œï¼‰
- LLMExecutorï¼ˆLLM å…œåº•ï¼‰
- ExecutorRouterï¼ˆè·¯ç”±åˆ†å‘ï¼‰
- TaskEngineï¼ˆå®Œæ•´æµç¨‹ï¼‰
- TaskEngineAgentï¼ˆAgent æ¡¥æ¥ï¼‰
- Desktop Toolsï¼ˆå·¥å…·æ³¨å†Œï¼‰
"""
import asyncio
import json
import os
import sys
from pathlib import Path
from unittest.mock import AsyncMock, MagicMock, patch

import aiohttp
import pytest

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨ sys.path ä¸­
src_path = Path(__file__).parent.parent
sys.path.insert(0, str(src_path))


# ============================================================
# æ•°æ®æ¨¡å‹æµ‹è¯•
# ============================================================

class TestModels:
    """æµ‹è¯• Task / Step / StepResult æ•°æ®æ¨¡å‹"""

    def test_executor_type_values(self):
        from task_engine.models import ExecutorType
        assert ExecutorType.SHELL == "shell"
        assert ExecutorType.LLM == "llm"
        assert ExecutorType.DESKTOP == "desktop"

    def test_task_status_values(self):
        from task_engine.models import TaskStatus
        assert TaskStatus.PENDING == "pending"
        assert TaskStatus.RUNNING == "running"
        assert TaskStatus.SUCCESS == "success"
        assert TaskStatus.FAILED == "failed"
        assert TaskStatus.ABORTED == "aborted"

    def test_step_creation(self):
        from task_engine.models import ExecutorType, Step
        step = Step(
            executor_type=ExecutorType.DESKTOP,
            description="æ¡Œé¢æ“æ§",
            params={"task": "æ‰“å¼€æµè§ˆå™¨"},
        )
        assert step.executor_type == ExecutorType.DESKTOP
        assert step.description == "æ¡Œé¢æ“æ§"
        assert step.params["task"] == "æ‰“å¼€æµè§ˆå™¨"

    def test_step_default_params(self):
        from task_engine.models import ExecutorType, Step
        step = Step(executor_type=ExecutorType.LLM, description="test")
        assert step.params == {}

    def test_step_result_creation(self):
        from task_engine.models import StepResult
        result = StepResult(success=True, message="å®Œæˆ")
        assert result.success is True
        assert result.message == "å®Œæˆ"
        assert result.data == {}

    def test_task_creation(self):
        from task_engine.models import Task, TaskStatus
        task = Task(user_input="æ’­æ”¾éŸ³ä¹")
        assert task.user_input == "æ’­æ”¾éŸ³ä¹"
        assert task.steps == []
        assert task.status == TaskStatus.PENDING
        assert task.result is None


# ============================================================
# Planner æµ‹è¯•
# ============================================================

class TestPlanner:
    """æµ‹è¯•ä»»åŠ¡è§„åˆ’å™¨"""

    @pytest.mark.asyncio
    async def test_desktop_task_detection(self):
        from task_engine.models import ExecutorType
        from task_engine.planner import plan
        task = await plan("æ‰“å¼€ç½‘é¡µé‡Œçš„éŸ³ä¹è¾“å…¥å‘¨æ°ä¼¦æ’­æ”¾éŸ³ä¹")
        assert len(task.steps) == 1
        assert task.steps[0].executor_type == ExecutorType.DESKTOP

    @pytest.mark.asyncio
    async def test_desktop_keywords_multiple_hits(self):
        from task_engine.models import ExecutorType
        from task_engine.planner import plan
        task = await plan("æ‰“å¼€æµè§ˆå™¨æ’­æ”¾è§†é¢‘")
        assert task.steps[0].executor_type == ExecutorType.DESKTOP

    @pytest.mark.asyncio
    async def test_non_desktop_task_fallback(self):
        from task_engine.models import ExecutorType
        from task_engine.planner import plan
        task = await plan("ä½ å¥½ï¼Œä»Šå¤©å¿ƒæƒ…ä¸é”™")
        assert len(task.steps) == 1
        assert task.steps[0].executor_type == ExecutorType.LLM

    @pytest.mark.asyncio
    async def test_single_keyword_not_desktop(self):
        from task_engine.models import ExecutorType
        from task_engine.planner import plan
        # åªå‘½ä¸­ 1 ä¸ªå…³é”®è¯ï¼Œä¸åº”è¢«è¯†åˆ«ä¸ºæ¡Œé¢ä»»åŠ¡
        task = await plan("éŸ³ä¹å¥½å¬")
        assert task.steps[0].executor_type == ExecutorType.LLM

    @pytest.mark.asyncio
    async def test_task_params_contain_user_input(self):
        from task_engine.planner import plan
        user_input = "æ‰“å¼€ç½‘é¡µæœç´¢å‘¨æ°ä¼¦"
        task = await plan(user_input)
        assert task.steps[0].params["task"] == user_input


# ============================================================
# Guard æµ‹è¯•
# ============================================================

class TestTaskGuard:
    """æµ‹è¯•ä¸‰å±‚å®‰å…¨å®ˆå«"""

    def test_allow_normal_operation(self):
        from task_engine.executors.desktop_executor.guard import GuardAction, TaskGuard
        guard = TaskGuard()
        action = guard.check("click", {"x": 100, "y": 200}, "å·²ç‚¹å‡»åæ ‡ (100, 200)")
        assert action == GuardAction.ALLOW

    def test_abort_on_login(self):
        from task_engine.executors.desktop_executor.guard import GuardAction, TaskGuard
        guard = TaskGuard()
        action = guard.check("vision_analyze", {}, "é¡µé¢æ˜¾ç¤ºç™»å½•æ¡†")
        assert action == GuardAction.ABORT

    def test_abort_on_payment(self):
        from task_engine.executors.desktop_executor.guard import GuardAction, TaskGuard
        guard = TaskGuard()
        action = guard.check("click", {}, "æ”¯ä»˜ç¡®è®¤æŒ‰é’®")
        assert action == GuardAction.ABORT

    def test_abort_on_password(self):
        from task_engine.executors.desktop_executor.guard import GuardAction, TaskGuard
        guard = TaskGuard()
        action = guard.check("type_text", {"text": "abc"}, "å¯†ç è¾“å…¥æ¡†")
        assert action == GuardAction.ABORT

    def test_abort_on_sudo(self):
        from task_engine.executors.desktop_executor.guard import GuardAction, TaskGuard
        guard = TaskGuard()
        action = guard.check("shell_run", {"command": "sudo apt install"}, "")
        assert action == GuardAction.ABORT

    def test_drift_accumulation(self):
        from task_engine.executors.desktop_executor.guard import GuardAction, TaskGuard
        guard = TaskGuard()
        # åç¦»ä¿¡å·ä¸è¶³é˜ˆå€¼æ—¶ä»ç„¶å…è®¸
        guard.check("vision_analyze", {}, "éœ€è¦ç™»å½•æ‰èƒ½ç»§ç»­")
        assert guard.drift_count == 0  # ç¬¬ä¸€å±‚å…ˆæ‹¦æˆª

    def test_drift_threshold_abort(self):
        from task_engine.executors.desktop_executor.guard import GuardAction, TaskGuard
        guard = TaskGuard()
        # æ¨¡æ‹Ÿå¤šæ¬¡åç¦»ï¼ˆä½¿ç”¨ä¸è§¦å‘ç¬¬ä¸€å±‚çš„åç¦»ä¿¡å·ï¼‰
        for _ in range(3):
            guard.check("screenshot", {}, "é¡µé¢æç¤ºï¼šä¼šå‘˜ä¸“äº«")
        action = guard.check("screenshot", {}, "æ­£å¸¸æ“ä½œ")
        assert action == GuardAction.ABORT

    def test_switch_on_repeated_failure(self):
        from task_engine.executors.desktop_executor.guard import GuardAction, TaskGuard
        guard = TaskGuard()
        # åŒä¸€ URL å¤šæ¬¡å¤±è´¥
        for _ in range(3):
            action = guard.check("app_open", {"url": "https://music.example.com"}, "æ‰“å¼€å¤±è´¥")
        assert action == GuardAction.SWITCH

    def test_reset(self):
        from task_engine.executors.desktop_executor.guard import TaskGuard
        guard = TaskGuard()
        guard.drift_count = 5
        guard.fail_counts["test"] = 3
        guard.reset()
        assert guard.drift_count == 0
        assert guard.fail_counts == {}


# ============================================================
# Platform æµ‹è¯•
# ============================================================

class TestPlatform:
    """æµ‹è¯•å¹³å°æ£€æµ‹"""

    def test_detect_platform_returns_valid_type(self):
        from task_engine.executors.desktop_executor.platform import PlatformType, detect_platform
        result = detect_platform()
        assert result in (PlatformType.MACOS, PlatformType.LINUX, PlatformType.UNKNOWN)

    def test_get_open_command(self):
        from task_engine.executors.desktop_executor.platform import get_open_command
        cmd = get_open_command()
        assert cmd in ("open", "xdg-open")

    def test_get_screenshot_command(self):
        from task_engine.executors.desktop_executor.platform import get_screenshot_command
        cmd = get_screenshot_command()
        assert "{path}" in cmd

    @patch("platform.system", return_value="Darwin")
    def test_macos_detection(self, mock_sys):
        from task_engine.executors.desktop_executor.platform import PlatformType, detect_platform
        assert detect_platform() == PlatformType.MACOS

    @patch("platform.system", return_value="Linux")
    def test_linux_detection(self, mock_sys):
        from task_engine.executors.desktop_executor.platform import PlatformType, detect_platform
        assert detect_platform() == PlatformType.LINUX

    @patch("platform.system", return_value="Windows")
    def test_unknown_detection(self, mock_sys):
        from task_engine.executors.desktop_executor.platform import PlatformType, detect_platform
        assert detect_platform() == PlatformType.UNKNOWN


# ============================================================
# Verifier æµ‹è¯•
# ============================================================

class TestVerifier:
    """æµ‹è¯•ç»“æœéªŒè¯å™¨"""

    @pytest.mark.asyncio
    async def test_verify_success(self):
        from task_engine.models import StepResult, Task, TaskStatus
        from task_engine.verifier import verify
        task = Task(user_input="test")
        task.result = StepResult(success=True, message="å®Œæˆ")
        task = await verify(task)
        assert task.status == TaskStatus.SUCCESS

    @pytest.mark.asyncio
    async def test_verify_failure(self):
        from task_engine.models import StepResult, Task, TaskStatus
        from task_engine.verifier import verify
        task = Task(user_input="test")
        task.result = StepResult(success=False, message="å‡ºé”™äº†")
        task = await verify(task)
        assert task.status == TaskStatus.FAILED

    @pytest.mark.asyncio
    async def test_verify_aborted(self):
        from task_engine.models import StepResult, Task, TaskStatus
        from task_engine.verifier import verify
        task = Task(user_input="test")
        task.result = StepResult(success=False, message="å®‰å…¨å®ˆå«ç»ˆæ­¢ï¼šæ£€æµ‹åˆ°å±é™©")
        task = await verify(task)
        assert task.status == TaskStatus.ABORTED

    @pytest.mark.asyncio
    async def test_verify_no_result(self):
        from task_engine.models import Task, TaskStatus
        from task_engine.verifier import verify
        task = Task(user_input="test")
        task = await verify(task)
        assert task.status == TaskStatus.FAILED


# ============================================================
# Reporter æµ‹è¯•
# ============================================================

class TestReporter:
    """æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""

    @pytest.mark.asyncio
    async def test_report_success(self):
        from task_engine.models import StepResult, Task, TaskStatus
        from task_engine.reporter import report
        task = Task(user_input="test")
        task.status = TaskStatus.SUCCESS
        task.result = StepResult(success=True, message="æ’­æ”¾äº†å‘¨æ°ä¼¦çš„æ­Œ")
        text = await report(task)
        assert "âœ…" in text
        assert "å‘¨æ°ä¼¦" in text

    @pytest.mark.asyncio
    async def test_report_failure(self):
        from task_engine.models import StepResult, Task, TaskStatus
        from task_engine.reporter import report
        task = Task(user_input="test")
        task.status = TaskStatus.FAILED
        task.result = StepResult(success=False, message="å¤±è´¥äº†")
        text = await report(task)
        assert "âŒ" in text

    @pytest.mark.asyncio
    async def test_report_aborted(self):
        from task_engine.models import StepResult, Task, TaskStatus
        from task_engine.reporter import report
        task = Task(user_input="test")
        task.status = TaskStatus.ABORTED
        task.result = StepResult(success=False, message="è¢«ç»ˆæ­¢")
        text = await report(task)
        assert "âš ï¸" in text

    @pytest.mark.asyncio
    async def test_report_pending(self):
        from task_engine.models import Task, TaskStatus
        from task_engine.reporter import report
        task = Task(user_input="test")
        task.status = TaskStatus.PENDING
        text = await report(task)
        assert "â³" in text


# ============================================================
# ShellExecutor æµ‹è¯•
# ============================================================

class TestShellExecutor:
    """æµ‹è¯•å®‰å…¨ Shell æ‰§è¡Œå™¨"""

    @pytest.mark.asyncio
    async def test_execute_safe_command(self):
        from task_engine.executors.shell_executor import ShellExecutor
        from task_engine.models import ExecutorType, Step
        executor = ShellExecutor()
        step = Step(
            executor_type=ExecutorType.SHELL,
            description="echo",
            params={"command": "echo hello"},
        )
        result = await executor.execute(step)
        assert result.success is True
        assert "hello" in result.message

    @pytest.mark.asyncio
    async def test_reject_rm_rf(self):
        from task_engine.executors.shell_executor import ShellExecutor
        from task_engine.models import ExecutorType, Step
        executor = ShellExecutor()
        step = Step(
            executor_type=ExecutorType.SHELL,
            description="rm",
            params={"command": "rm -rf /"},
        )
        result = await executor.execute(step)
        assert result.success is False
        assert "å®‰å…¨æ‹’ç»" in result.message

    @pytest.mark.asyncio
    async def test_reject_sudo(self):
        from task_engine.executors.shell_executor import ShellExecutor
        from task_engine.models import ExecutorType, Step
        executor = ShellExecutor()
        step = Step(
            executor_type=ExecutorType.SHELL,
            description="sudo",
            params={"command": "sudo apt-get install foo"},
        )
        result = await executor.execute(step)
        assert result.success is False
        assert "å®‰å…¨æ‹’ç»" in result.message

    @pytest.mark.asyncio
    async def test_missing_command(self):
        from task_engine.executors.shell_executor import ShellExecutor
        from task_engine.models import ExecutorType, Step
        executor = ShellExecutor()
        step = Step(
            executor_type=ExecutorType.SHELL,
            description="empty",
            params={},
        )
        result = await executor.execute(step)
        assert result.success is False
        assert "ç¼ºå°‘" in result.message


# ============================================================
# LLMExecutor æµ‹è¯•
# ============================================================

class TestLLMExecutor:
    """æµ‹è¯• LLM æ‰§è¡Œå™¨"""

    @pytest.mark.asyncio
    async def test_execute_with_task(self):
        from task_engine.executors.llm_executor import LLMExecutor
        from task_engine.models import ExecutorType, Step
        executor = LLMExecutor()
        step = Step(
            executor_type=ExecutorType.LLM,
            description="llm",
            params={"task": "ä½ å¥½"},
        )
        result = await executor.execute(step)
        assert result.success is True
        assert "ä½ å¥½" in result.message

    @pytest.mark.asyncio
    async def test_execute_missing_task(self):
        from task_engine.executors.llm_executor import LLMExecutor
        from task_engine.models import ExecutorType, Step
        executor = LLMExecutor()
        step = Step(
            executor_type=ExecutorType.LLM,
            description="empty",
            params={},
        )
        result = await executor.execute(step)
        assert result.success is False


# ============================================================
# ExecutorRouter æµ‹è¯•
# ============================================================

class TestExecutorRouter:
    """æµ‹è¯•æ‰§è¡Œå™¨è·¯ç”±"""

    @pytest.mark.asyncio
    async def test_route_shell(self):
        from task_engine.executor_router import route_and_execute
        from task_engine.models import ExecutorType, Step
        step = Step(
            executor_type=ExecutorType.SHELL,
            description="echo",
            params={"command": "echo router_test"},
        )
        result = await route_and_execute(step)
        assert result.success is True
        assert "router_test" in result.message

    @pytest.mark.asyncio
    async def test_route_llm(self):
        from task_engine.executor_router import route_and_execute
        from task_engine.models import ExecutorType, Step
        step = Step(
            executor_type=ExecutorType.LLM,
            description="llm",
            params={"task": "test"},
        )
        result = await route_and_execute(step)
        assert result.success is True


# ============================================================
# Desktop Tools æµ‹è¯•
# ============================================================

class TestDesktopTools:
    """æµ‹è¯•æ¡Œé¢å·¥å…·æ³¨å†Œè¡¨"""

    def test_tool_registry_completeness(self):
        from task_engine.executors.desktop_executor.tools import TOOL_REGISTRY
        expected_tools = [
            "shell_run", "app_open", "screenshot",
            "vision_analyze", "page_analyze", "click", "type_text", "key_press",
        ]
        for tool_name in expected_tools:
            assert tool_name in TOOL_REGISTRY, f"å·¥å…· {tool_name} æœªæ³¨å†Œ"

    def test_tool_definitions_completeness(self):
        from task_engine.executors.desktop_executor.tools import TOOL_DEFINITIONS
        names = [td["function"]["name"] for td in TOOL_DEFINITIONS]
        expected = ["app_open", "screenshot", "vision_analyze", "click", "type_text", "key_press", "shell_run", "page_analyze"]
        for name in expected:
            assert name in names, f"å·¥å…·å®šä¹‰ {name} ç¼ºå¤±"

    def test_tool_definitions_format(self):
        from task_engine.executors.desktop_executor.tools import TOOL_DEFINITIONS
        for td in TOOL_DEFINITIONS:
            assert td["type"] == "function"
            assert "name" in td["function"]
            assert "description" in td["function"]
            assert "parameters" in td["function"]

    @pytest.mark.asyncio
    async def test_shell_run_safe(self):
        from task_engine.executors.desktop_executor.tools.shell_run import shell_run
        result = await shell_run("echo tool_test")
        assert "tool_test" in result

    @pytest.mark.asyncio
    async def test_shell_run_reject_sudo(self):
        from task_engine.executors.desktop_executor.tools.shell_run import shell_run
        result = await shell_run("sudo rm -rf /")
        assert "å®‰å…¨æ‹’ç»" in result

    @pytest.mark.asyncio
    async def test_vision_analyze_missing_file(self):
        from task_engine.executors.desktop_executor.tools.vision_analyze import vision_analyze
        result = await vision_analyze("/nonexistent/file.png", "æœç´¢æ¡†")
        data = json.loads(result)
        assert "error" in data

    @pytest.mark.asyncio
    async def test_vision_analyze_encode_image(self):
        """æµ‹è¯•å›¾ç‰‡ base64 ç¼–ç """
        import tempfile
        from task_engine.executors.desktop_executor.tools.vision_analyze import _encode_image
        # åˆ›å»ºä¸´æ—¶æµ‹è¯•å›¾ç‰‡
        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as f:
            f.write(b"\x89PNG\r\n\x1a\n" + b"\x00" * 100)
            tmp_path = f.name
        try:
            encoded = _encode_image(tmp_path)
            assert isinstance(encoded, str)
            assert len(encoded) > 0
            # éªŒè¯ base64 å¯è§£ç 
            import base64
            decoded = base64.b64decode(encoded)
            assert decoded[:4] == b"\x89PNG"
        finally:
            os.remove(tmp_path)

    def test_vision_analyze_get_mime_type(self):
        """æµ‹è¯• MIME ç±»å‹è¯†åˆ«"""
        from task_engine.executors.desktop_executor.tools.vision_analyze import _get_mime_type
        assert _get_mime_type("test.png") == "image/png"
        assert _get_mime_type("test.jpg") == "image/jpeg"
        assert _get_mime_type("test.jpeg") == "image/jpeg"
        assert _get_mime_type("test.gif") == "image/gif"
        assert _get_mime_type("test.webp") == "image/webp"
        assert _get_mime_type("test.bmp") == "image/bmp"
        assert _get_mime_type("test.unknown") == "image/png"  # é»˜è®¤å€¼

    def test_parse_vlm_response_valid_json(self):
        """æµ‹è¯• VLM å“åº”è§£æ - æœ‰æ•ˆ JSON"""
        from task_engine.executors.desktop_executor.tools.vision_analyze import _parse_vlm_response
        content = json.dumps({
            "found": True,
            "elements": [
                {"description": "æœç´¢æ¡†", "x": 500, "y": 100, "width": 200, "height": 30, "confidence": 0.95}
            ],
        })
        result = _parse_vlm_response(content, "æœç´¢æ¡†")
        assert result["found"] is True
        assert result["query"] == "æœç´¢æ¡†"
        assert len(result["elements"]) == 1
        assert result["elements"][0]["x"] == 500
        assert result["elements"][0]["y"] == 100
        assert result["elements"][0]["confidence"] == 0.95

    def test_parse_vlm_response_json_in_code_block(self):
        """æµ‹è¯• VLM å“åº”è§£æ - JSON è¢«ä»£ç å—åŒ…è£¹"""
        from task_engine.executors.desktop_executor.tools.vision_analyze import _parse_vlm_response
        content = '```json\n{"found": true, "elements": [{"description": "æŒ‰é’®", "x": 200, "y": 300, "width": 80, "height": 40, "confidence": 0.9}]}\n```'
        result = _parse_vlm_response(content, "æŒ‰é’®")
        assert result["found"] is True
        assert len(result["elements"]) == 1
        assert result["elements"][0]["x"] == 200

    def test_parse_vlm_response_not_found(self):
        """æµ‹è¯• VLM å“åº”è§£æ - æœªæ‰¾åˆ°å…ƒç´ """
        from task_engine.executors.desktop_executor.tools.vision_analyze import _parse_vlm_response
        content = json.dumps({"found": False, "elements": []})
        result = _parse_vlm_response(content, "æœç´¢æ¡†")
        assert result["found"] is False
        assert result["elements"] == []

    def test_parse_vlm_response_invalid_json(self):
        """æµ‹è¯• VLM å“åº”è§£æ - æ— æ•ˆ JSON å›é€€"""
        from task_engine.executors.desktop_executor.tools.vision_analyze import _parse_vlm_response
        content = "è¿™ä¸æ˜¯ JSON æ ¼å¼çš„å›å¤"
        result = _parse_vlm_response(content, "æœç´¢æ¡†")
        assert result["found"] is False
        assert "message" in result

    def test_parse_vlm_response_multiple_elements(self):
        """æµ‹è¯• VLM å“åº”è§£æ - å¤šä¸ªå…ƒç´ """
        from task_engine.executors.desktop_executor.tools.vision_analyze import _parse_vlm_response
        content = json.dumps({
            "found": True,
            "elements": [
                {"description": "æœç´¢æ¡†1", "x": 100, "y": 200, "width": 300, "height": 30, "confidence": 0.9},
                {"description": "æœç´¢æ¡†2", "x": 400, "y": 500, "width": 300, "height": 30, "confidence": 0.7},
            ],
        })
        result = _parse_vlm_response(content, "æœç´¢æ¡†")
        assert result["found"] is True
        assert len(result["elements"]) == 2

    def test_parse_vlm_response_invalid_element(self):
        """æµ‹è¯• VLM å“åº”è§£æ - æ— æ•ˆå…ƒç´ è¢«è¿‡æ»¤"""
        from task_engine.executors.desktop_executor.tools.vision_analyze import _parse_vlm_response
        content = json.dumps({
            "found": True,
            "elements": [
                {"description": "æœ‰æ•ˆ", "x": 100, "y": 200},
                {"description": "æ— æ•ˆ-ç¼ºå°‘åæ ‡"},
            ],
        })
        result = _parse_vlm_response(content, "æµ‹è¯•")
        assert len(result["elements"]) == 1

    def test_parse_vlm_response_unclosed_code_block(self):
        """æµ‹è¯• VLM å“åº”è§£æ - æœªé—­åˆä»£ç å—ä¸å´©æºƒ"""
        from task_engine.executors.desktop_executor.tools.vision_analyze import _parse_vlm_response
        content = '```json\n{"found": false, "elements": []}'
        result = _parse_vlm_response(content, "æµ‹è¯•")
        assert result["found"] is False

    @pytest.mark.asyncio
    async def test_vision_analyze_vlm_api_success(self):
        """æµ‹è¯• VLM API è°ƒç”¨æˆåŠŸåœºæ™¯ï¼ˆmockï¼‰"""
        import tempfile
        from task_engine.executors.desktop_executor.tools.vision_analyze import vision_analyze

        # åˆ›å»ºä¸´æ—¶æµ‹è¯•å›¾ç‰‡
        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as f:
            f.write(b"\x89PNG\r\n\x1a\n" + b"\x00" * 100)
            tmp_path = f.name

        vlm_response = {
            "choices": [{
                "message": {
                    "content": json.dumps({
                        "found": True,
                        "elements": [
                            {"description": "æœç´¢æ¡†", "x": 500, "y": 100, "width": 200, "height": 30, "confidence": 0.95}
                        ],
                    })
                }
            }]
        }

        mock_resp = AsyncMock()
        mock_resp.status = 200
        mock_resp.json = AsyncMock(return_value=vlm_response)

        mock_session = AsyncMock()
        mock_session.post = MagicMock(return_value=AsyncMock(
            __aenter__=AsyncMock(return_value=mock_resp),
            __aexit__=AsyncMock(return_value=False),
        ))

        with patch("aiohttp.ClientSession", return_value=AsyncMock(
            __aenter__=AsyncMock(return_value=mock_session),
            __aexit__=AsyncMock(return_value=False),
        )):
            try:
                result_str = await vision_analyze(tmp_path, "æœç´¢æ¡†")
                result = json.loads(result_str)
                assert result["found"] is True
                assert len(result["elements"]) == 1
                assert result["elements"][0]["x"] == 500
            finally:
                os.remove(tmp_path)

    @pytest.mark.asyncio
    async def test_vision_analyze_vlm_api_connection_error(self):
        """æµ‹è¯• VLM API è¿æ¥å¤±è´¥åœºæ™¯"""
        import tempfile
        from task_engine.executors.desktop_executor.tools.vision_analyze import vision_analyze

        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as f:
            f.write(b"\x89PNG\r\n\x1a\n" + b"\x00" * 100)
            tmp_path = f.name

        with patch("aiohttp.ClientSession") as mock_cls:
            mock_session = AsyncMock()
            mock_session.post = MagicMock(side_effect=aiohttp.ClientError("è¿æ¥å¤±è´¥"))
            mock_cls.return_value.__aenter__ = AsyncMock(return_value=mock_session)
            mock_cls.return_value.__aexit__ = AsyncMock(return_value=False)

            try:
                result_str = await vision_analyze(tmp_path, "æœç´¢æ¡†")
                result = json.loads(result_str)
                assert result["found"] is False
                assert "error" in result
            finally:
                os.remove(tmp_path)

    def test_draw_bounding_boxes_single_element(self):
        """æµ‹è¯•ç»˜åˆ¶å•ä¸ªå…ƒç´ è¾¹æ¡†æ ‡æ³¨"""
        import tempfile
        from PIL import Image
        from task_engine.executors.desktop_executor.tools.vision_analyze import draw_bounding_boxes

        # åˆ›å»ºä¸´æ—¶æµ‹è¯•å›¾ç‰‡ (200x200 ç™½è‰²å›¾ç‰‡)
        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as f:
            tmp_path = f.name
        img = Image.new("RGB", (200, 200), "white")
        img.save(tmp_path)

        elements = [
            {"description": "æœç´¢æ¡†", "x": 100, "y": 50, "width": 120, "height": 30, "confidence": 0.95}
        ]
        try:
            annotated_path = draw_bounding_boxes(tmp_path, elements)
            assert annotated_path is not None
            assert os.path.exists(annotated_path)
            assert "_annotated" in annotated_path
            # éªŒè¯æ ‡æ³¨å›¾ç‰‡å¯ä»¥æ­£å¸¸æ‰“å¼€
            annotated_img = Image.open(annotated_path)
            assert annotated_img.size == (200, 200)
        finally:
            os.remove(tmp_path)
            if annotated_path and os.path.exists(annotated_path):
                os.remove(annotated_path)

    def test_draw_bounding_boxes_multiple_elements(self):
        """æµ‹è¯•ç»˜åˆ¶å¤šä¸ªå…ƒç´ è¾¹æ¡†æ ‡æ³¨"""
        import tempfile
        from PIL import Image
        from task_engine.executors.desktop_executor.tools.vision_analyze import draw_bounding_boxes

        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as f:
            tmp_path = f.name
        img = Image.new("RGB", (400, 300), "white")
        img.save(tmp_path)

        elements = [
            {"description": "æœç´¢æ¡†", "x": 200, "y": 50, "width": 200, "height": 30, "confidence": 0.95},
            {"description": "æ’­æ”¾æŒ‰é’®", "x": 100, "y": 200, "width": 60, "height": 40, "confidence": 0.8},
        ]
        try:
            annotated_path = draw_bounding_boxes(tmp_path, elements)
            assert annotated_path is not None
            assert os.path.exists(annotated_path)
        finally:
            os.remove(tmp_path)
            if annotated_path and os.path.exists(annotated_path):
                os.remove(annotated_path)

    def test_draw_bounding_boxes_no_width_height(self):
        """æµ‹è¯•å…ƒç´ æ— å®½é«˜æ—¶ä½¿ç”¨é»˜è®¤å¤§å°"""
        import tempfile
        from PIL import Image
        from task_engine.executors.desktop_executor.tools.vision_analyze import draw_bounding_boxes

        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as f:
            tmp_path = f.name
        img = Image.new("RGB", (200, 200), "white")
        img.save(tmp_path)

        elements = [
            {"description": "æŒ‰é’®", "x": 100, "y": 100, "width": 0, "height": 0, "confidence": 0.7}
        ]
        try:
            annotated_path = draw_bounding_boxes(tmp_path, elements)
            assert annotated_path is not None
        finally:
            os.remove(tmp_path)
            if annotated_path and os.path.exists(annotated_path):
                os.remove(annotated_path)

    def test_draw_bounding_boxes_empty_elements(self):
        """æµ‹è¯•ç©ºå…ƒç´ åˆ—è¡¨è¿”å› None"""
        from task_engine.executors.desktop_executor.tools.vision_analyze import draw_bounding_boxes
        result = draw_bounding_boxes("/some/path.png", [])
        assert result is None

    def test_draw_bounding_boxes_missing_file(self):
        """æµ‹è¯•ä¸å­˜åœ¨çš„æ–‡ä»¶è¿”å› None"""
        from task_engine.executors.desktop_executor.tools.vision_analyze import draw_bounding_boxes
        result = draw_bounding_boxes("/nonexistent/file.png", [{"x": 100, "y": 100}])
        assert result is None

    @pytest.mark.asyncio
    async def test_page_analyze_no_cdp(self):
        """æµ‹è¯• page_analyze åœ¨æ—  CDP è¿æ¥æ—¶è¿”å›åˆç†ç»“æœ"""
        from task_engine.executors.desktop_executor.tools.page_analyze import page_analyze
        result_str = await page_analyze("search")
        result = json.loads(result_str)
        assert result["found"] is False
        assert result["query"] == "search"

    @pytest.mark.asyncio
    async def test_page_analyze_invalid_type_defaults_to_search(self):
        """æµ‹è¯• page_analyze æ— æ•ˆç±»å‹é»˜è®¤å›é€€ä¸º search"""
        from task_engine.executors.desktop_executor.tools.page_analyze import page_analyze
        result_str = await page_analyze("invalid_type")
        result = json.loads(result_str)
        assert result["query"] == "search"

    @pytest.mark.asyncio
    async def test_page_analyze_with_mock_cdp(self):
        """æµ‹è¯• page_analyze é€šè¿‡ mock CDP è¿”å›å…ƒç´ """
        from task_engine.executors.desktop_executor.tools.page_analyze import page_analyze

        mock_js_result = json.dumps([
            {
                "type": "search",
                "tag": "input",
                "id": "srch",
                "name": "query",
                "className": "search-input",
                "placeholder": "æœç´¢éŸ³ä¹",
                "ariaLabel": "æœç´¢",
                "x": 600,
                "y": 35,
                "width": 200,
                "height": 30,
            }
        ])

        with patch(
            "task_engine.executors.desktop_executor.tools.page_analyze._run_browser_js",
            new_callable=AsyncMock,
            return_value=mock_js_result,
        ):
            result_str = await page_analyze("search")
            result = json.loads(result_str)
            assert result["found"] is True
            assert len(result["elements"]) == 1
            assert result["elements"][0]["x"] == 600
            assert result["elements"][0]["y"] == 35
            assert "æœç´¢éŸ³ä¹" in result["elements"][0]["description"]

    @pytest.mark.asyncio
    async def test_page_analyze_search_fallback_to_input(self):
        """æµ‹è¯• page_analyze search æœªæ‰¾åˆ°æ—¶å›é€€åˆ° input ç±»å‹"""
        from task_engine.executors.desktop_executor.tools.page_analyze import page_analyze

        mock_js_result = json.dumps([
            {
                "type": "input",
                "tag": "input",
                "id": "text-field",
                "name": "",
                "className": "text-input",
                "placeholder": "è¾“å…¥å†…å®¹",
                "ariaLabel": "",
                "x": 400,
                "y": 50,
                "width": 150,
                "height": 25,
            }
        ])

        with patch(
            "task_engine.executors.desktop_executor.tools.page_analyze._run_browser_js",
            new_callable=AsyncMock,
            return_value=mock_js_result,
        ):
            result_str = await page_analyze("search")
            result = json.loads(result_str)
            assert result["found"] is True
            assert len(result["elements"]) == 1
            assert result["elements"][0]["x"] == 400

    @pytest.mark.asyncio
    async def test_page_analyze_invalid_js_result(self):
        """æµ‹è¯• page_analyze JS è¿”å›æ— æ•ˆç»“æœ"""
        from task_engine.executors.desktop_executor.tools.page_analyze import page_analyze

        with patch(
            "task_engine.executors.desktop_executor.tools.page_analyze._run_browser_js",
            new_callable=AsyncMock,
            return_value="not-valid-json",
        ):
            result_str = await page_analyze("search")
            result = json.loads(result_str)
            assert result["found"] is False
            assert "error" in result

    def test_vision_analyze_system_prompt_has_search_hints(self):
        """æµ‹è¯• VLM system prompt åŒ…å«æœç´¢æ¡†è§†è§‰ç‰¹å¾æè¿°"""
        from task_engine.executors.desktop_executor.tools.vision_analyze import _VISION_SYSTEM_PROMPT
        assert "æœç´¢æ¡†" in _VISION_SYSTEM_PROMPT
        assert "æ”¾å¤§é•œ" in _VISION_SYSTEM_PROMPT
        assert "å¯¼èˆªæ " in _VISION_SYSTEM_PROMPT
        assert "music.163.com" in _VISION_SYSTEM_PROMPT

    def test_scale_elements_retina_2x(self):
        """æµ‹è¯• Retina 2x ç¼©æ”¾å› å­ä¸‹çš„åæ ‡è½¬æ¢"""
        from task_engine.executors.desktop_executor.tools.vision_analyze import _scale_elements
        elements = [
            {"description": "æœç´¢æ¡†", "x": 810, "y": 334, "width": 400, "height": 60, "confidence": 0.95}
        ]
        scaled = _scale_elements(elements, 2.0)
        assert len(scaled) == 1
        assert scaled[0]["x"] == 405
        assert scaled[0]["y"] == 167
        assert scaled[0]["width"] == 200
        assert scaled[0]["height"] == 30
        assert scaled[0]["confidence"] == 0.95
        assert scaled[0]["description"] == "æœç´¢æ¡†"

    def test_scale_elements_no_scaling(self):
        """æµ‹è¯•ç¼©æ”¾å› å­ä¸º 1.0 æ—¶ä¸ä¿®æ”¹åæ ‡"""
        from task_engine.executors.desktop_executor.tools.vision_analyze import _scale_elements
        elements = [
            {"description": "æŒ‰é’®", "x": 400, "y": 200, "width": 100, "height": 40, "confidence": 0.9}
        ]
        scaled = _scale_elements(elements, 1.0)
        assert scaled[0]["x"] == 400
        assert scaled[0]["y"] == 200

    def test_scale_elements_near_one(self):
        """æµ‹è¯•æ¥è¿‘ 1.0 çš„ç¼©æ”¾å› å­ï¼ˆå·®å¼‚ < 0.01ï¼‰ä¸ä¿®æ”¹åæ ‡"""
        from task_engine.executors.desktop_executor.tools.vision_analyze import _scale_elements
        elements = [
            {"description": "æŒ‰é’®", "x": 400, "y": 200, "width": 100, "height": 40, "confidence": 0.9}
        ]
        scaled = _scale_elements(elements, 1.005)
        # Should return unchanged values since diff < 0.01
        assert scaled[0]["x"] == 400
        assert scaled[0]["y"] == 200
        assert scaled[0]["width"] == 100
        assert scaled[0]["height"] == 40

    def test_scale_elements_multiple(self):
        """æµ‹è¯•å¤šä¸ªå…ƒç´ çš„åæ ‡ç¼©æ”¾"""
        from task_engine.executors.desktop_executor.tools.vision_analyze import _scale_elements
        elements = [
            {"description": "æœç´¢æ¡†", "x": 800, "y": 300, "width": 400, "height": 60, "confidence": 0.9},
            {"description": "æ’­æ”¾æŒ‰é’®", "x": 600, "y": 500, "width": 80, "height": 80, "confidence": 0.8},
        ]
        scaled = _scale_elements(elements, 2.0)
        assert len(scaled) == 2
        assert scaled[0]["x"] == 400
        assert scaled[0]["y"] == 150
        assert scaled[1]["x"] == 300
        assert scaled[1]["y"] == 250

    def test_scale_elements_preserves_original(self):
        """æµ‹è¯•åæ ‡ç¼©æ”¾ä¸ä¿®æ”¹åŸå§‹å…ƒç´ """
        from task_engine.executors.desktop_executor.tools.vision_analyze import _scale_elements
        elements = [
            {"description": "æœç´¢æ¡†", "x": 800, "y": 300, "width": 400, "height": 60, "confidence": 0.9},
        ]
        scaled = _scale_elements(elements, 2.0)
        # åŸå§‹å…ƒç´ ä¸åº”è¢«ä¿®æ”¹
        assert elements[0]["x"] == 800
        assert elements[0]["y"] == 300
        # ç¼©æ”¾åçš„å…ƒç´ åº”è¯¥æ˜¯æ–°åˆ—è¡¨
        assert scaled[0]["x"] == 400
        assert scaled[0]["y"] == 150

    def test_get_image_size(self):
        """æµ‹è¯•è·å–å›¾ç‰‡å°ºå¯¸"""
        import tempfile
        from PIL import Image
        from task_engine.executors.desktop_executor.tools.vision_analyze import _get_image_size
        # åˆ›å»ºæµ‹è¯•å›¾ç‰‡
        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as f:
            img = Image.new("RGB", (2880, 1800), color="white")
            img.save(f, format="PNG")
            tmp_path = f.name
        try:
            w, h = _get_image_size(tmp_path)
            assert w == 2880
            assert h == 1800
        finally:
            os.remove(tmp_path)

    def test_get_image_size_nonexistent(self):
        """æµ‹è¯•è·å–ä¸å­˜åœ¨æ–‡ä»¶çš„å°ºå¯¸"""
        from task_engine.executors.desktop_executor.tools.vision_analyze import _get_image_size
        w, h = _get_image_size("/nonexistent/file.png")
        assert w is None
        assert h is None

    @pytest.mark.asyncio
    async def test_get_scale_factor_retina(self):
        """æµ‹è¯• Retina å±å¹•ä¸‹çš„ç¼©æ”¾å› å­è®¡ç®—"""
        import tempfile
        from PIL import Image
        from task_engine.executors.desktop_executor.tools.vision_analyze import _get_scale_factor
        # åˆ›å»º 2880x1800 çš„æµ‹è¯•å›¾ç‰‡ï¼ˆæ¨¡æ‹Ÿ Retina æˆªå›¾ï¼‰
        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as f:
            img = Image.new("RGB", (2880, 1800), color="white")
            img.save(f, format="PNG")
            tmp_path = f.name
        try:
            # Mock å±å¹•åˆ†è¾¨ç‡ä¸º 1440x900
            with patch(
                "task_engine.executors.desktop_executor.tools.vision_analyze.get_screen_resolution",
                return_value=(1440, 900),
            ):
                scale = await _get_scale_factor(tmp_path)
                assert scale == 2.0
        finally:
            os.remove(tmp_path)

    @pytest.mark.asyncio
    async def test_get_scale_factor_no_scaling(self):
        """æµ‹è¯•é HiDPI å±å¹•ä¸‹ç¼©æ”¾å› å­ä¸º 1.0"""
        import tempfile
        from PIL import Image
        from task_engine.executors.desktop_executor.tools.vision_analyze import _get_scale_factor
        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as f:
            img = Image.new("RGB", (1920, 1080), color="white")
            img.save(f, format="PNG")
            tmp_path = f.name
        try:
            with patch(
                "task_engine.executors.desktop_executor.tools.vision_analyze.get_screen_resolution",
                return_value=(1920, 1080),
            ):
                scale = await _get_scale_factor(tmp_path)
                assert scale == 1.0
        finally:
            os.remove(tmp_path)

    @pytest.mark.asyncio
    async def test_get_scale_factor_no_screen_resolution(self):
        """æµ‹è¯•æ— æ³•è·å–å±å¹•åˆ†è¾¨ç‡æ—¶è¿”å› None"""
        import tempfile
        from PIL import Image
        from task_engine.executors.desktop_executor.tools.vision_analyze import _get_scale_factor
        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as f:
            img = Image.new("RGB", (1920, 1080), color="white")
            img.save(f, format="PNG")
            tmp_path = f.name
        try:
            with patch(
                "task_engine.executors.desktop_executor.tools.vision_analyze.get_screen_resolution",
                return_value=None,
            ):
                scale = await _get_scale_factor(tmp_path)
                assert scale is None
        finally:
            os.remove(tmp_path)


# ============================================================
# å·¥å…·æ—¥å¿—è¾…åŠ©å‡½æ•°æµ‹è¯•
# ============================================================

class TestToolLogHelpers:
    """æµ‹è¯•å·¥å…·æ—¥å¿—è¾…åŠ©å‡½æ•°"""

    def test_get_tool_icon(self):
        from task_engine.executors.desktop_executor.executor import _get_tool_icon
        assert _get_tool_icon("screenshot") == "ğŸ“¸"
        assert _get_tool_icon("vision_analyze") == "ğŸ‘ï¸"
        assert _get_tool_icon("page_analyze") == "ğŸ”"
        assert _get_tool_icon("click") == "ğŸ–±ï¸"
        assert _get_tool_icon("type_text") == "âŒ¨ï¸"
        assert _get_tool_icon("key_press") == "âŒ¨ï¸"
        assert _get_tool_icon("app_open") == "ğŸŒ"
        assert _get_tool_icon("shell_run") == "ğŸ’»"
        assert _get_tool_icon("unknown_tool") == "ğŸ”§"

    def test_summarize_args_screenshot(self):
        from task_engine.executors.desktop_executor.executor import _summarize_args
        assert _summarize_args("screenshot", {}) == ""

    def test_summarize_args_click(self):
        from task_engine.executors.desktop_executor.executor import _summarize_args
        result = _summarize_args("click", {"x": 100, "y": 200})
        assert "x=100" in result
        assert "y=200" in result

    def test_summarize_args_type_text(self):
        from task_engine.executors.desktop_executor.executor import _summarize_args
        result = _summarize_args("type_text", {"text": "å‘¨æ°ä¼¦"})
        assert "å‘¨æ°ä¼¦" in result

    def test_summarize_args_key_press(self):
        from task_engine.executors.desktop_executor.executor import _summarize_args
        result = _summarize_args("key_press", {"key": "Return"})
        assert "Return" in result

    def test_summarize_args_app_open(self):
        from task_engine.executors.desktop_executor.executor import _summarize_args
        result = _summarize_args("app_open", {"url": "https://music.163.com"})
        assert "music.163.com" in result

    def test_summarize_args_vision_analyze(self):
        from task_engine.executors.desktop_executor.executor import _summarize_args
        result = _summarize_args("vision_analyze", {"query": "æœç´¢æ¡†", "image_path": "/tmp/test.png"})
        assert "æœç´¢æ¡†" in result

    def test_summarize_args_page_analyze(self):
        from task_engine.executors.desktop_executor.executor import _summarize_args
        result = _summarize_args("page_analyze", {"element_type": "search"})
        assert "search" in result

    def test_summarize_result_vision_found(self):
        from task_engine.executors.desktop_executor.executor import _summarize_result
        result_json = json.dumps({
            "found": True,
            "elements": [{"description": "æœç´¢æ¡†", "x": 100, "y": 50}]
        })
        summary = _summarize_result("vision_analyze", result_json)
        assert "æ‰¾åˆ°" in summary
        assert "æœç´¢æ¡†" in summary

    def test_summarize_result_vision_not_found(self):
        from task_engine.executors.desktop_executor.executor import _summarize_result
        result_json = json.dumps({"found": False, "elements": []})
        summary = _summarize_result("vision_analyze", result_json)
        assert "æœªæ‰¾åˆ°" in summary

    def test_summarize_result_page_analyze_found(self):
        from task_engine.executors.desktop_executor.executor import _summarize_result
        result_json = json.dumps({
            "found": True,
            "elements": [{"description": "input(placeholder=\"æœç´¢\")", "x": 500, "y": 35}]
        })
        summary = _summarize_result("page_analyze", result_json)
        assert "DOM æ‰¾åˆ°" in summary

    def test_summarize_result_page_analyze_not_found(self):
        from task_engine.executors.desktop_executor.executor import _summarize_result
        result_json = json.dumps({"found": False, "elements": []})
        summary = _summarize_result("page_analyze", result_json)
        assert "DOM æœªæ‰¾åˆ°" in summary

    def test_summarize_result_truncation(self):
        from task_engine.executors.desktop_executor.executor import _summarize_result
        long_result = "a" * 300
        summary = _summarize_result("click", long_result)
        assert len(summary) <= 200

    def test_summarize_result_screenshot_json(self):
        """æµ‹è¯• screenshot JSON ç»“æœçš„æ‘˜è¦"""
        from task_engine.executors.desktop_executor.executor import _summarize_result
        result_json = json.dumps({
            "file_path": "/tmp/desktop_screenshot_123.png",
            "image_width": 2880,
            "image_height": 1800,
            "screen_width": 1440,
            "screen_height": 900,
            "scale_factor": 2.0,
        })
        summary = _summarize_result("screenshot", result_json)
        assert "/tmp/desktop_screenshot_123.png" in summary
        assert "scale=2.0" in summary

    def test_summarize_result_screenshot_json_no_scale(self):
        """æµ‹è¯•æ— ç¼©æ”¾çš„ screenshot JSON ç»“æœçš„æ‘˜è¦"""
        from task_engine.executors.desktop_executor.executor import _summarize_result
        result_json = json.dumps({
            "file_path": "/tmp/desktop_screenshot_123.png",
            "image_width": 1920,
            "image_height": 1080,
        })
        summary = _summarize_result("screenshot", result_json)
        assert "/tmp/desktop_screenshot_123.png" in summary
        assert "scale" not in summary

    def test_summarize_result_screenshot_plain_string(self):
        """æµ‹è¯•æ—§æ ¼å¼çš„ screenshot ç»“æœå…¼å®¹æ€§"""
        from task_engine.executors.desktop_executor.executor import _summarize_result
        summary = _summarize_result("screenshot", "/tmp/screenshot.png")
        assert "/tmp/screenshot.png" in summary


# ============================================================
# DesktopExecutor æµ‹è¯•
# ============================================================

class TestDesktopExecutor:
    """æµ‹è¯•æ¡Œé¢æ“æ§æ‰§è¡Œå™¨"""

    @pytest.mark.asyncio
    async def test_missing_task_param(self):
        from task_engine.executors.desktop_executor.executor import DesktopExecutor
        from task_engine.models import ExecutorType, Step
        executor = DesktopExecutor()
        step = Step(executor_type=ExecutorType.DESKTOP, description="test", params={})
        result = await executor.execute(step)
        assert result.success is False
        assert "ç¼ºå°‘" in result.message

    @pytest.mark.asyncio
    async def test_llm_call_failure_returns_error(self):
        from task_engine.executors.desktop_executor.executor import DesktopExecutor
        from task_engine.models import ExecutorType, Step
        executor = DesktopExecutor()
        step = Step(
            executor_type=ExecutorType.DESKTOP,
            description="test",
            params={"task": "æ‰“å¼€æµè§ˆå™¨æ’­æ”¾éŸ³ä¹"},
        )
        # vLLM æœªè¿è¡Œï¼Œ_call_llm ä¼šè¿”å› None
        result = await executor.execute(step)
        assert result.success is False
        assert "LLM è°ƒç”¨å¤±è´¥" in result.message

    @pytest.mark.asyncio
    async def test_tool_call_loop_completion(self):
        """æµ‹è¯• LLM è¿”å›æ—  tool_calls æ—¶æ­£å¸¸å®Œæˆ"""
        from task_engine.executors.desktop_executor.executor import DesktopExecutor
        from task_engine.models import ExecutorType, Step

        executor = DesktopExecutor()

        # Mock _call_llm è¿”å›æ—  tool_callsï¼ˆä»»åŠ¡å®Œæˆï¼‰
        async def mock_call_llm(messages):
            return {"content": "å·²å®Œæˆæ’­æ”¾å‘¨æ°ä¼¦çš„æ™´å¤©", "tool_calls": None}

        executor._call_llm = mock_call_llm
        step = Step(
            executor_type=ExecutorType.DESKTOP,
            description="test",
            params={"task": "æ’­æ”¾éŸ³ä¹"},
        )
        result = await executor.execute(step)
        assert result.success is True
        assert "å‘¨æ°ä¼¦" in result.message

    @pytest.mark.asyncio
    async def test_guard_abort_during_loop(self):
        """æµ‹è¯•å®ˆå«åœ¨å¾ªç¯ä¸­æ£€æµ‹åˆ°å±é™©æ“ä½œæ—¶ç»ˆæ­¢"""
        from task_engine.executors.desktop_executor.executor import DesktopExecutor
        from task_engine.models import ExecutorType, Step

        executor = DesktopExecutor()
        call_count = 0

        async def mock_call_llm(messages):
            nonlocal call_count
            call_count += 1
            return {
                "content": "",
                "tool_calls": [{
                    "id": f"call_{call_count}",
                    "function": {
                        "name": "click",
                        "arguments": json.dumps({"x": 100, "y": 200}),
                    },
                }],
            }

        # Mock click å·¥å…·ï¼Œé€šè¿‡ TOOL_REGISTRY æ›¿æ¢
        async def mock_click(**kwargs):
            return "ç‚¹å‡»äº†æ”¯ä»˜æŒ‰é’®"

        with patch.dict(
            "task_engine.executors.desktop_executor.tools.TOOL_REGISTRY",
            {"click": mock_click},
        ):
            executor._call_llm = mock_call_llm
            step = Step(
                executor_type=ExecutorType.DESKTOP,
                description="test",
                params={"task": "æµ‹è¯•"},
            )
            result = await executor.execute(step)
            assert result.success is False
            assert "å®‰å…¨å®ˆå«ç»ˆæ­¢" in result.message

    @pytest.mark.asyncio
    async def test_multi_step_tool_call_flow(self):
        """æµ‹è¯•å¤šæ­¥éª¤å·¥å…·è°ƒç”¨æµç¨‹ï¼ˆæˆªå›¾â†’åˆ†æâ†’ç‚¹å‡»â†’è¾“å…¥â†’å›è½¦ï¼‰å¹¶éªŒè¯æ—¥å¿—è¾“å‡º"""
        from task_engine.executors.desktop_executor.executor import DesktopExecutor
        from task_engine.models import ExecutorType, Step

        executor = DesktopExecutor()
        call_count = 0

        async def mock_call_llm(messages):
            nonlocal call_count
            call_count += 1
            if call_count == 1:
                # ç¬¬ä¸€è½®ï¼šæ‰“å¼€ç½‘é¡µ
                return {
                    "content": "æˆ‘æ¥æ‰“å¼€éŸ³ä¹ç½‘ç«™",
                    "tool_calls": [{
                        "id": "call_1",
                        "function": {
                            "name": "app_open",
                            "arguments": json.dumps({"url": "https://music.163.com"}),
                        },
                    }],
                }
            elif call_count == 2:
                # ç¬¬äºŒè½®ï¼šæˆªå›¾
                return {
                    "content": "ç½‘é¡µå·²æ‰“å¼€ï¼Œæˆ‘æ¥æˆªå›¾",
                    "tool_calls": [{
                        "id": "call_2",
                        "function": {
                            "name": "screenshot",
                            "arguments": "{}",
                        },
                    }],
                }
            elif call_count == 3:
                # ç¬¬ä¸‰è½®ï¼šè§†è§‰åˆ†æ + ç‚¹å‡» + è¾“å…¥ + å›è½¦
                return {
                    "content": "æˆ‘æ¥æœç´¢å‘¨æ°ä¼¦",
                    "tool_calls": [
                        {
                            "id": "call_3a",
                            "function": {
                                "name": "click",
                                "arguments": json.dumps({"x": 500, "y": 100}),
                            },
                        },
                        {
                            "id": "call_3b",
                            "function": {
                                "name": "type_text",
                                "arguments": json.dumps({"text": "å‘¨æ°ä¼¦"}),
                            },
                        },
                        {
                            "id": "call_3c",
                            "function": {
                                "name": "key_press",
                                "arguments": json.dumps({"key": "Return"}),
                            },
                        },
                    ],
                }
            else:
                # ç¬¬å››è½®ï¼šå®Œæˆ
                return {
                    "content": "å·²æˆåŠŸæœç´¢å¹¶æ’­æ”¾å‘¨æ°ä¼¦çš„éŸ³ä¹",
                    "tool_calls": None,
                }

        # Mock æ‰€æœ‰å·¥å…·
        async def mock_app_open(**kwargs):
            return f"å·²æ‰“å¼€: {kwargs.get('url', '')}"

        async def mock_screenshot(**kwargs):
            return "/tmp/test_screenshot.png"

        async def mock_click(**kwargs):
            return f"å·²ç‚¹å‡»åæ ‡ ({kwargs.get('x')}, {kwargs.get('y')})"

        async def mock_type_text(**kwargs):
            return f"å·²è¾“å…¥æ–‡æœ¬: {kwargs.get('text', '')}"

        async def mock_key_press(**kwargs):
            return f"å·²æŒ‰ä¸‹: {kwargs.get('key', '')}"

        with patch.dict(
            "task_engine.executors.desktop_executor.tools.TOOL_REGISTRY",
            {
                "app_open": mock_app_open,
                "screenshot": mock_screenshot,
                "click": mock_click,
                "type_text": mock_type_text,
                "key_press": mock_key_press,
            },
        ):
            executor._call_llm = mock_call_llm
            step = Step(
                executor_type=ExecutorType.DESKTOP,
                description="test",
                params={"task": "æ‰“å¼€ç½‘é¡µé‡Œçš„éŸ³ä¹è¾“å…¥å‘¨æ°ä¼¦æ’­æ”¾éŸ³ä¹"},
            )
            result = await executor.execute(step)
            assert result.success is True
            assert "å‘¨æ°ä¼¦" in result.message
            assert result.data["iterations"] == 4
            assert call_count == 4

    @pytest.mark.asyncio
    async def test_max_iterations_reached(self):
        """æµ‹è¯•è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°æ—¶è¿”å›å¤±è´¥"""
        from task_engine.executors.desktop_executor.executor import DesktopExecutor
        from task_engine.models import ExecutorType, Step

        executor = DesktopExecutor()

        async def mock_call_llm(messages):
            return {
                "content": "ç»§ç»­æˆªå›¾",
                "tool_calls": [{
                    "id": "call_loop",
                    "function": {
                        "name": "screenshot",
                        "arguments": "{}",
                    },
                }],
            }

        async def mock_screenshot(**kwargs):
            return "/tmp/loop_screenshot.png"

        with patch.dict(
            "task_engine.executors.desktop_executor.tools.TOOL_REGISTRY",
            {"screenshot": mock_screenshot},
        ):
            executor._call_llm = mock_call_llm
            step = Step(
                executor_type=ExecutorType.DESKTOP,
                description="test",
                params={"task": "æ— é™å¾ªç¯ä»»åŠ¡"},
            )
            result = await executor.execute(step)
            assert result.success is False
            assert "æœ€å¤§è¿­ä»£æ¬¡æ•°" in result.message


# ============================================================
# TaskEngine å®Œæ•´æµç¨‹æµ‹è¯•
# ============================================================

class TestTaskEngine:
    """æµ‹è¯•å®Œæ•´ä»»åŠ¡å¼•æ“æµç¨‹"""

    @pytest.mark.asyncio
    async def test_llm_fallback_flow(self):
        """éæ¡Œé¢ä»»åŠ¡èµ° LLM å…œåº•"""
        from task_engine.engine import TaskEngine
        engine = TaskEngine()
        result = await engine.run("ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”çœŸå¥½")
        assert "âœ…" in result
        assert "LLM" in result

    @pytest.mark.asyncio
    async def test_desktop_flow_without_vllm(self):
        """æ¡Œé¢ä»»åŠ¡åœ¨æ—  vLLM æ—¶åº”è¿”å›å¤±è´¥"""
        from task_engine.engine import TaskEngine
        engine = TaskEngine()
        result = await engine.run("æ‰“å¼€ç½‘é¡µé‡Œçš„éŸ³ä¹è¾“å…¥å‘¨æ°ä¼¦æ’­æ”¾éŸ³ä¹")
        assert "âŒ" in result or "LLM è°ƒç”¨å¤±è´¥" in result


# ============================================================
# TaskEngineAgent æµ‹è¯•
# ============================================================

class TestTaskEngineAgent:
    """æµ‹è¯• TaskEngine Agent æ¡¥æ¥"""

    @pytest.fixture
    def agent(self):
        from src.agents.plugins.task_engine_agent import TaskEngineAgent
        return TaskEngineAgent()

    def test_agent_name(self, agent):
        assert agent.name == "TaskEngineAgent"

    def test_agent_description(self, agent):
        assert len(agent.description) > 0
        assert "æ¡Œé¢" in agent.description

    def test_skills(self, agent):
        assert "desktop_control" in agent.skills
        assert "music_play" in agent.skills
        assert "web_automation" in agent.skills

    def test_skill_keywords(self, agent):
        kw = agent.skill_keywords
        assert "desktop_control" in kw
        assert "æ‰“å¼€" in kw["desktop_control"]

    def test_skill_description(self, agent):
        desc = agent.get_skill_description("desktop_control")
        assert desc is not None

    def test_can_handle_high_confidence(self, agent):
        from src.agents.models import ChatContext, Message
        msg = Message(content="æ‰“å¼€ç½‘é¡µæ’­æ”¾éŸ³ä¹", user_id="u1", chat_id="c1")
        ctx = ChatContext(chat_id="c1")
        confidence = agent.can_handle(msg, ctx)
        assert confidence >= 0.75

    def test_can_handle_mention(self, agent):
        from src.agents.models import ChatContext, Message
        msg = Message(
            content="@TaskEngineAgent å¸®æˆ‘æ“ä½œ",
            user_id="u1",
            chat_id="c1",
            metadata={"mentions": ["@TaskEngineAgent"]},
        )
        ctx = ChatContext(chat_id="c1")
        assert agent.can_handle(msg, ctx) == 1.0

    def test_can_handle_no_match(self, agent):
        from src.agents.models import ChatContext, Message
        msg = Message(content="ä»Šå¤©å¿ƒæƒ…å¾ˆå¥½", user_id="u1", chat_id="c1")
        ctx = ChatContext(chat_id="c1")
        assert agent.can_handle(msg, ctx) == 0.0

    def test_can_handle_single_keyword_low(self, agent):
        from src.agents.models import ChatContext, Message
        msg = Message(content="éŸ³ä¹å¥½å¬", user_id="u1", chat_id="c1")
        ctx = ChatContext(chat_id="c1")
        confidence = agent.can_handle(msg, ctx)
        assert confidence == 0.4

    def test_memory_read_write(self, agent):
        agent.memory_write("u1", {"count": 1})
        data = agent.memory_read("u1")
        assert data["count"] == 1

    def test_memory_read_empty(self, agent):
        data = agent.memory_read("nonexistent")
        assert data == {}

    def test_can_provide_skill(self, agent):
        assert agent.can_provide_skill("desktop_control") is True
        assert agent.can_provide_skill("nonexistent") is False

    def test_respond_non_desktop(self, agent):
        """æµ‹è¯•éæ¡Œé¢ä»»åŠ¡çš„ respond"""
        from src.agents.models import AgentResponse, ChatContext, Message
        msg = Message(content="ä½ å¥½", user_id="u1", chat_id="c1")
        ctx = ChatContext(chat_id="c1")
        response = agent.respond(msg, ctx)
        assert isinstance(response, AgentResponse)
        assert response.agent_name == "TaskEngineAgent"

"""
Tests for LLM message structure in Agent Orchestrator
æµ‹è¯•Agentç¼–æ’å™¨å‘é€ç»™LLMçš„æ¶ˆæ¯ç»“æ„
"""
import pytest
from unittest.mock import Mock, AsyncMock
import json

from src.agents.orchestrator import (
    AgentOrchestrator, OrchestratorResult, IntentType, IntentSource
)
from src.agents import Message, ChatContext, BaseAgent


class MockLLMProvider:
    """Mock LLM provider that captures the messages sent to it"""
    
    def __init__(self, response_json):
        self.response_json = response_json
        self.last_messages = None  # Capture the messages sent
    
    async def generate_response(self, messages, context=None):
        """Capture messages and return a mock JSON response"""
        self.last_messages = messages
        return f"```json\n{json.dumps(self.response_json)}\n```"


class TestOrchestratorMessageStructure:
    """æµ‹è¯•ç¼–æ’å™¨å‘é€ç»™LLMçš„æ¶ˆæ¯ç»“æ„"""
    
    @pytest.mark.asyncio
    async def test_message_structure_with_conversation_history(self):
        """æµ‹è¯•åŒ…å«å¯¹è¯å†å²æ—¶çš„æ¶ˆæ¯ç»“æ„"""
        # Mock LLM response
        llm_response = {
            "intent": "direct_response",
            "agents": [],
            "reasoning": "ç›´æ¥å›å¤",
            "direct_reply": "å¥½çš„ï¼Œæˆ‘ç†è§£äº†",
            "emotion": "gentle",
            "emotion_description": "æ¸©æŸ”ã€è½»å£°",
            "memory": {
                "is_important": False
            }
        }
        
        llm_provider = MockLLMProvider(llm_response)
        orchestrator = AgentOrchestrator([], llm_provider=llm_provider, enable_unified_mode=True)
        
        # åˆ›å»ºåŒ…å«å¯¹è¯å†å²çš„ä¸Šä¸‹æ–‡
        conversation_history = [
            Message(content="ä½ å¥½", user_id="user", chat_id="456"),
            Message(content="ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ", user_id="assistant", chat_id="456"),
            Message(content="ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ", user_id="user", chat_id="456"),
            Message(content="ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé˜³å…‰æ˜åªš", user_id="assistant", chat_id="456"),
        ]
        
        context = ChatContext(
            chat_id="456",
            conversation_history=conversation_history,
            system_prompt="ä½ æ˜¯å›¢å›¢ï¼Œä¸€åæ´»æ³¼ã€å¤©çœŸçš„å°é™ªä¼´å¥³ç”Ÿ"
        )
        
        # å‘é€å½“å‰æ¶ˆæ¯
        message = Message(content="æˆ‘æƒ³çŸ¥é“æ˜å¤©çš„è®¡åˆ’", user_id="user", chat_id="456")
        
        await orchestrator.analyze_intent_unified(message, context)
        
        # éªŒè¯å‘é€ç»™LLMçš„æ¶ˆæ¯ç»“æ„
        assert llm_provider.last_messages is not None, "åº”è¯¥æ•è·åˆ°å‘é€çš„æ¶ˆæ¯"
        
        messages = llm_provider.last_messages
        print(f"\nğŸ“¨ Total messages sent: {len(messages)}")
        for i, msg in enumerate(messages):
            print(f"  {i+1}. role={msg['role']}, content_preview={msg['content'][:50]}...")
        
        # éªŒè¯æ¶ˆæ¯æ•°é‡ï¼š1ä¸ªsystem + 4ä¸ªå†å² + 1ä¸ªå½“å‰ç”¨æˆ·æ¶ˆæ¯ = 6æ¡
        assert len(messages) == 6, f"åº”è¯¥æœ‰6æ¡æ¶ˆæ¯ï¼Œå®é™…æœ‰{len(messages)}æ¡"
        
        # éªŒè¯ç¬¬ä¸€æ¡æ˜¯ system
        assert messages[0]["role"] == "system", "ç¬¬ä¸€æ¡åº”è¯¥æ˜¯systemæ¶ˆæ¯"
        assert "å›¢å›¢" in messages[0]["content"], "systemæ¶ˆæ¯åº”è¯¥åŒ…å«å®Œæ•´çš„å¢å¼ºprompt"
        
        # éªŒè¯æ¥ä¸‹æ¥æ˜¯å¯¹è¯å†å²ï¼ˆ4æ¡ï¼‰
        assert messages[1]["role"] == "user", "ç¬¬2æ¡åº”è¯¥æ˜¯useræ¶ˆæ¯"
        assert messages[1]["content"] == "ä½ å¥½", "å†…å®¹åº”è¯¥åŒ¹é…å†å²æ¶ˆæ¯"
        
        assert messages[2]["role"] == "assistant", "ç¬¬3æ¡åº”è¯¥æ˜¯assistantæ¶ˆæ¯"
        assert messages[2]["content"] == "ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ", "å†…å®¹åº”è¯¥åŒ¹é…å†å²å›å¤"
        
        assert messages[3]["role"] == "user", "ç¬¬4æ¡åº”è¯¥æ˜¯useræ¶ˆæ¯"
        assert messages[3]["content"] == "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ", "å†…å®¹åº”è¯¥åŒ¹é…å†å²æ¶ˆæ¯"
        
        assert messages[4]["role"] == "assistant", "ç¬¬5æ¡åº”è¯¥æ˜¯assistantæ¶ˆæ¯"
        assert messages[4]["content"] == "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé˜³å…‰æ˜åªš", "å†…å®¹åº”è¯¥åŒ¹é…å†å²å›å¤"
        
        # éªŒè¯æœ€åä¸€æ¡æ˜¯å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼ˆåŒ…å«ç»Ÿä¸€promptæ¨¡æ¿ï¼‰
        assert messages[5]["role"] == "user", "æœ€åä¸€æ¡åº”è¯¥æ˜¯useræ¶ˆæ¯"
        assert "æˆ‘æƒ³çŸ¥é“æ˜å¤©çš„è®¡åˆ’" in messages[5]["content"], "åº”è¯¥åŒ…å«å½“å‰ç”¨æˆ·æ¶ˆæ¯"
        assert "ä»»åŠ¡1ï¼šæ„å›¾è¯†åˆ«" in messages[5]["content"], "åº”è¯¥åŒ…å«ç»Ÿä¸€promptæ¨¡æ¿"
    
    @pytest.mark.asyncio
    async def test_message_structure_without_history(self):
        """æµ‹è¯•æ²¡æœ‰å¯¹è¯å†å²æ—¶çš„æ¶ˆæ¯ç»“æ„"""
        llm_response = {
            "intent": "direct_response",
            "agents": [],
            "reasoning": "ç›´æ¥å›å¤",
            "direct_reply": "ä½ å¥½ï¼",
            "emotion": "happy",
            "emotion_description": "å¼€å¿ƒã€è½»å¿«",
            "memory": {
                "is_important": False
            }
        }
        
        llm_provider = MockLLMProvider(llm_response)
        orchestrator = AgentOrchestrator([], llm_provider=llm_provider, enable_unified_mode=True)
        
        # åˆ›å»ºæ²¡æœ‰å†å²çš„ä¸Šä¸‹æ–‡
        context = ChatContext(
            chat_id="456",
            conversation_history=[],
            system_prompt="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹"
        )
        
        message = Message(content="ä½ å¥½", user_id="user", chat_id="456")
        
        await orchestrator.analyze_intent_unified(message, context)
        
        # éªŒè¯æ¶ˆæ¯ç»“æ„
        messages = llm_provider.last_messages
        
        # æ²¡æœ‰å†å²æ—¶ï¼š1ä¸ªsystem + 1ä¸ªå½“å‰ç”¨æˆ·æ¶ˆæ¯ = 2æ¡
        assert len(messages) == 2, f"åº”è¯¥æœ‰2æ¡æ¶ˆæ¯ï¼Œå®é™…æœ‰{len(messages)}æ¡"
        
        assert messages[0]["role"] == "system"
        assert messages[1]["role"] == "user"
        assert "ä½ å¥½" in messages[1]["content"]
    
    @pytest.mark.asyncio
    async def test_message_structure_with_long_history(self):
        """æµ‹è¯•æœ‰å¾ˆå¤šå¯¹è¯å†å²æ—¶åªå–æœ€è¿‘10æ¡"""
        llm_response = {
            "intent": "direct_response",
            "agents": [],
            "reasoning": "ç›´æ¥å›å¤",
            "direct_reply": "æ˜ç™½äº†",
            "emotion": None,
            "memory": {"is_important": False}
        }
        
        llm_provider = MockLLMProvider(llm_response)
        orchestrator = AgentOrchestrator([], llm_provider=llm_provider, enable_unified_mode=True)
        
        # åˆ›å»º15æ¡å†å²æ¶ˆæ¯ï¼ˆåº”è¯¥åªå–æœ€è¿‘10æ¡ï¼‰
        conversation_history = []
        for i in range(15):
            conversation_history.append(
                Message(content=f"ç”¨æˆ·æ¶ˆæ¯{i}", user_id="user", chat_id="456")
            )
            conversation_history.append(
                Message(content=f"åŠ©æ‰‹å›å¤{i}", user_id="assistant", chat_id="456")
            )
        
        context = ChatContext(
            chat_id="456",
            conversation_history=conversation_history,
            system_prompt="ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹"
        )
        
        message = Message(content="å½“å‰æ¶ˆæ¯", user_id="user", chat_id="456")
        
        await orchestrator.analyze_intent_unified(message, context)
        
        messages = llm_provider.last_messages
        
        # éªŒè¯æœ€å¤šå–10æ¡å†å²ï¼š1ä¸ªsystem + 10æ¡å†å² + 1ä¸ªå½“å‰ = 12æ¡
        assert len(messages) == 12, f"åº”è¯¥æœ‰12æ¡æ¶ˆæ¯ï¼ˆsystem + 10æ¡å†å² + å½“å‰ï¼‰ï¼Œå®é™…æœ‰{len(messages)}æ¡"
        
        # éªŒè¯å–çš„æ˜¯æœ€è¿‘çš„10æ¡ï¼ˆå³æœ€åçš„10æ¡å†å²æ¶ˆæ¯ï¼‰
        # 15è½® = 30æ¡å†å²æ¶ˆæ¯ï¼Œå–æœ€å10æ¡å³ç´¢å¼•20-29ï¼Œå¯¹åº”"ç”¨æˆ·æ¶ˆæ¯10"åˆ°"åŠ©æ‰‹å›å¤14"
        assert "ç”¨æˆ·æ¶ˆæ¯10" in messages[1]["content"], "åº”è¯¥ä»ç¬¬10è½®å¼€å§‹ï¼ˆæœ€å10æ¡æ¶ˆæ¯ï¼‰"
        assert "åŠ©æ‰‹å›å¤14" in messages[10]["content"], "åº”è¯¥åŒ…å«æœ€åä¸€è½®çš„å›å¤"
    
    @pytest.mark.asyncio
    async def test_message_roles_correctly_identified(self):
        """æµ‹è¯•æ¶ˆæ¯è§’è‰²è¢«æ­£ç¡®è¯†åˆ«"""
        llm_response = {
            "intent": "direct_response",
            "agents": [],
            "reasoning": "ç›´æ¥å›å¤",
            "direct_reply": "å¥½çš„",
            "emotion": None,
            "memory": {"is_important": False}
        }
        
        llm_provider = MockLLMProvider(llm_response)
        orchestrator = AgentOrchestrator([], llm_provider=llm_provider, enable_unified_mode=True)
        
        # æµ‹è¯•ä¸åŒçš„user_idè¯†åˆ«
        conversation_history = [
            Message(content="æ¶ˆæ¯1", user_id="user", chat_id="456"),
            Message(content="å›å¤1", user_id="assistant", chat_id="456"),
            Message(content="æ¶ˆæ¯2", user_id="bot", chat_id="456"),  # "bot"åº”è¯¥è¢«è¯†åˆ«ä¸ºassistant
            Message(content="æ¶ˆæ¯3", user_id="agent", chat_id="456"),  # "agent"åº”è¯¥è¢«è¯†åˆ«ä¸ºassistant
        ]
        
        context = ChatContext(
            chat_id="456",
            conversation_history=conversation_history,
            system_prompt="æµ‹è¯•"
        )
        
        message = Message(content="å½“å‰", user_id="user", chat_id="456")
        await orchestrator.analyze_intent_unified(message, context)
        
        messages = llm_provider.last_messages
        
        # éªŒè¯è§’è‰²
        assert messages[1]["role"] == "user"
        assert messages[2]["role"] == "assistant"
        assert messages[3]["role"] == "assistant"  # "bot"åº”è¯¥è¢«è¯†åˆ«ä¸ºassistant
        assert messages[4]["role"] == "assistant"  # "agent"åº”è¯¥è¢«è¯†åˆ«ä¸ºassistant
